[
  {
    "objectID": "presentation.html#workflows-for-reproducibility-1",
    "href": "presentation.html#workflows-for-reproducibility-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Workflows for Reproducibility",
    "text": "Workflows for Reproducibility\nWe will discuss three workflows for reproducibility:\n\n\nRstudio project to Zenodo pipeline\nContainerization with Docker\nVersion control with Git\n\n\n\nThese are suggestions for different approaches and we hope that in future you will be able to adapt these workflows to the needs of your own research projects."
  },
  {
    "objectID": "presentation.html#workflows-for-reproducibility-2",
    "href": "presentation.html#workflows-for-reproducibility-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Workflows for Reproducibility",
    "text": "Workflows for Reproducibility"
  },
  {
    "objectID": "exercises/Rproj-Zenodo_exercise.html",
    "href": "exercises/Rproj-Zenodo_exercise.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "In this exercise you will setup of a basic RStudio project following good practices for organizing data, writing clean code and decomposing your workflow introduced in Rproject section. We‘ll also initialize renv to manage package dependencies, ensuring reproducibility. Once the project is set up, we’ll go through the process of how to publish the project to Zenodo as described in the Zenodo workflow section.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\n\n\nClick here to download the resources for the exercise:  Download resources for exercise\nUnzip the downloaded file and move the folder to a location on your computer where you can easily find it.\n\n\n\n\nStart by creating a new RStudio project in the root of the exercise_1_data directory. you have just downloaded. You can name the project what you like but in this example, we have named it Rice_farm_analysis.proj.\n\nOpen RStudio.\nCreate the project using: File &gt; New Project &gt; Existing Directory.\nSelect the exercise_1_data folder as the location and give the project a name, for example, Rice_farm_analysis.proj.\n\nThis creates a .Rproj file in the root of your project to help manage the workspace and project-specific settings.\n\n\n\nIt’s good practice to organize raw and processed data in separate folders. Let’s start by organizing the data:\n\nCreate a directory Data/Raw inside your project folder.\nMove the provided CSV file into this Data/Raw directory.\n\n\n\n\nWe’ll now organize the project’s scripts by splitting the original script into separate analysis and visualization scripts.\n\nCreate a Scripts folder inside your project directory.\nMove the original RiceFarm_project.R script into the Scripts folder.\n\nWe’ll now organize the project’s scripts by splitting the original script into separate analysis and visualization scripts.\n\nCreate a scripts folder inside your project directory.\nMove the original RiceFarm_project.R script into the scripts folder.\nCreate two new scripts named 01_data_analysis.R and 02_data_visualisation.R.\n\n\n\n\nCopy the following code from RiceFarm_project.R:\n\nThe call to the relevant library library(stringr)\nEverything before the call to the ggplot() function\n\nIn addition to this, replace the setwd() function with this code to set up relative paths, create a directory to save the processed data in and save rice_data_summary to disk after processing:\n\n\n# vector and create processed data save dir\nsave_dir &lt;- \"Data/Processed\"\ndir.create(save_dir,\n           showWarnings = FALSE,\n           recursive = TRUE)\n\n# Load csv file of data\nrice_data &lt;- read.csv(file.path(raw_dir, \"RiceFarms.csv\"))\n  \n# Save the summarized data\nwrite.csv(rice_data_summary,\n          file.path(save_dir, \"RiceFarms_summary.csv\"),\n          row.names = FALSE)\n\n\n\n\n\nCopy the following code from RiceFarm_project.R:\n\nThe call to the relevant library library(ggplot2)\nThe call to the functions ggplot() and ggsave()\n\nAdd the following code after the library() call to create an output directory for the plots and load the summarized data from the processed data folder:\n\n\n# Directory for saving plots\nplot_dir &lt;- \"Output/Visualisations\"\ndir.create(plot_dir, showWarnings = FALSE, recursive = TRUE)\n\n# load the summarized data\nrice_data_summary &lt;- read.csv(\"Data/Processed/RiceFarms_summary.csv\")\n\n\n\n\n\n\nAdd headers to both new scripts. You can use this template:\n\n\n# -----------------------------------------------\n# Script Name: [01_data_analysis.R / 02_data_visualisation.R]\n# Project: Rice Farm Analysis\n# Purpose: [Data analysis / Data visualization]\n# Author: [Your Name]\n# Date: [YYYY-MM-DD]\n# -----------------------------------------------\n\n\n\n\nAs a next step you will create a master script that runs both the data analysis and visualization scripts.\n\nIn the root of your project, create a new file named RiceFarm_master.R\nAdd a header as in Step 5.\nAdd the following code snippet to the script to source 01_data_analysis.R and 02_data_visualisation.R:\n\n\n###=========================================================================\n### 01- Data analysis\n### =========================================================================\n\n# Source the data analysis script\nsource(\"Scripts/01_data_analysis.R\")\n\n### =========================================================================\n### 02- Data visualization\n### =========================================================================\n\n# Source the data visualization script\nsource(\"Scripts/02_data_visualisation.R\")\n\nRunning this master script will execute both analysis and visualization steps.\n\n\n\nWe will use renv to make your projects package environment reproducible.\n\ninstall renv package\n\n\ninstall.packages(\"renv\")\n\n\nRun the following command in your master script to set up the project-specific environment\n\n\nrenv::init()\n\nThis creates a local library for your project and captures the required packages.\n\nOnce the initialization is complete, run:\n\n\n\n\nFor convenience, we can configure RStudio to automatically open the master script when the project is loaded.\n\ninstall the rstudioapi package:\n\n\ninstall.packages(\"rstudioapi\")\n\n\nOpen the .Rprofile file in the root of your project directory. The file might be hidden. On Windows click “View” &gt; “Show” &gt; “Hidden items” in the explorer and on MacOS click Press Command+Shift+Dot within the root directory to see the file.\n\n\nrenv::snapshot()\n\nThis records the project’s environment in a renv.lock file, which is essential for reproducibility.\n\n\n\nFor convenience, we will configure RStudio to automatically open the master script when the project is loaded.\n\nOpen the .Rprofile file in the root of your project directory. The file might be hidden. On Windows click View &gt; Show &gt; Hidden items in the explorer and on MacOS click Press Command+Shift+Dot within the root directory to see the file.\nAdd the following R code to the .Rprofile file:\n\n\nrsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession)\n    rstudioapi::navigateToFile('RiceFarm_master.R', line = -1L, column = -1L)\n}, action = \"append\")\n\n\n\n\nAfter modifying the .Rprofile file, it’s important to capture these changes in the renv.lock file.\n\nRun the following command in your master script to ensure that the rstudioapi package (which enables automatic script opening) is included in the snapshot:\n\n\nrenv::snapshot()\n\nNow we a have a nicely organised project structure with the workflow decomposed into seperate scripts and a master script that to run the whole project.\n\n\n\nThe following steps are heavily based on [1]. We have extracted the most relevant parts to explain the workflow. If you are interested in more details, check out their user manual at: https://cran.r-project.org/web/packages/zen4R/vignettes/zen4R.html.\nFor this exercise we will not be using Zenodo directly but Zenodo Sandbox. The Zenodo Sandbox is a separate, secure testing environment where users can explore Zenodo‘s features without impacting the main platform‘s publicly accessible data. It allows you to test file uploads, generate test DOIs, and experiment with API integrations. DOIs created in the sandbox are only for testing and use a different prefix. You will need a separate account and access token for the sandbox, distinct from those used on Zenodo‘s main site.\n\nCreate an account on https://sandbox.zenodo.org.\n\nZenodo can be accessed with the R package zen4R to upload, edit, publish and download data.\n\nCreate a new R script outside of the project directory.\nInstall zen4R library with the following code:\n\n\n#install dependency \"remotes\"\ninstall.packages(\"remotes\")\n        \n#install zen4R\nrequire(\"remotes\")\ninstall_github(\"eblondel/zen4R\")\n\n\n\n\nA Zenodo record includes metadata, data and a Digital Object Identifier (DOI) which is automatically generated by Zenodo for all uploads. But before you can add records to Zenodo, you need to get access to your account through R.\n\nGo to https://sandbox.zenodo.org/account/settings/applications/.\nLog into your account and then create a new “Personal access token” in the “Applications” section of your account.\nThen run the following code in your script to establish the access and create a new record.\n\n\nlibrary(zen4R)\n        \n#Create manager to access your Zenodo repository\nzenodo &lt;- ZenodoManager$new(\n    token = \"your_token\",\n    sandbox = TRUE,\n    logger = \"INFO\" \n    )\n        \n##Prepare a new record to be filled with metadata and uploaded to Zenodo\nmyrec &lt;- ZenodoRecord$new()\n\nIf you want to connect to Zenodo and not Zenodo Sandbox, create the token in your Zenodo account and remove the line sandbox = True in the code above.\nThe types of metadata that can be included in a Zenodo record are vast. A full list can be found in the documentation.\n\nCopy and run the example below to add metadata to your record.\n\n\nmyrec$setTitle(\"RiceFarm\") #title of the record\nmyrec$addAdditionalTitle(\"This is an alternative title\", type = \"alternative-title\")\nmyrec$setDescription(\"Calculating statistics of RiceFarm dataset\") #description\nmyrec$addAdditionalDescription(\"This is an abstract\", type = \"abstract\")\nmyrec$setPublicationDate(\"2024-09-16\") #Format YYYY-MM-DD\nmyrec$setResourceType(\"dataset\")\nmyrec$addCreator(firstname = \"Yourfirstname\", lastname = \"Yourlastname\", role = \"datamanager\", orcid = \"0000-0001-0002-0003\")\nmyrec$setKeywords(c(\"R\",\"dataset\")) #For filtering\nmyrec$addReference(\"Blondel E. et al., 2024 zen4R: R Interface to Zenodo REST API\")\n\nA record can be deposited on Zenodo before it is published. This will add the record to your account without making it public yet. A deposited record can still be edited or deleted. You can also upload data to a deposited record. If you prefer a graphical interface, you can also edit the record on the Zenodo website.\n\nDeposit the record on Zenodo:\n\n\n#deposit record\nmyrec &lt;- zenodo$depositRecord(myrec, publish = TRUE)\n\n\nView the deposited record at https://sandbox.zenodo.org/me/uploads?q=&l=list&p=1&s=10&sort=newest\nCompress your project directory to a .zip file.\nUpload the .zip file to your deposited record:\n\n\n#add data to the record, adjust the path below\nzenodo$uploadFile(\"path/to/your/file\", record = myrec)\n\n\nPublish the record:\n\n\n#make the record publicly available on Zenodo (Sandbox).\nmyrec &lt;- zenodo$publishRecord(myrec$id)\n\n\n\n\n\nIt is also possible to edit or update the metadata of published records:\n\n\n#get your record by metadata query, e.g. by title\nmyrec &lt;- zenodo$getDepositions(q='title:zen4R')\n\n#get depositions creates a list, access first element\nmyrec &lt;- myrec[[1]]\n\n#edit metadata\nmyrec &lt;- zenodo$editRecord(myrec$id)\nmyrec$setTitle(\"zen4R 2.0\")\n\n#redeposit and publish the edited record\nmyrec &lt;- zenodo$depositRecord(myrec, publish = TRUE)\n\n\nOnce a record has been published, it is not possible to edit the data that has been attached to it. However, it is possible to upload an updated version of the data. The previous version of the data will remain accessible via Zenodo. The record will have one overall DOI, while each version will have its own DOI.\nReconnect to your account:\n\n\nzenodo &lt;- ZenodoManager$new(\n      token = \"your_token\",\n      sandbox = TRUE,\n      logger = \"INFO\")\n\n\nAccess your record:\n\n\n#get your record by querying the metadata, e.g. by title, this will give you a list of all records with that title.\nmyrec &lt;- zenodo$getDepositions(q='title:RiceFarm Statistics')\n    \n#access the first item in the list, as there should only be one record with that particular title\nmyrec &lt;- myrec[[1]]\n\n\nRename your .zip file on your computer\nUpload the renamed .zip file:\n\n\n#edit data, delete_latest_files = TRUE to not include data of previous version in newer version\nmyrec &lt;- zenodo$depositRecordVersion(myrec, delete_latest_files = TRUE, files = \"path/to/your/new/file\", publish = TRUE)\n\n\nAgain, go to https://sandbox.zenodo.org/me/uploads?q=&l=list&p=1&s=10&sort=newest\nActivate “View all versions” on the left hand side.\nCheck if both versions show up"
  },
  {
    "objectID": "exercises/Rproj-Zenodo_exercise.html#sec-Rproj_zenodo_exercise",
    "href": "exercises/Rproj-Zenodo_exercise.html#sec-Rproj_zenodo_exercise",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "In this exercise you will setup of a basic RStudio project following good practices for organizing data, writing clean code and decomposing your workflow introduced in Rproject section. We‘ll also initialize renv to manage package dependencies, ensuring reproducibility. Once the project is set up, we’ll go through the process of how to publish the project to Zenodo as described in the Zenodo workflow section.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\n\n\nClick here to download the resources for the exercise:  Download resources for exercise\nUnzip the downloaded file and move the folder to a location on your computer where you can easily find it.\n\n\n\n\nStart by creating a new RStudio project in the root of the exercise_1_data directory. you have just downloaded. You can name the project what you like but in this example, we have named it Rice_farm_analysis.proj.\n\nOpen RStudio.\nCreate the project using: File &gt; New Project &gt; Existing Directory.\nSelect the exercise_1_data folder as the location and give the project a name, for example, Rice_farm_analysis.proj.\n\nThis creates a .Rproj file in the root of your project to help manage the workspace and project-specific settings.\n\n\n\nIt’s good practice to organize raw and processed data in separate folders. Let’s start by organizing the data:\n\nCreate a directory Data/Raw inside your project folder.\nMove the provided CSV file into this Data/Raw directory.\n\n\n\n\nWe’ll now organize the project’s scripts by splitting the original script into separate analysis and visualization scripts.\n\nCreate a Scripts folder inside your project directory.\nMove the original RiceFarm_project.R script into the Scripts folder.\n\nWe’ll now organize the project’s scripts by splitting the original script into separate analysis and visualization scripts.\n\nCreate a scripts folder inside your project directory.\nMove the original RiceFarm_project.R script into the scripts folder.\nCreate two new scripts named 01_data_analysis.R and 02_data_visualisation.R.\n\n\n\n\nCopy the following code from RiceFarm_project.R:\n\nThe call to the relevant library library(stringr)\nEverything before the call to the ggplot() function\n\nIn addition to this, replace the setwd() function with this code to set up relative paths, create a directory to save the processed data in and save rice_data_summary to disk after processing:\n\n\n# vector and create processed data save dir\nsave_dir &lt;- \"Data/Processed\"\ndir.create(save_dir,\n           showWarnings = FALSE,\n           recursive = TRUE)\n\n# Load csv file of data\nrice_data &lt;- read.csv(file.path(raw_dir, \"RiceFarms.csv\"))\n  \n# Save the summarized data\nwrite.csv(rice_data_summary,\n          file.path(save_dir, \"RiceFarms_summary.csv\"),\n          row.names = FALSE)\n\n\n\n\n\nCopy the following code from RiceFarm_project.R:\n\nThe call to the relevant library library(ggplot2)\nThe call to the functions ggplot() and ggsave()\n\nAdd the following code after the library() call to create an output directory for the plots and load the summarized data from the processed data folder:\n\n\n# Directory for saving plots\nplot_dir &lt;- \"Output/Visualisations\"\ndir.create(plot_dir, showWarnings = FALSE, recursive = TRUE)\n\n# load the summarized data\nrice_data_summary &lt;- read.csv(\"Data/Processed/RiceFarms_summary.csv\")\n\n\n\n\n\n\nAdd headers to both new scripts. You can use this template:\n\n\n# -----------------------------------------------\n# Script Name: [01_data_analysis.R / 02_data_visualisation.R]\n# Project: Rice Farm Analysis\n# Purpose: [Data analysis / Data visualization]\n# Author: [Your Name]\n# Date: [YYYY-MM-DD]\n# -----------------------------------------------\n\n\n\n\nAs a next step you will create a master script that runs both the data analysis and visualization scripts.\n\nIn the root of your project, create a new file named RiceFarm_master.R\nAdd a header as in Step 5.\nAdd the following code snippet to the script to source 01_data_analysis.R and 02_data_visualisation.R:\n\n\n###=========================================================================\n### 01- Data analysis\n### =========================================================================\n\n# Source the data analysis script\nsource(\"Scripts/01_data_analysis.R\")\n\n### =========================================================================\n### 02- Data visualization\n### =========================================================================\n\n# Source the data visualization script\nsource(\"Scripts/02_data_visualisation.R\")\n\nRunning this master script will execute both analysis and visualization steps.\n\n\n\nWe will use renv to make your projects package environment reproducible.\n\ninstall renv package\n\n\ninstall.packages(\"renv\")\n\n\nRun the following command in your master script to set up the project-specific environment\n\n\nrenv::init()\n\nThis creates a local library for your project and captures the required packages.\n\nOnce the initialization is complete, run:\n\n\n\n\nFor convenience, we can configure RStudio to automatically open the master script when the project is loaded.\n\ninstall the rstudioapi package:\n\n\ninstall.packages(\"rstudioapi\")\n\n\nOpen the .Rprofile file in the root of your project directory. The file might be hidden. On Windows click “View” &gt; “Show” &gt; “Hidden items” in the explorer and on MacOS click Press Command+Shift+Dot within the root directory to see the file.\n\n\nrenv::snapshot()\n\nThis records the project’s environment in a renv.lock file, which is essential for reproducibility.\n\n\n\nFor convenience, we will configure RStudio to automatically open the master script when the project is loaded.\n\nOpen the .Rprofile file in the root of your project directory. The file might be hidden. On Windows click View &gt; Show &gt; Hidden items in the explorer and on MacOS click Press Command+Shift+Dot within the root directory to see the file.\nAdd the following R code to the .Rprofile file:\n\n\nrsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession)\n    rstudioapi::navigateToFile('RiceFarm_master.R', line = -1L, column = -1L)\n}, action = \"append\")\n\n\n\n\nAfter modifying the .Rprofile file, it’s important to capture these changes in the renv.lock file.\n\nRun the following command in your master script to ensure that the rstudioapi package (which enables automatic script opening) is included in the snapshot:\n\n\nrenv::snapshot()\n\nNow we a have a nicely organised project structure with the workflow decomposed into seperate scripts and a master script that to run the whole project.\n\n\n\nThe following steps are heavily based on [1]. We have extracted the most relevant parts to explain the workflow. If you are interested in more details, check out their user manual at: https://cran.r-project.org/web/packages/zen4R/vignettes/zen4R.html.\nFor this exercise we will not be using Zenodo directly but Zenodo Sandbox. The Zenodo Sandbox is a separate, secure testing environment where users can explore Zenodo‘s features without impacting the main platform‘s publicly accessible data. It allows you to test file uploads, generate test DOIs, and experiment with API integrations. DOIs created in the sandbox are only for testing and use a different prefix. You will need a separate account and access token for the sandbox, distinct from those used on Zenodo‘s main site.\n\nCreate an account on https://sandbox.zenodo.org.\n\nZenodo can be accessed with the R package zen4R to upload, edit, publish and download data.\n\nCreate a new R script outside of the project directory.\nInstall zen4R library with the following code:\n\n\n#install dependency \"remotes\"\ninstall.packages(\"remotes\")\n        \n#install zen4R\nrequire(\"remotes\")\ninstall_github(\"eblondel/zen4R\")\n\n\n\n\nA Zenodo record includes metadata, data and a Digital Object Identifier (DOI) which is automatically generated by Zenodo for all uploads. But before you can add records to Zenodo, you need to get access to your account through R.\n\nGo to https://sandbox.zenodo.org/account/settings/applications/.\nLog into your account and then create a new “Personal access token” in the “Applications” section of your account.\nThen run the following code in your script to establish the access and create a new record.\n\n\nlibrary(zen4R)\n        \n#Create manager to access your Zenodo repository\nzenodo &lt;- ZenodoManager$new(\n    token = \"your_token\",\n    sandbox = TRUE,\n    logger = \"INFO\" \n    )\n        \n##Prepare a new record to be filled with metadata and uploaded to Zenodo\nmyrec &lt;- ZenodoRecord$new()\n\nIf you want to connect to Zenodo and not Zenodo Sandbox, create the token in your Zenodo account and remove the line sandbox = True in the code above.\nThe types of metadata that can be included in a Zenodo record are vast. A full list can be found in the documentation.\n\nCopy and run the example below to add metadata to your record.\n\n\nmyrec$setTitle(\"RiceFarm\") #title of the record\nmyrec$addAdditionalTitle(\"This is an alternative title\", type = \"alternative-title\")\nmyrec$setDescription(\"Calculating statistics of RiceFarm dataset\") #description\nmyrec$addAdditionalDescription(\"This is an abstract\", type = \"abstract\")\nmyrec$setPublicationDate(\"2024-09-16\") #Format YYYY-MM-DD\nmyrec$setResourceType(\"dataset\")\nmyrec$addCreator(firstname = \"Yourfirstname\", lastname = \"Yourlastname\", role = \"datamanager\", orcid = \"0000-0001-0002-0003\")\nmyrec$setKeywords(c(\"R\",\"dataset\")) #For filtering\nmyrec$addReference(\"Blondel E. et al., 2024 zen4R: R Interface to Zenodo REST API\")\n\nA record can be deposited on Zenodo before it is published. This will add the record to your account without making it public yet. A deposited record can still be edited or deleted. You can also upload data to a deposited record. If you prefer a graphical interface, you can also edit the record on the Zenodo website.\n\nDeposit the record on Zenodo:\n\n\n#deposit record\nmyrec &lt;- zenodo$depositRecord(myrec, publish = TRUE)\n\n\nView the deposited record at https://sandbox.zenodo.org/me/uploads?q=&l=list&p=1&s=10&sort=newest\nCompress your project directory to a .zip file.\nUpload the .zip file to your deposited record:\n\n\n#add data to the record, adjust the path below\nzenodo$uploadFile(\"path/to/your/file\", record = myrec)\n\n\nPublish the record:\n\n\n#make the record publicly available on Zenodo (Sandbox).\nmyrec &lt;- zenodo$publishRecord(myrec$id)\n\n\n\n\n\nIt is also possible to edit or update the metadata of published records:\n\n\n#get your record by metadata query, e.g. by title\nmyrec &lt;- zenodo$getDepositions(q='title:zen4R')\n\n#get depositions creates a list, access first element\nmyrec &lt;- myrec[[1]]\n\n#edit metadata\nmyrec &lt;- zenodo$editRecord(myrec$id)\nmyrec$setTitle(\"zen4R 2.0\")\n\n#redeposit and publish the edited record\nmyrec &lt;- zenodo$depositRecord(myrec, publish = TRUE)\n\n\nOnce a record has been published, it is not possible to edit the data that has been attached to it. However, it is possible to upload an updated version of the data. The previous version of the data will remain accessible via Zenodo. The record will have one overall DOI, while each version will have its own DOI.\nReconnect to your account:\n\n\nzenodo &lt;- ZenodoManager$new(\n      token = \"your_token\",\n      sandbox = TRUE,\n      logger = \"INFO\")\n\n\nAccess your record:\n\n\n#get your record by querying the metadata, e.g. by title, this will give you a list of all records with that title.\nmyrec &lt;- zenodo$getDepositions(q='title:RiceFarm Statistics')\n    \n#access the first item in the list, as there should only be one record with that particular title\nmyrec &lt;- myrec[[1]]\n\n\nRename your .zip file on your computer\nUpload the renamed .zip file:\n\n\n#edit data, delete_latest_files = TRUE to not include data of previous version in newer version\nmyrec &lt;- zenodo$depositRecordVersion(myrec, delete_latest_files = TRUE, files = \"path/to/your/new/file\", publish = TRUE)\n\n\nAgain, go to https://sandbox.zenodo.org/me/uploads?q=&l=list&p=1&s=10&sort=newest\nActivate “View all versions” on the left hand side.\nCheck if both versions show up"
  },
  {
    "objectID": "exercises/Git_exercise.html",
    "href": "exercises/Git_exercise.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "In this exercise we will show how version control with Git can be implemented for an example R project. The project is the same that is created in the first workflow exercise, however to save time or in case you haven’t completed this exercise we will start with the finished output from it.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\n\n\nDownload Git from https://git-scm.com/downloads\nOnce downloaded open the Git terminal window and type in the following with your credentials\n\ngit config --global user.name \"NVHarisena1\" git config --global user.email \"NVHarisena1\\@ethz.ch\" git config --global --list\nThe third command should return your updated user-name and email id.\n\n\n\nWe will make a quick repository on Github for an individual project, without changing much of the specific configurations since it will be beyond the scope of this workshop.\n\nLogin to your account at https://github.com/\nCreate a new repository by clicking the ‘+’ sign in the top right side of the website or in the ‘Start new repository’ section in the homepage\nProvide a clear name for the repository for e.g. “R_repro_nv” and a quick description like “Test for Reproducible research workshop”\nSet the visibility of the profile to “Public”\nInitialize this repository with: Add a README file.\nSelect a license for your repository in the “Choose your license” section. Check out this website to identify which license works for you. Even though it is optional to add license information to a repository, it is good practice to include this (See more details).\nClick the green ‘create repository’ button\n\n\n\n\nSetting up the repository\n\n\n\n\n\n\nOpen a new session in R studio and create a new project\nIn the ‘New Project Wizard’ navigate to ‘Version Control’&gt;‘Git’\nIn the “repository URL” paste the URL of your new GitHub repository. It will be something like this https://github.com/nvharisena1/R_repro_test.\nAdd folder location where you want the project to be saved locally in your computer in “Create project as subdirectory of” section\nClick ‘Create project’\n\n\n\n\nLinking repository to R-Studio\n\n\nYou will see R-studio has now been set up for your local clone of the project to communicate with your online repository. You can see a drop-down to the right of ‘New Branch’ button in the Git tab. This will show you all the branches available to pull or push data to. Your drop-down should show only a ‘main’ branch, since no new branches were created. As stated in the workflow introduction, creating branches is useful for projects with multiple collaborators or sub-themes. Pushing to different branches and then setting up a ‘pull-request’ to merge to the ‘main’ branch allows for systematic version control of the project.\n\n\n\nR-Studio new project session with git link\n\n\n\n\n\n\n\n\n\n\n\n\nOpen the README.md from the file viewer pane\nAdd a description section for the project with a heading and a describing sentence, for e.g. “This project is is a test”.\nAdd license information for the project, for e.g. “This project is licensed under the terms of the MIT license.”\nSave the file.\n\n\n\nEdit README.md\n\n\n\n\n\n\nThe git ignore functionality tells git which files to ignore while ‘pushing’ the local changes to the remote (online) repository see more details.In this example we will tell git to ignore all .html files. .html files are created when you preview a file, for example click preview on the edited README.md and a .html should be created.\n\nOpen the .gitignore file and add .html in a new line and save the file\n\n\n\n\nEditing the .gitignore file\n\n\n\n\n\n\nClick the Commit button in the Git tab\nCheck all the files listed in the top left section\nWrite a sentence describing the changes i.e. “Updated exercise, data, Readme and .gitignore”\nClick Commit and close the window\nclick Push in the Git tab, a window will pop up showing the interface with the remote system and details of the upload.\n\n\n\n\nCommit changes\n\n\nGreat! You have finished your first project update (local changes committed and pushed) via Git on Rstudio."
  },
  {
    "objectID": "exercises/Git_exercise.html#version-control-with-git-exercise",
    "href": "exercises/Git_exercise.html#version-control-with-git-exercise",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "In this exercise we will show how version control with Git can be implemented for an example R project. The project is the same that is created in the first workflow exercise, however to save time or in case you haven’t completed this exercise we will start with the finished output from it.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\n\n\nDownload Git from https://git-scm.com/downloads\nOnce downloaded open the Git terminal window and type in the following with your credentials\n\ngit config --global user.name \"NVHarisena1\" git config --global user.email \"NVHarisena1\\@ethz.ch\" git config --global --list\nThe third command should return your updated user-name and email id.\n\n\n\nWe will make a quick repository on Github for an individual project, without changing much of the specific configurations since it will be beyond the scope of this workshop.\n\nLogin to your account at https://github.com/\nCreate a new repository by clicking the ‘+’ sign in the top right side of the website or in the ‘Start new repository’ section in the homepage\nProvide a clear name for the repository for e.g. “R_repro_nv” and a quick description like “Test for Reproducible research workshop”\nSet the visibility of the profile to “Public”\nInitialize this repository with: Add a README file.\nSelect a license for your repository in the “Choose your license” section. Check out this website to identify which license works for you. Even though it is optional to add license information to a repository, it is good practice to include this (See more details).\nClick the green ‘create repository’ button\n\n\n\n\nSetting up the repository\n\n\n\n\n\n\nOpen a new session in R studio and create a new project\nIn the ‘New Project Wizard’ navigate to ‘Version Control’&gt;‘Git’\nIn the “repository URL” paste the URL of your new GitHub repository. It will be something like this https://github.com/nvharisena1/R_repro_test.\nAdd folder location where you want the project to be saved locally in your computer in “Create project as subdirectory of” section\nClick ‘Create project’\n\n\n\n\nLinking repository to R-Studio\n\n\nYou will see R-studio has now been set up for your local clone of the project to communicate with your online repository. You can see a drop-down to the right of ‘New Branch’ button in the Git tab. This will show you all the branches available to pull or push data to. Your drop-down should show only a ‘main’ branch, since no new branches were created. As stated in the workflow introduction, creating branches is useful for projects with multiple collaborators or sub-themes. Pushing to different branches and then setting up a ‘pull-request’ to merge to the ‘main’ branch allows for systematic version control of the project.\n\n\n\nR-Studio new project session with git link\n\n\n\n\n\n\n\n\n\n\n\n\nOpen the README.md from the file viewer pane\nAdd a description section for the project with a heading and a describing sentence, for e.g. “This project is is a test”.\nAdd license information for the project, for e.g. “This project is licensed under the terms of the MIT license.”\nSave the file.\n\n\n\nEdit README.md\n\n\n\n\n\n\nThe git ignore functionality tells git which files to ignore while ‘pushing’ the local changes to the remote (online) repository see more details.In this example we will tell git to ignore all .html files. .html files are created when you preview a file, for example click preview on the edited README.md and a .html should be created.\n\nOpen the .gitignore file and add .html in a new line and save the file\n\n\n\n\nEditing the .gitignore file\n\n\n\n\n\n\nClick the Commit button in the Git tab\nCheck all the files listed in the top left section\nWrite a sentence describing the changes i.e. “Updated exercise, data, Readme and .gitignore”\nClick Commit and close the window\nclick Push in the Git tab, a window will pop up showing the interface with the remote system and details of the upload.\n\n\n\n\nCommit changes\n\n\nGreat! You have finished your first project update (local changes committed and pushed) via Git on Rstudio."
  },
  {
    "objectID": "contents/Zenodo_workflow.html",
    "href": "contents/Zenodo_workflow.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "The graphic above shows the main steps of this workflow. It starts by developing your research project as an Rstudio project following the good practice project guidelines we have discussed. Then, it uses the renv package to manage the project environment so that others can re-create it. Finally, the code, data and environment are uploaded to the open-access repository Zenodo, which provides a DOI for your work, ensuring long-term accessibility and reproducibility.\nThe renv package helps maintain the R environment, allowing others to recreate the environment in which your analysis was conducted. By combining renv with Zenodo, you create a comprehensive solution for reproducible research. renv ensures that the computational environment is captured, while Zenodo makes your research outputs accessible and citable, supporting the FAIR principles of findability, accessibility, interoperability, and reusability [1].\nThe next sections provide some additional detail on environment management with renv and some background and practical tips on Zenodo. You will have the chance to apply this workflow to an example project in the accompanying exercise.\n\nEnvironment Management with renv\nrenv is a powerful R package designed to help manage project environments by creating project-specific libraries and lockfiles. As mentioned earlier, renv captures the exact versions of R packages used in a project, storing this information in a renv.lock file. This allows users to recreate the exact package environment when revisiting a project or transferring it to a different machine, ensuring reproducibility.\nThe renv workflow is straightforward:\n\nInitialize renv in a project: renv creates a separate library in the project folder, isolating the packages from the system-wide library.\nSnapshot dependencies: renv scans the project, identifying which packages are being used and recording their versions in the lockfile.\nRestore environments: Anyone cloning or receiving the project can run renv::restore() to install the exact versions of the packages listed in the lockfile from the project library, reproducing the original project package environment.\n\nOne of the core strengths of renv is its flexibility. It integrates seamlessly with tools like RStudio, allowing easy management of dependencies without disrupting existing workflows. This makes it particularly well-suited for ensuring that research projects are reproducible across different systems and platforms.\nHowever, renv does not manage the entire system environment (such as the version of R itself or external dependencies like system libraries). For complete reproducibility, combining renv with\ncontainerization tools (like Docker) or publishing outputs (such as code or data) via repositories like Zenodo is recommended.\n\n\n as a research repository\nZenodo is a platform created under the European Commission‘s OpenAIRE project in partnership with CERN to publish, archive, and share scientific research outputs, including datasets, code, and publications.\nOf course there are many other similar research repositories, such as Dryad, Figshare, Mendeley Data and OSF, but we recommend Zenodo for several reasons:\n\nGenerous upload size of 50GB (100 files) per record\nAligns with FAIR and Open Science principles: The practical features of Zenodo that ensure this are described in it‘s principles\nAbility to create communities: Zenodo Communities are used to group similar records together. This is useful for creating a collection of related research outputs, either for a research group or a large-scale funded project.\nLong term preservation with assignment of DOIs: Each item published on Zenodo is assigned a permanent Digital Object Identifier (DOI), which is a better way than a URL to cite the record in academic writing.\nOpen source: This means that Zenodo is not just free to use but you can even see the code it is built on and contribute to it.\nVersioning functionality: Every record starts with a 1st version and new versions can be added as research is updated, while earlier versions remain accessible. This is crucial in scientific research, where updated analyses and data corrections are often necessary, but also transparency around earlier versions of the work should be maintained.\nIntegration with GitHub: When a research project (e.g., code) is hosted on GitHub, Zenodo can be used to archive the repository upon each new release, creating a snapshot with a DOI. This means that a version of the code can be more easily cited in scientific publications.\nApplication programming Interface (API) to access records programmatically: This a useful feature as it allows for interfacing with Zenodo records without using the website and is the backbone of the zen4R package that allows for publishing records directly from R which we discuss in more detail below.\n\n\n\nPublishing to Zenodo with zen4R\nThe zen4R package [2] provides functions to interact with Zenodo‘s API directly from R. The package allows you to:\n\nRetrieve the metadata of, and download, existing Zenodo records.\nCreate new records and versions of records, write their metadata and upload files to Zenodo.\n\nWe will use zen4R to publish the code, data, and environment of our example project to Zenodo in the accompanying exercise.\n\n\n\n\n\n\n Back to topReferences\n\n1. Wilkinson MD, Dumontier M, Aalbersberg IjJ, et al (2016) The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3(1):160018. https://doi.org/10.1038/sdata.2016.18\n\n\n2. Blondel E, Barde J (2024) zen4R: R Interface to Zenodo REST API"
  },
  {
    "objectID": "contents/Resources.html",
    "href": "contents/Resources.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "[ to  pipeline] https://rstudio.github.io/renv/articles/renv.html\n[ to  pipeline] https://intro2r.com/"
  },
  {
    "objectID": "contents/intro.html",
    "href": "contents/intro.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "We are four researchers from the research group Planning of Landscape and Urban Systems (PLUS) at ETH Zürich. Click on the social icons below our pictures to find out more about our individual research or get in touch with us.\n\n\n\n\n\nDoctoral Researcher\n    \n\n\n\n\n\n\nDoctoral Researcher\n   \n\n\n\n\n\n\nResearch Assistant\n   \n\n\n\n\n\n\nSenior scientist"
  },
  {
    "objectID": "contents/intro.html#about-us",
    "href": "contents/intro.html#about-us",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "We are four researchers from the research group Planning of Landscape and Urban Systems (PLUS) at ETH Zürich. Click on the social icons below our pictures to find out more about our individual research or get in touch with us.\n\n\n\n\n\nDoctoral Researcher\n    \n\n\n\n\n\n\nDoctoral Researcher\n   \n\n\n\n\n\n\nResearch Assistant\n   \n\n\n\n\n\n\nSenior scientist"
  },
  {
    "objectID": "contents/intro.html#what-is-reproducible-research",
    "href": "contents/intro.html#what-is-reproducible-research",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "What is reproducible research?",
    "text": "What is reproducible research?\nReproducibility is a key aspect of reliable scientific research. It enables other researchers to reproduce the same results using the original data, code, and documentation [1]. Below are the core principles to ensure reproducibility in research:\nStarts with planning\nReproducibility begins during the planning stage. It is essential to organize data management and ensure clear protocols are in place even before starting the analysis. Consistent Data Storage Regular backups of data are crucial. Storing data in multiple locations ensures accessibility and minimizes the risk of data loss. [2]\nContains clear documentation\nThorough documentation is essential to guarantee that data and methods can be accurately interpreted and reproduced by others. This entails the use of well-organised files and the inclusion of metadata that describes the data, how it was obtained, and how it was processed. [[2]][3]\nUtilizes version control\nUsing version control systems helps track changes in the project over time. This approach preserves the history of the project and facilitates the reversion of files to a previous state in the event of an error. [2]\nIs accessible\nData should be stored in nonproprietary, portable formats to ensure broad accessibility and long-term usability. This practice ensures that researchers can access the data without relying on specific software tools. Making data and code publicly available in accessible repositories supports scientific transparency and allows broader use of research outputs. [[2]][3]\nBy following these steps, researchers contribute to the wider scientific community, ensuring that their work can be efficiently and accurately reproduced by others.\nIntroducing the FAIR Principles\nWhile the principles above lay the groundwork for reproducibility, the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) provide a more comprehensive framework for enhancing the value of research data in the digital age. These principles expand on reproducibility by emphasizing not only human access to research outputs but also machine actionability, ensuring that data can be effectively found, accessed, and reused by both people and computational tools​. [4]\nHow FAIR Principles Build on Reproducibility\nThe FAIR principles naturally complement and expand on the core aspects of reproducible research:\n\nFindability reinforces the importance of clear documentation. Assigning persistent identifiers and providing rich metadata makes it easier for researchers and search tools to locate and understand datasets, ensuring that your research remains accessible over time.\nAccessibility builds on the concept of using nonproprietary formats. FAIR emphasizes that data should be retrievable using open, standardized protocols, which ensures long-term access to both the data and its metadata, even if the data itself becomes unavailable.\nInteroperability relates to the consistent use of data standards and version control. By using standardized formats and vocabularies, research data can be more easily integrated with other datasets, supporting reuse and long-term relevance in broader research contexts.\nReusability directly aligns with the goals of reproducible research by ensuring that data is accompanied by clear licensing and provenance, allowing others to confidently reuse it. This principle reinforces the need for thorough documentation and transparent methods.\n\nBy incorporating the FAIR principles, researchers ensure that their data not only meets the standards of reproducibility but is also optimized for long-term use and discovery. This fosters a research environment where data is more transparent, accessible, and impactful over time​. [4]"
  },
  {
    "objectID": "contents/intro.html#why-strive-for-reproducible-research",
    "href": "contents/intro.html#why-strive-for-reproducible-research",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Why strive for reproducible research?",
    "text": "Why strive for reproducible research?\nIn recent years, various scientific disciplines have experienced what is known as a “replication crisis”. This crisis arises when researchers are unable to reproduce the headline results of key studies using the reported data and methods [[5]][6] [7]. This lack of reproducibility undermines public trust in science, as it raises doubts about the validity of research findings.\n\nAdvantages of Reproducibility for Your Research\nPersonal Reference\nConducting reproducible research simplifies the process of remembering how and why specific analyses were performed. This makes it easier to explain your work to collaborators, supervisors, and reviewers, enhancing communication throughout your project. [2]\nEfficient\nModifications Reproducible research enables you to quickly adjust analyses and figures when requested by supervisors, collaborators, or reviewers. This streamlined process can save substantial time during revisions. [2]\nStreamlined Future Projects\nBy maintaining well-organized and reproducible systems, you can reuse code and organizational structures for future projects. This reduces the time and effort required for similar tasks in subsequent research. [2]\nDemonstrates Rigor and Transparency\nReproducibility demonstrates scientific rigor and transparency. It allows others to verify your methods and results, improving the peer review process and reducing the risk of errors or accusations of misconduct. [2]\nIncreases Impact and Citations\nMaking your research reproducible can lead to higher citation rates [8] [9]. By sharing your code and data, you enable others to reuse your work, broadening its impact and increasing its relevance in the scientific community. [10] [11].\n\n\nAdvantages of Reproducibility for Other Researchers\nFacilitates Learning\nSharing data and code helps others learn from your work more easily. New researchers can use your data and code as a reference, speeding up their learning curve and improving the quality of their analyses. [2]\nEnables Reproducibility\nReproducible research makes it simpler for others to reproduce and build upon your work, fostering more compatible and robust research across studies. [2]\nError Detection\nBy allowing others to access and review your data and code, reproducibility helps detect and correct errors, ensuring that mistakes are caught early and reducing the chance of their propagation in future research. [2]"
  },
  {
    "objectID": "contents/intro.html#why-for-reproducible-research",
    "href": "contents/intro.html#why-for-reproducible-research",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Why  for reproducible research?",
    "text": "Why  for reproducible research?\nR is increasingly recognized as a powerful tool for ensuring reproducibility in scientific research. Here are some key advantages of using R for reproducible research:\nOpen Source\nAccessibility R is freely available to everyone, eliminating cost barriers and promoting inclusive access to research tools. This open-source model ensures that researchers around the world can use and contribute to its development, fostering a collaborative research environment. [3]\nComprehensive Documentation\nR encourages thorough documentation of the entire research process. This ensures that analyses are well-tracked and can be easily replicated across different projects, enhancing the overall transparency and reliability of the research.\nIntegrated Version Control\nR seamlessly integrates with version control systems like Git, allowing researchers to track changes to code, data, and documents. This helps maintain a detailed record of a project’s evolution and ensures that all steps are easily reproducible. [3]\nConsistency Across Platforms\nR provides a stable environment that works consistently across different operating systems, whether you are using Windows, Mac, or Linux. This cross-platform consistency greatly enhances the reproducibility of research across diverse systems.\nBroad Community Support\nThe R community is large and active, continuously contributing to the improvement of the software. This broad support makes R a reliable choice for long-term research projects, ensuring that new tools and methods are constantly being developed and shared.\nFlexibility and Adaptability\nR offers a wide range of tools and functions that can be adapted to various research needs. This flexibility allows researchers to handle diverse tasks within a reproducible framework, making it a versatile tool for projects of all kinds."
  },
  {
    "objectID": "contents/Docker_workflow.html",
    "href": "contents/Docker_workflow.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "The title of this workflow raises two questions, the first being: what is containerization? and the second: what is Docker? ### Containerisation\nSimply put containerization is the process of bundling code along with all of it’s dependencies, i.e. all the components we discussed as making up the environment, including the operating system, software libraries (packages), and other system software. The fact everything needed to run the code is included means that the code is portable and can be run on any platform or cloud service. This also makes containerization something of a gold standard for reproducibility as the entire environment is explicitly re-produced.\n\nDocker\nDocker is an open-source, and the most popular, platform for containerization. Before we dive into a practical example using Docker for research projects with R it is important to introduce some three key terms that we will come across:\n\nDockerfile: The first step in the containerization process, they are a straightforward text file containing a collection of commands or procedures to create a new Docker Image. In this sense we can consider a Dockerfile are the source code of Docker Image. Importantly, Dockerfiles typically start from a base image, which is a existing Docker Image that your image is extending.\nDocker Image: A read-only file that contains the instructions for creating a Docker Container. Think of an image as the blueprint of what will be in a container when it is running. Docker Images can be shared via Dockerhub, so that they can be used by others.\nDocker Container: Is an actual running instance of a Docker image. It runs completely isolated from the host environment by default and only accesses host files (i.e. data) if it has been configured to do so. It is possible to create multiple containers simultaneously from the same Docker Image, and each container can be started, stopped, moved, and deleted independently of the others.\n\nThe graphic above shows the relationships between these components including the central commands of Docker that connect them build and run.\n\n\nUsing Docker with R\nSo to create a Docker Image to containerize our R research projects we need to start by creating a Dockerfile and, as mentioned above, this should start with a base image. In our case this base image must logically include R and RStudio (if we want to utilise the RStudio Projects features).\nFortunately there is a project that specifically catalogs and manages Docker Images for R projects: Rocker. The images available through the Rocker project not only include different versions of R and RStudio but also images containing collections of R packages for specific purposes (e.g. tidyverse for data wrangling and visualisation, geospatial packages etc.).\nIn terms of actually creating the Dockerfile for our R project, this can be done manually (See a good R-focused tutorialhere), however there are also R packages that can help with this process such as dockerfiler and the [rrtools](https://github.com/benmarwick/rrtools) package.\nFor our exercise of this workflow we will use the dockerfiler package, which creates a custom class object that represents the Dockerfile and has slots corresponding to common elements of Docker images. This allows us to add elements to the dockerfile in a more R-like way. The following code snippet demonstrates adding Maintainer details to a Dockerfile object, before saving it:\n\nlibrary(dockerfiler)\n# Create a dockerfile template\nmy_dock &lt;- Dockerfile$new()\n\n# Add maintainer\nmy_dock$MAINTAINER(\"Jane Doe\", \"jane_doe@gmail.com\")\n\n# Save\nmy_dock$write()\n\n\n\nDocker with renv\nDocker can be used with the renv package to manage the package environment of your project.\nThere are two methods of implementing this which come with their own considerations:\n\nUse renv to install packages when the Docker image is built: This approach is useful if yo plan to have have multiple projects with identical package requirements. This because by creating an image containing this package library you can simply re-use the image as a base for new images for different projects [1].Warning: Restoring the package library (renv::restore()) when building the image will be slow if there are large numbers of packages so try to avoid the need to re-build the base image many times.\nUse renv to install/restore packages only when Docker containers are run: This approach is better when you plan to have multiple projects that are built from the same base image but require different package requirements. Hence it is preferable to not included the package library in the image but instead to mount different project specific libraries to the container when it is run [1]. If project libraries are dynamically provisioned in this way and renv::restore() is run with caching this means that the packages are not re-installed everytime the container is run.\n\n\n\n\n\n\n\n Back to topReferences\n\n1. Ushey K, Wickham H (2024) Using renv with docker"
  },
  {
    "objectID": "contents/Git_workflow.html",
    "href": "contents/Git_workflow.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "Version control software output can have multiple uses including creating a systematic procedure for collaboration, working as a team along with increasing the ease of reproduction of the work by other users or by the original researcher as well. We all know the difficulty of tracing back the workflow of work projects a few months down the line after having shelved it, and it is here that Git and Github can be highly useful ([1]).\n\nAbout Git\nGit allows us to make snapshot or record of the changes undertaken in a script, and store it as with a message that defines the change. In this way even after multiple updates, the history is preserved allowing us to revert, compare and systematically trace back the workflow development.\nGit is useful also for data scientists and researchers that work individually yet want to create systematically reproducible workflows with version control ([1]).\n\n\nGitHub repositories and functionalities\nThe version controlled code and all other auxiliary files related to the project are stored in a Github repository which was created by the user in their account on Github. A repository can either be set as public or private as per the users need for visibility of their work.\nTo help with version control Github repositories provide multiple functionalities like creating ‘branches’, ‘clones’, ‘merging’ multiple branches, setting up a ‘pull request’ before merge etc. As we get into more complicated workflows handled by multiple developers, Github allows many more functionalities as checks and balances to code development. However, we will limit our understanding to what is needed to create work with a version control history allowing for small scale collaboration, but with the main goal of creating reproducible research (see more details).\n\n\nBasic functionalities\n\nCreating a repository and setting up user authorisations: A project repository must first be set up on GitHub as either a private or a public repository. If it is not an individual project, collaborators can be added with appropriate (read or write) permission levels (see more details). It is good to elaborate the ‘Readme’ file so as to help viewers get an idea of the repository (see more details).\nPush and Pull: The data and code related to a project must be cloned from the remote version to a local version before changes are made. Make sure to pull from updated (i.e. merged branches; see below) branches before making changes. Once changes are made the user must ‘commit’ all the correct changes. Once this is done the changed code can be pushed back to the branch.\nBranching and merging : Git allows users to create branches that feed into the ‘main’ branch of a project repository on GitHub. Each branch can be created either for different tasks or for different users as per the requirement. To merge branches into ‘main’ users have to set up a pull-request which needs to be approved by an authorised user.\nGit provides additionally many more functionalities for identifying differences between changed files or between branches, to make temporary commits and to revert back to a certain commit in history. However this is beyond the scope of this workshop.\n\n\n\nGit in R-Studio\nWhen a Github repository is connected to an R project, R-studio adds a ‘Git’ tab (see image below) with ‘push’, ‘pull’, ‘commit’ and ‘diff’ functionalities. We can switch branches to pull from or push to and additionally trace the history of changes in the Github respository by all users.\n\n\n\nGit intergration into R-Studio\n\n\nAlternatively you can also use Github desktop to perform the same functionalities.\n\n\nAdditional functionalities\n\nPublic Github repositories can now be archived on Zenodo as a permanent record of the work with a Digital Object Identifier (DOI) that can be cited in academic work (Click to see more details).\nGitHub further provides advanced functionalities like GitHub Actions that allows the user to automate certain processes for application development or project management.\nGitHub releases can be automatically published on Docker Hub or GitHub packages as part of a Continuous Integration(CI) workflow (see more details).\n\n\n\n\n\n\n Back to topReferences\n\n1. Alexander R (2023) Telling stories with data, 1st Edition"
  },
  {
    "objectID": "contents/Quarto_intro.html",
    "href": "contents/Quarto_intro.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "Quarto is an open source scientific and technical publishing system developed by Posit the same company that also created Rstudio. Quarto allows you to integrate code in multiple programming languages, with written material, and a wide variety of interactive visual components into a range of different document formats. If you are familiar with Rmarkdown then you will find Quarto familiar as it is in many ways an evolution of this.\nThe Quarto website presents many examples of the application of the software but here we will focus on some of it’s key uses and features that are relevant for academics and producing reproducible research.\n\n\n\n\n\n\n\n\n\nWriting academic manuscripts\nWe all know how painful it can be switching between multiple programs to produce academic manuscripts, maybe you write your text in word, and produce your visualizations in R or Python before having to convert the end result to PDF for submission. This is especially annoying when you need to update parts of the manuscript as part of the review process.\nQuarto solves this problem by allowing you to write full academic manuscripts from start to finish including text, code, and visualizations in a single program. This functionality has been expanded even further with the release of Quarto Manuscripts as a project type from Quarto version 1.4.\nSome key benefits of writing your manuscript with Quarto include:\n\nFigures and tables are dynamically updated as your code changes\nSupports the use and inclusion of R, Python and Julia code as well as LaTeX and Markdown\nSimple cross-referencing capability for figures, tables, and sections\nDocuments can be rendered as Word, PDF, or HTML\nEasily include citations and bibliographies from Crossref, DataCite, and PubMed as well as with direct integration with Zotero\nHigh quality formatting options for equations and tables\nQuarto’s .qmd files can be edited with various code/text editors including VS Code, RStudio and more\nTrack changes and collaborate using Git or other version control systems.\n\nWriting your academic manuscripts with Quarto is more reproducible as it allows others to use your underlying manuscript file in combination with your data to directly re-create your results.\nIn one of our exercises you will practice creating a manuscript with Quarto for an example project\n\n\nPresentations with RevealJS\nQuarto also allows you to create presentations in several formats RevealJS, Microsoft Powerpoint and Beamer using a common syntax. In fact the presentation document for this workshop is created using Quarto and RevealJS.\nSome useful features of making presentations with Quarto include:\n\nSelection of pre-existing modern themes with functionality to publish your own theme.\nInclude interactive content: Executable code blocks and visual components (graphs and maps)\nDynamic resizing of content depending on screen size\nFunctionality for slide notes, automatic transitions, timers etc.\nEasy export to PDF or HTML\nSimilar to manuscripts code-based content is dynamically updated.\n\n\n\nWebsites\nQuarto can also be used to create websites that can be freely hosted through Github Pages or other services like Netlify or Posit Connect. This is a great way to share your research with a wider audience or promote your work. Creating Quarto websites is an intuitive and user-friendly process and because it uses the Bootstrap framework there is a lot of guidance available for customizing beyond the 25 default themes\nSome examples from the authors:\n\nThis website is created with Quarto and hosted on Github Pages and you can see all of it’s source code here.\nThis is an example of personal website created by Ben to share publications, presentations and blog content: https://blenback.github.io/\n\n\n\n\nResearchers personal website\n\n\n\nThis is a Multi-lingual website created for a research project in Peru to share progress and results of the project: https://nascent-peru.github.io/.\n\n\n\n\nNASCENT-Peru website\n\n\n\n\nDashboards to demostrate your research output\nQuarto dashboards allow you to arrange multiple interactive or static components in a single page with a highly customizable layout. These components can include text summaries, tables, plots, maps and more. This is a great way to collect feedback on aspects of your research during the development process or to present your results in a visually appealing way.\nHere is an interactive example produced using Python interactive plots:\n\n\n\n\nGapminder dashboard from JJallaire.\n\n\n\n\n\nData exploration and visualization\nAs has been mentioned already Quarto provides a lot of options for creating interactive data visualisations, tables and diagrams using frameworks such as:\n\nPlotly\nLeaflet\nJupyter Widgets\nhtmlwidgets\nObservableJS\nShiny\n\nThere are lots of examples of these on the Quarto website but one we particularly like is Leaflet which allows you to create interactive maps with markers, popups and other features. In a research context this is useful to others to explore the spatial results of your research. Here is a simple example we used to show the locations and timings of workshops we were conducting in Peru.\n\n\nCode\nlibrary(leaflet)\nlibrary(fontawesome)\nlibrary(mapview)\n\n\nnational &lt;- makeAwesomeIcon(text = fa(\"people-roof\"),\n                              iconColor = 'white',\n                              library = 'fa',\n                              markerColor = 'darkgreen')\n  \nregional &lt;- makeAwesomeIcon(text = fa(\"people-roof\"),\n                              iconColor = 'white',\n                              library = 'fa',\n                              markerColor = 'lightgreen')\n\n\n#uncomment for saving map\n#Workshop_locations &lt;- \n  leaflet(options = leafletOptions(zoomControl = FALSE,\n                                 attributionControl=FALSE)) %&gt;%\n  #addProviderTiles(providers$Esri.WorldTerrain) %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addAwesomeMarkers(lng=-77.043,\n                    lat=-12.038,\n                    icon= national,\n                    popup=\"Lima, National Workshop 9th May\",\n                    label = \"Lima, National Workshop, 9th May\",\n                    labelOptions = labelOptions(#noHide = TRUE,\n                                                direction = \"bottom\",\n                                                style = list(\"color\" = \"black\",\n                                                \"font-family\" = \"Roboto\",\n                                                \"font-style\" = \"italic\",\n                                                \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n                                                \"font-size\" = \"12px\",\n                                                \"border-color\" = \"rgba(0,0,0,0.5)\")))%&gt;%\n  addAwesomeMarkers(lng=-69.189, lat=-12.594, icon = regional, popup=\"Puerto Maldonado, Regional Workshop\", label = \"Puerto Maldonado Regional Workshop 28th May\",\n                    labelOptions = labelOptions(#noHide = TRUE,\n                                                direction = \"bottom\",\n                                                style = list(\"color\" = \"black\",\n                                                \"font-family\" = \"Roboto\",\n                                                \"font-style\" = \"italic\",\n                                                \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n                                                \"font-size\" = \"12px\",\n                                                \"border-color\" = \"rgba(0,0,0,0.5)\")))%&gt;%\n  addAwesomeMarkers(lng=-77.529, lat=-9.526, icon = regional, popup=\"Huaraz, Regional Workshop\", label = \"Huaraz, Regional Workshop 4th June\",\n                    labelOptions = labelOptions(#noHide = TRUE,\n                                                direction = \"bottom\",\n                                                style = list(\"color\" = \"black\",\n                                                \"font-family\" = \"Roboto\",\n                                                \"font-style\" = \"italic\",\n                                                \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n                                                \"font-size\" = \"12px\",\n                                                \"border-color\" = \"rgba(0,0,0,0.5)\")))%&gt;%\n  addAwesomeMarkers(lng=-76.370, lat=-6.485, icon = regional, popup=\"Tarapoto, Regional Workshop\", label = \"Tarapoto, Regional Workshop 10th June\",\n                    labelOptions = labelOptions(#noHide = TRUE,\n                                                direction = \"bottom\",\n                                                style = list(\"color\" = \"black\",\n                                                \"font-family\" = \"Roboto\",\n                                                \"font-style\" = \"italic\",\n                                                \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n                                                \"font-size\" = \"12px\",\n                                                \"border-color\" = \"rgba(0,0,0,0.5)\")))%&gt;%\n  addAwesomeMarkers(lng=-80.63282, lat=-5.19449, icon = regional, popup=\"Piura, Regional Workshop\", label=\"Piura, Regional Workshop 20th May\",\n                    labelOptions = labelOptions(#noHide = TRUE,\n                                                direction = \"bottom\",\n                                                style = list(\"color\" = \"black\",\n                                                \"font-family\" = \"Roboto\",\n                                                \"font-style\" = \"italic\",\n                                                \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n                                                \"font-size\" = \"12px\",\n                                                \"border-color\" = \"rgba(0,0,0,0.5)\")))\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/Rprojects.html",
    "href": "contents/Rprojects.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "Let’s start with a definition of what makes a good R project from Jenny Bryan:\nA good R project… “creates everything it needs, in its own workspace or folder, and it touches nothing it did not create.” [1]\nThis is a good definition that contains concepts, such as the notion that projects should be ‘self-contained’. However we add one more caveat to this definition which is that a good R project should explain itself.\nFor the purpose of this workshop we will approach this topic by splitting it up into 6 topics which are highlighted in this graphic:\nAs you move through these you will see that there are areas of overlap and complementarity between them. These topics are also central to the choice of approaches in the three workflows for reproducibility that we will share."
  },
  {
    "objectID": "contents/Rprojects.html#sec-projects",
    "href": "contents/Rprojects.html#sec-projects",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": " projects",
    "text": "projects\nHow many times have you opened an R script and been greeted by this line:\n\nsetwd(\"C:/Users/ben/path/that/only/I/have\")\n\nWhile it is well-intentioned (i.e. avoiding the need to have full paths for all objects that will subsequently be loaded or daved ) the problem with it is obvious: This specific path is only relevant for the author and not other potential users and even for the author it will be invalid if they happen to change computers. The good news is there is a very simple way to avoid having to use setwd() at all by using Rstudio Projects.\nRstudio projects designate new or existing folders as a defined working directory by creating an .RProj file within them. This means that when you open a project the working directory of the Rstudio session will automatically be set to the directory that the .RProj file is located in and the paths of all files in this folder will be relative to this.\nThe .Rproj file can be shared along with the rest of the research project files meaning that others users can easily open the Project to have the same working directory removing the need for those troublesome setwd() lines.\n\nCreating and opening projects\nCreating an Rstudio project is as simple as using File &gt; New Project in the top left and then choosing between creating the Project in a new or existing directory.\nThere are several ways to open a Project:\n\nUsing File &gt; Open Project in the top left of Rstudio.\n\n\n\n\n\n\n\nUsing the drop down menu in the top-right of the Rstudio session.\n\n\n\n\n\n\n\nOutside of R by double clicking on the .Rproj file in the folder.\n\n\n\n\n\n\n\n\nUtilising project specific .Rprofile’s\nAnother useful feature of Rstudio projects is the ability to store project-specific settings using the .Rprofile file which controls the initialisation behaviour of the R session when the project is opened. A useful application of this for reproducible research projects is automatically open a particular script, for example a master script that runs all the code in the project (which is a concept that will discussed under workflow decomposition).\nTo do this the contents of your .Rprofile file would like this:\n\nsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession)\n    # Open the script specificed by the path\n    rstudioapi::navigateToFile('scripts/script_to_open.R', line = -1L, column = -1L)\n}, action = \"append\")\n\nThe easiest way to create and edit .Rprofile files is to use the functions from the package usethis:\n\n# Note the use of scope = \"project\" to create a project specific .Rprofile\nusethis::edit_r_profile(scope = \"project\")"
  },
  {
    "objectID": "contents/Rprojects.html#sec-environment-management",
    "href": "contents/Rprojects.html#sec-environment-management",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Environment management",
    "text": "Environment management\nThese lines of code are also probably familiar from the beginning of many an R script:\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nBut what is wrong with these lines?\nWell firstly, there is no indication of what version of the package is to be installed and hence if the code installing this package is old it may not work with the most recent version of the package (This is less of a problem for well established packages like the Tidyverse but for less common packages, that may see large changes between versions, it could be substantial).\nSecondly, having the user install an unspecified version of a package could also cause dependency conflicts with other packages required by the code. This is because almost all packages have some form of dependency (i.e. they use the functionality of) on other packages. This is shown aptly by the image below which, while out-dated now, showed that in 2014 to install the 7 most popular R packages at the time would actually install 63 packages in total when considering their dependencies.\n\n\n\nPackage dependencies of popular R package [2]\n\n\nHowever the problem is bigger than just packages because when your code runs it is also utilising:\n\nA specific version of R\nA specific operating system\nSpecific versions of system dependencies, i.e. other software in other languages that R packages themselves utilise e.g GDAL for spatial analysis packages like terra.\n\nAll of these things together make up what is known as the ‘environment’ of your code. Hence the process of documenting and managing this environment to is ensure that your code is reproducible (i.e. it not only runs but also consistently produces the same results).\nThere are different approaches to environment management that differ in their complexity and hence maybe suited to some projects and not others. For the purpose of this workshop we will focus on what we have found is one of the most user-friendly ways to manage your package environment (caveat that will be discussed) in R which is the package renv. Below we will introduce this package in more detail as it will form a central part of the three workflows for reproducibility that we present.\n\nCreating reproducible environments with renv\nAs mentioned above renv is an R package that helps you create reproducible environments for your R projects by not only documenting your package environment but also providing functionality to re-create it.\nIt does this by creating project specific libraries (i.e. directories: renv/library) which contain all the packages used by your project. This is different from the default approach to package usage and installation whereby all packages are stored in a single library on your machine (system library). Having separate project libraries means “that different projects can use different versions of packages and installing, updating, or removing packages in one project doesn’t affect any other project.” [3]. In order to make sure that your project uses the project library everytime it is opened renv utilises the functionality of .Rprofile's to set the project library as the default library.\nAnother key process of renv is to create project specific lockfiles (renv.lock) which contain sufficient metadata about each package in the project library so that it can be re-installed on a new machine.\nAs alluded to, renv does a great job of managing your packages but is not intended to manage other aspects of your environment such as: tracking your version of R or your operating system. This is why if you want ‘bullet-proof’ reproducibility renv needs to be used alongside other approaches such as containerization which is the 3rd and most complex workflow we will discuss."
  },
  {
    "objectID": "contents/Rprojects.html#sec-writing-clean-code",
    "href": "contents/Rprojects.html#sec-writing-clean-code",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Writing clean code",
    "text": "Writing clean code\nThe notion of writing ‘clean’ code can be daunting, especially for those new to programming. However, the most important thing to bear in mind is that there is no objective measure that makes code ‘clean’ vs. ‘un-clean’, rather we should of think ‘clean’ coding as the pursuit of making your code easier to read, understand and maintain. Also while we should aspire to writing clean code, it is arguably more important that it functions correctly and efficiently.\nThe central concept of clean coding is that, like normal writing, we should follow a set of rules and conventions. For example, in English a sentence should start with a capital letter and end with a full stop. Unfortunately, in terms of writing R code there is not a single set of conventions that everyone proscribes to, instead there are numerous styles that have been outlined and the important thing is to choose a style and apply it consistently in your coding.\nPerhaps the two most common styles are the Tidyverse style and the Google R style (Which is actually a derivative of the former). Neither style can be said to be the more correct, rather they express opinionated preferences on a series of common topics such as: Object naming, use of assignment operators, spacing, indentation, line length, parentheses placement, etc.\nRather than detail all of these topics here we will focus on just on some related tips that we think are most relevant for scientific research coding, including how to automate the formatting of your code to a particular style. However, we encourage you to go through the different style guides when you have the time.\n\nScript headers\nStarting your scripts with a consistent header containing information about it’s purpose, author/s, creation and modification dates is a great step making your workflow more understandable and hopefully reproducible. There are no rules as to what such a header should look like but this is the style I like to use:\n\n#############################################################################\n## Script_title: Brief description of script purpose\n##\n## Notes: More detailed notes about the script and it's purpose\n##\n## Date created: \n## Author(s):\n#############################################################################\n\nTo save time inserting this header into new scripts you use Rstudio’s Code snippets feature. Code snippets are simply text macros that quickly insert a section of code using a short keyword.\nTo create your own Code snippet go to Tools &gt; Global Options &gt; Code &gt; Edit Snippets and then add a new snippet with your code below it:\n\n\n\n\n\nTo use a code snippet simply start typing the keyword in the script and the auto-completion list will appear then press Tab and the code section will be inserted:\n\n\n\n\n\n\n\nCode sections\nAs you may already know braced ({}) sections of code (i.e. function definitions, conditional blocks, etc.) can be folded to hide their contents in RStudio by clicking on the small triangle in the left margin.\n\n\n\n\n\nHowever, an often overlooked feature is the ability to create named code sections that can be also folded, as well as easily navigated between. These can be used to break longer scripts into a set of discrete regions according to specific parts of the analysis (discussed in more detail later). In this regard, another good tip is to give the resulting sections sequential alphabetical or numerical Pre-fixes. Code sections are created by inserting a comment line that contains at least four trailing dashes (-), equal signs (=), or pound signs (#):\n\n# Section One ---------------------------------\n \n# Section Two =================================\n \n# Section Three #############################\n\nAlternatively you can use the Code &gt; Insert Section command.\nTo navigate between code sections:\n\nUse the Jump To menu available at the bottom of the editor[4]\n\n\n\n\n\n\n\nUse the document outline pane in the top right corner of the source pane\n\n\n\n\n\n\n\n\nAutomating the styling of your code\nThere are two R packages that are very helpful in terms of ensuring your code confirms to a consistent style: lintr and styler.\n\nlintr checks your code for common style issues and potential programming errors then presents them to you to correct, think of it like doing a ‘spellcheck’ on a written document.\nstyler is more active in the sense that it automatically format’s your code to a particular style, the default of which is the tidyverse style.\n\nTo use lintr and styler you call their functions like any package but styler can also be used through the Rstudio Addins menu below the Navigation bar as shown in this gif:\n\n\n\n\n\nAnother very useful feature of both packages is that they can be used as part of a continuous integration (CI) workflow using a version control application like Git. This is a topic that we will cover as part of our Version control with Git workflow but what it means is that the styler and lintr functions are run automatically when you push your code to a remote repository."
  },
  {
    "objectID": "contents/Rprojects.html#sec-workflow-decomposition",
    "href": "contents/Rprojects.html#sec-workflow-decomposition",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Workflow decomposition",
    "text": "Workflow decomposition\nIn computer sciences workflow decomposition refers to the structuring or compartmentalising of your code into seperate logical parts that makes it easier to maintain [5].\nIn terms of coding scientific research projects many of us probably already instinctively do decomposition to some degree by splitting typical processes such as data preparation, statistical modelling, analysis of results and producing final visualizations.\nHowever this is not always realized in the most understandable way, for example we may have seperate scripts with logical sounding names like: Data_prep.R and Data_analysis.R but can others really be expected to know exactly which order these must be run in, or indeed whether they even need to be run sequentially at all?\nA good 1st step to remedying this is to give your scripts sequential numeric tags in their names, e.g. 01_Data_prep.R, 02_Data_analysis.R. This will also ensure that they are presented in numerical order when placed in a designated directory Structuring your project directory and can be explicitly described in your project documentation.\nBut you can take this to the next level by creating a Master script that sources your other scripts in sequence (think of them as sub-scripts) so that users of your code need only run one script. To do this is as simple as creating the master script as you would any normal R script (File &gt; New File &gt; R script) and then using the base::source() function to run the sub-scripts:\n\n#############################################################################\n## Master_script: Run steps of research project in order\n##\n## Date created: 30/7/2024\n## Author(s): Jane Doe\n#############################################################################\n\n### =========================================================================\n### A- Prepare dependent variable data\n### =========================================================================\n\n#Prepare LULC data\nsource(\"Scripts/Preparation/Dep_var_dat_prep.R\", local = scripting_env)\n\n### =========================================================================\n### B- Prepare independent variable data\n### =========================================================================\n\n#Prepare predictor data\nsource(\"Scripts/Preparation/Ind_var_data_prep.R\", local = scripting_env)\n\n### =========================================================================\n### C- Perform statisical modelling\n### =========================================================================\n\nsource(\"Scripts/Modelling/Fit_stat_models.R\", local = scripting_env)\n\nAs you can see in this example code I have also made use of a script header and code sections, that were previously discussed, to make the division of sub-processes even clearer. Another advantage of this approach is that all sub-scripts can utilise the same environment (defined by the source(local= ) argument) which means that each individual script does not need to load packages or paths as objects.\nFinally, within your sub-scripts processes should also be seperated into code sections and ideally any repetitive tasks should be performed with custom functions which again are contained within their own files.\nFollowing this approach you end up with a workflow that will look something like this:\n\nThe benefit of this hierarchical approach to structuring is that it is not only easier to debug and maintain individual processes but it is also more amenable to adding new processes."
  },
  {
    "objectID": "contents/Rprojects.html#sec-structuring",
    "href": "contents/Rprojects.html#sec-structuring",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Structuring your project directory",
    "text": "Structuring your project directory\nSimilar to having clean code, having a clean project directory that has well-organised sub-directories goes a long way towards making your projects code easier to understand for others. For software development there are numerous sets of conventions for directories structures although these are not always so applicable for scientific research projects. However we can borrow some basic principles, try to use: - Use logical naming - Stick to a consistent style, i.e. use of captialisation and seperators - Make use of nested sub-directories e.g data/raw/climatic/precipitation/2020/precip_2020.rds vs. data/precip_2020_raw.rds. This is very helpful when it comes to programatically constructing file paths especially in projects with a lot of data.\nAs an example my go-to project directory structure looks like this:\n\n└── my_project\n    ├── data # The research data\n    │   ├── raw\n    │   └── processed\n    ├── output # Storing results\n    ├── publication # Containing the academic manuscript of the project\n    ├── src # For all files that perform operations in the project\n    │   ├── scripts\n    │   └── functions\n    └── tools # Auxilliary files and settings\n\nRather than manually create this directory structure everytime you start a new project, save yourself some time and automate it by using Rstudio’s Project Templates functionality. This allows you to select a custom template as an option when creating a new Rstudio project through the New project wizard (File &gt; New Project &gt; New Directory &gt; New Project Template).\nTo implement this even as an intermediate R user is fairly labor intensive as your custom project directory template needs to be contained within an R-package, in order to be available in the wizard. However, quite a few templates with directory structures appropriate for scientific research projects have been created by others:\n\nrrtools\nProjectTemplate\ntemplate\naddinit (Not a template but an interactive shiny add-in for project creation)"
  },
  {
    "objectID": "contents/Rprojects.html#sec-documentation",
    "href": "contents/Rprojects.html#sec-documentation",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Project documentation",
    "text": "Project documentation\nAs an example of why documentation is important think about if you bought a new table from Ikea only to excitedly rip open the box and find that there are no instructions for how to assemble it. Sure, you know what a table is supposedly to look like and given enough time you will end up with something that will probably be mostly right but maybe it’s missing small details. Also it will probably take you just as long to take it apart in 5 years time. Well, working with undocumented code for research projects is similar except a lot more complicated!\nWriting comprehensive documentation that covers all aspects of our projects is time-consuming which is why it is often neglected. For example, there are a lot of different metadata conventions that exist that you could apply. However, learning and adhering strictly to these can be overwhelming and possibly lead to the opposite effect i.e. they are not simple for others to understand either.\nIn response to this there has been a movement in the R research community to adopt the research as package approach, which, as the name suggests, involves creating your project as an R-package which has a strict set of conventions for documentation [6]. This is a viable approach for those who are familiar with R-packages but is arguably not the best for all projects and users.\nInstead, we would suggest to follow the maxim of not letting the perfect be the enemy of the good and to focus on these key areas:\n\nProvide adequate in-script commentary: This is perhaps contentious for those from a software development community, but given the choice I would rather have to read through a script with too many comments than one with too few. However remember that comments should be used to explain the purpose of the code, not what the code is doing. In line with this use script headers.\nDocument your functions with roxygen skeletons:\nInclude a README file: README files are where you should document your project at the macro-level i.e. what it is about and how it is supposed to work.\n\nThe latter of these two are more detailed so we have provided further information and tips in sections below.\n\nFunction documentation with roxygen2\nBase R provides a standard way of documenting a package where each function is documented in an .Rd file (R documentation). This documentation uses a custom syntax to detail key aspects of the functions such as their input parameters, outputs and any package dependencies [7].\nIn the case of many research projects you will not be creating a package however it is still useful to apply this documentation style to your functions as it is a good way to make them understandable and easier to modify by others. For example, having clear information about the object (e.g. a vector or data.frame) that a function accepts, saves others time in guessing what the function is expecting if they are trying to use new data.\nHowever, rather than manually writing .Rd files, we can use the roxygen2 package to automatically generate these files from a block of comments that are added to the top of the function scripts. To add this comment block, place your cursor inside a function you want to document and press Ctrl + Shift + R (or Cmd + Shift + R on Mac) or you can go to code tools &gt; insert roxygen skeleton (code tools is represented by the wand icon in the top row of the source pane). As you can see in this gif below, when you insert the roxygen block it will already contain the names of the function, its arguments and any returns. You can then fill in the rest of the information, such as the description and dependencies etc. for a guide to these other fields see the roxygen2 documentation.\n\n\n\nInserting roxygen block\n\n\n\n\nTips for README writing\nIf you look at the source code of R packages or projects that use R in Github repositories you will see that they all contain README.md files. .md is Markdown format which is the most common format for README files in R projects because it can be read by many programs and rendered in a variety of formats. These files are often accompanied by the corresponding file README.Rmd which generates the README.md file. In this sense writing the README for your project in markdown makes sense and there tools available to help you do this such as the usethis package which has a function use_readme_rmd() that will create a README.Rmd file for you. However, depending on who you anticipate using your project you may also want to create your README as a raw text file (.txt) which may be a more familiar format for some users and again can be opened by many different programs.\nAgain there is not a single standardised format for what should be included in your README file but here is an example of a README file that was written for one of the authors code/data upload alongside a publication: README.txt\nYou will see that one of the things this README includes is a tree diagram which shows the directory structure of the project right down to the file level. This is a useful way to give an overview of what users should find included in the project and then explanatory notes can be added to explain the purpose of each file or directory. Such a diagram can be easily generated using the fs package:\n\ninstall.packages(\"fs\")\nlibrary(fs)\n\n#vector path of the target directory to make a file tree from\nTarget_dir &lt;- \"YOUR DIR\"\n\n#produce tree diagram of directory sub-dirs and files and save output using capture.ouput from base R utils.\ncapture.output(dir_tree(Target_dir), file= 'Dir_tree_output.txt')"
  },
  {
    "objectID": "exercises/Docker_exercise.html",
    "href": "exercises/Docker_exercise.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "In this exercise we will create and run a Docker container for an example R project. The project is the same that is created in the first workflow exercise, however to save time or in case you haven’t completed this exercise we will start with the finished output from it.\nWarning: Docker is a complex software and getting Docker Desktop running on different machines is not always smooth. For example, I had no problem getting it running on my desktop computer but my work laptop did not have the capabilities. If you do run into issues there is good support available online but also asking for help from your IT department may be a good idea.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\n\n\nClick here to download the resources for the exercise:  Download resources for exercise\nUnzip the downloaded file and move the folder to a location on your computer where you can easily find it.\n\n\n\n\n\nDownload Docker Desktop for your operating system from the Docker website.\nOnce downloaded run the installer like you would for other software. If your computer is managed by your institution or your employer you will likely need an admin account to run the installer and you may need to restart your computer after installation.\nWhile you are running the installer it is useful to make a Docker account. This is not necessary but can be useful for managing your containers. You can also sign in with your GitHub account.\n\n\n\n\n\nOpen the Docker desktop app. If the app does not open you may need to yourself to the program user-group on your computer. This is a common issue on Windows machines because only the admin account is added to the user-group by default. To add yourself to the user-group search computer management in the start menu and right-click and select to run it with admin privileges. Then navigate to Local Users and Groups -&gt; Groups -&gt; Docker Users. Right click on Docker Users and select Add to Group. Then add your user account to the group.\nOnce the Docker desktop app is open it should automatically start the docker engine which is the software that runs the containers. In the bottom left of app window you will see the status of the engine.\n\n\n\n\nDocker Engine status in app\n\n\nAlternatively if you look in the system tray on Windows or the top menu bar on Mac. You will see an icon of the Docker whale logo and if you click on this you can see the status of the engine.\n\n\n\nDocker Engine status in system try\n\n\n\n\n\n\nOpen Rstudio and navigate to the folder you downloaded in step 1.\nCreate a new R script and name it Create_Dockerfile.R.\nInstall the Dockerfiler package: install.packages(\"dockerfiler\").\nAdd the following code to the script and replace the entries with your details:\n\n\n# Load dockerfiler package\nlibrary(dockerfiler)\n\n# Get your R version to select a base image to use for your image/container\nR.Version()$version.string \n\n# Create a dockerfile template object using the Dockerfile class from the\n# dockerfiler package and specify your version of R in the base image name\n# my version is 4.3.1 hence the base image is rocker/r-ver:4.3.1\n# but you should replace the end of this string with your version number from above\nRiceFarm_dock &lt;- Dockerfile$new(FROM = \"rocker/r-ver:4.3.1\")\n\n# Add maintainer information (replace with your details)\nRiceFarm_dock$MAINTAINER(\"Your_name\", \"Your_email\")\n\n# By default docker images contain a home directory and because our project\n# is simple we will move the files we need there\n\n# Copy the data directory \n# (1st argument is the source, 2nd is the destination in the container)\nRiceFarm_dock$COPY(\"/Data\", \"/home/Data\")\n\n# Copy the scripts directory\nRiceFarm_dock$COPY(\"/Scripts\", \"/home/Scripts\")\n\n# Copy the master script\nRiceFarm_dock$COPY(\"/RiceFarm_master.R\", \"/home\")\n\n# For our project we need \"ggplot2\" and \"stringr\" packages\n# We could try to find a base image on Rocker that has these installed\n# But because we are not using lots of packages lets just install them in the container\n# Note that the R commands are wrapped in `r()` which is a helper function from dockerfiler\n# that then wraps the command in the correct syntax for the Dockerfile\nRiceFarm_dock$RUN(r(install.packages(\"ggplot2\")))\nRiceFarm_dock$RUN(r(install.packages(\"stringr\")))\n\n# Add the command to run the master script\n# Note the use of `Rscript` which is the command line tool included with R to run scripts\nRiceFarm_dock$CMD(\"Rscript /home/RiceFarm_master.R \")\n\n# Save the Dockerfile\nRiceFarm_dock$write()\n\n# Create dir in the host directory to receive the results from container\ndir.create(\"/output\")\n\n\nAfter running this code you will see that a Dockerfile has been created in the directory where you downloaded the resources.\n\n\n\n\n\nThe Docker command build is used to create a Docker image from the instructions contained in your Dockerfile.\nThe build command should be called through a Command Line Interface (CLI) such as the terminal in Rstudio or the CLI of your operating system (e.g Command Prompt for Windows).\nIn Rstudio switch to the terminal tab next to the console pane:\n\n\n\n\nRstudio terminal\n\n\n\nRun the following command: docker build  -t ricerarm_01 . Note: The -t flag is used to tag the image with a name (in this case we are using ricerarm_01). The . at the end of the command is used to specify the current directory as the location of Dockerfile that is to be used.\nAfter running the command you will see the Docker engine pulling the base image from the Docker Hub and then building the image. This process can take a few minutes depending on the size of the base image and the number of packages you are installing. The output in the terminal will look something like this:\n\n\n\n\nDocker build output in terminal\n\n\n\nOnce the image has been built you can check that it is there by running the command docker images in the terminal. This will show you a list of all the images on your computer. You should see the image you just created in the list.\nAlternatively you can check the image in the Docker desktop app. You will see the image in the list of images on the left of the app window. You can inspect the image by clicking on it and see the details of the image:\n\n\n\n\nDocker build output in app\n\n\n\n\n\n\nThe Docker command run is used to run a container from an image.\nThis can be done through the CLI: In the terminal tab in Rstudio run the following command: docker run ricerarm_01.\nOr in the Docker desktop app: Click on the image you want to run and then click the run button in the top right. This will open a window where you can specify the settings for the container but for now you should just run the container with the default settings.\n\n\n\n\nDocker images in app\n\n\n\nAfter running the container you can also check the status of the container in the Docker desktop app. You will see the container in the list of containers on the left of the app window. You can inspect the container by clicking on it and see the details of the container.\n\n\n\n\nDocker containers in app\n\n\n\n\n\n\nOne way to access the files created inside your container is to mount a directory from your host machine to a directory in the the container. This is done using the -v flag in the docker run command. However, this is not so effective in the example container we are using because the code is completed in a matter of seconds and after that the container is exited.\nInstead we will copy the output files from the container using the CLI. To do this you need to know the container ID of the container you want to copy files from. You can get the container ID by running the command: docker ps -a in the terminal, this will show you a list of all the containers on your computer and in the terminal output you can copy the ID:\n\n\n\n\nDocker container ID in terminal\n\n\n\nNow you have the ID in the terminal run the command: docker cp &lt;Container ID&gt;:/home/Output/Visualisations/Regional_size_summary_bar.png ./Output/ and replace the &lt;Container ID&gt; with the ID you copied. The first argument /home/Output/Visualisations/Regional_size_summary_bar.png is the path to the file you want to copy in the container. The second argument ./Output/ is the path to the directory to copy the file to on your host machine, again this is a relative path and the . specifies the current directory. After running the command you should a message printed in the terminal and the file should be copied to the directory you specified:\n\n\n\n\nDocker copy output in terminal\n\n\nThat’s it, you have successfully created a Dockerfile, Docker image and container, run your code inside the container and copied the output back to your host machine. If you were to share the Dockerfile with someone else they could build the image and run the container on their machine and get exactly the same results as you. Obviously this is a very simple example but the same principles apply to more complex projects where reproducibility becomes more challenging."
  },
  {
    "objectID": "exercises/Docker_exercise.html#sec-docker_exercise",
    "href": "exercises/Docker_exercise.html#sec-docker_exercise",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "In this exercise we will create and run a Docker container for an example R project. The project is the same that is created in the first workflow exercise, however to save time or in case you haven’t completed this exercise we will start with the finished output from it.\nWarning: Docker is a complex software and getting Docker Desktop running on different machines is not always smooth. For example, I had no problem getting it running on my desktop computer but my work laptop did not have the capabilities. If you do run into issues there is good support available online but also asking for help from your IT department may be a good idea.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\n\n\nClick here to download the resources for the exercise:  Download resources for exercise\nUnzip the downloaded file and move the folder to a location on your computer where you can easily find it.\n\n\n\n\n\nDownload Docker Desktop for your operating system from the Docker website.\nOnce downloaded run the installer like you would for other software. If your computer is managed by your institution or your employer you will likely need an admin account to run the installer and you may need to restart your computer after installation.\nWhile you are running the installer it is useful to make a Docker account. This is not necessary but can be useful for managing your containers. You can also sign in with your GitHub account.\n\n\n\n\n\nOpen the Docker desktop app. If the app does not open you may need to yourself to the program user-group on your computer. This is a common issue on Windows machines because only the admin account is added to the user-group by default. To add yourself to the user-group search computer management in the start menu and right-click and select to run it with admin privileges. Then navigate to Local Users and Groups -&gt; Groups -&gt; Docker Users. Right click on Docker Users and select Add to Group. Then add your user account to the group.\nOnce the Docker desktop app is open it should automatically start the docker engine which is the software that runs the containers. In the bottom left of app window you will see the status of the engine.\n\n\n\n\nDocker Engine status in app\n\n\nAlternatively if you look in the system tray on Windows or the top menu bar on Mac. You will see an icon of the Docker whale logo and if you click on this you can see the status of the engine.\n\n\n\nDocker Engine status in system try\n\n\n\n\n\n\nOpen Rstudio and navigate to the folder you downloaded in step 1.\nCreate a new R script and name it Create_Dockerfile.R.\nInstall the Dockerfiler package: install.packages(\"dockerfiler\").\nAdd the following code to the script and replace the entries with your details:\n\n\n# Load dockerfiler package\nlibrary(dockerfiler)\n\n# Get your R version to select a base image to use for your image/container\nR.Version()$version.string \n\n# Create a dockerfile template object using the Dockerfile class from the\n# dockerfiler package and specify your version of R in the base image name\n# my version is 4.3.1 hence the base image is rocker/r-ver:4.3.1\n# but you should replace the end of this string with your version number from above\nRiceFarm_dock &lt;- Dockerfile$new(FROM = \"rocker/r-ver:4.3.1\")\n\n# Add maintainer information (replace with your details)\nRiceFarm_dock$MAINTAINER(\"Your_name\", \"Your_email\")\n\n# By default docker images contain a home directory and because our project\n# is simple we will move the files we need there\n\n# Copy the data directory \n# (1st argument is the source, 2nd is the destination in the container)\nRiceFarm_dock$COPY(\"/Data\", \"/home/Data\")\n\n# Copy the scripts directory\nRiceFarm_dock$COPY(\"/Scripts\", \"/home/Scripts\")\n\n# Copy the master script\nRiceFarm_dock$COPY(\"/RiceFarm_master.R\", \"/home\")\n\n# For our project we need \"ggplot2\" and \"stringr\" packages\n# We could try to find a base image on Rocker that has these installed\n# But because we are not using lots of packages lets just install them in the container\n# Note that the R commands are wrapped in `r()` which is a helper function from dockerfiler\n# that then wraps the command in the correct syntax for the Dockerfile\nRiceFarm_dock$RUN(r(install.packages(\"ggplot2\")))\nRiceFarm_dock$RUN(r(install.packages(\"stringr\")))\n\n# Add the command to run the master script\n# Note the use of `Rscript` which is the command line tool included with R to run scripts\nRiceFarm_dock$CMD(\"Rscript /home/RiceFarm_master.R \")\n\n# Save the Dockerfile\nRiceFarm_dock$write()\n\n# Create dir in the host directory to receive the results from container\ndir.create(\"/output\")\n\n\nAfter running this code you will see that a Dockerfile has been created in the directory where you downloaded the resources.\n\n\n\n\n\nThe Docker command build is used to create a Docker image from the instructions contained in your Dockerfile.\nThe build command should be called through a Command Line Interface (CLI) such as the terminal in Rstudio or the CLI of your operating system (e.g Command Prompt for Windows).\nIn Rstudio switch to the terminal tab next to the console pane:\n\n\n\n\nRstudio terminal\n\n\n\nRun the following command: docker build  -t ricerarm_01 . Note: The -t flag is used to tag the image with a name (in this case we are using ricerarm_01). The . at the end of the command is used to specify the current directory as the location of Dockerfile that is to be used.\nAfter running the command you will see the Docker engine pulling the base image from the Docker Hub and then building the image. This process can take a few minutes depending on the size of the base image and the number of packages you are installing. The output in the terminal will look something like this:\n\n\n\n\nDocker build output in terminal\n\n\n\nOnce the image has been built you can check that it is there by running the command docker images in the terminal. This will show you a list of all the images on your computer. You should see the image you just created in the list.\nAlternatively you can check the image in the Docker desktop app. You will see the image in the list of images on the left of the app window. You can inspect the image by clicking on it and see the details of the image:\n\n\n\n\nDocker build output in app\n\n\n\n\n\n\nThe Docker command run is used to run a container from an image.\nThis can be done through the CLI: In the terminal tab in Rstudio run the following command: docker run ricerarm_01.\nOr in the Docker desktop app: Click on the image you want to run and then click the run button in the top right. This will open a window where you can specify the settings for the container but for now you should just run the container with the default settings.\n\n\n\n\nDocker images in app\n\n\n\nAfter running the container you can also check the status of the container in the Docker desktop app. You will see the container in the list of containers on the left of the app window. You can inspect the container by clicking on it and see the details of the container.\n\n\n\n\nDocker containers in app\n\n\n\n\n\n\nOne way to access the files created inside your container is to mount a directory from your host machine to a directory in the the container. This is done using the -v flag in the docker run command. However, this is not so effective in the example container we are using because the code is completed in a matter of seconds and after that the container is exited.\nInstead we will copy the output files from the container using the CLI. To do this you need to know the container ID of the container you want to copy files from. You can get the container ID by running the command: docker ps -a in the terminal, this will show you a list of all the containers on your computer and in the terminal output you can copy the ID:\n\n\n\n\nDocker container ID in terminal\n\n\n\nNow you have the ID in the terminal run the command: docker cp &lt;Container ID&gt;:/home/Output/Visualisations/Regional_size_summary_bar.png ./Output/ and replace the &lt;Container ID&gt; with the ID you copied. The first argument /home/Output/Visualisations/Regional_size_summary_bar.png is the path to the file you want to copy in the container. The second argument ./Output/ is the path to the directory to copy the file to on your host machine, again this is a relative path and the . specifies the current directory. After running the command you should a message printed in the terminal and the file should be copied to the directory you specified:\n\n\n\n\nDocker copy output in terminal\n\n\nThat’s it, you have successfully created a Dockerfile, Docker image and container, run your code inside the container and copied the output back to your host machine. If you were to share the Dockerfile with someone else they could build the image and run the container on their machine and get exactly the same results as you. Obviously this is a very simple example but the same principles apply to more complex projects where reproducibility becomes more challenging."
  },
  {
    "objectID": "exercises/Quarto-manuscript_exercise.html",
    "href": "exercises/Quarto-manuscript_exercise.html",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "Exercise content coming soon!"
  },
  {
    "objectID": "exercises/Quarto-manuscript_exercise.html#write-a-paper-with-quarto",
    "href": "exercises/Quarto-manuscript_exercise.html#write-a-paper-with-quarto",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "Because Quarto is bundled with Rstudio if you have a recent version of Rstudio you may already have Quarto installed. If not, you can install Quarto from CRAN with the following command:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "",
    "text": "This image highlights some the key concepts we will discuss in the workshop, which have been divided into seperate sections:"
  },
  {
    "objectID": "index.html#about-us",
    "href": "index.html#about-us",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "About us",
    "text": "About us\nWe are four researchers from the research group Planning of Landscape and Urban Systems (PLUS) at ETH Zürich. Click on the social icons below our pictures to find out more about our individual research or get in touch with us.\n\n\n\n\nBen Black\nDoctoral Researcher\n    \n\n\n\n\n\nNivedita Harisena\nDoctoral Researcher\n   \n\n\n\n\n\nManuel Kurmann\nResearch Assistant\n   \n\n\n\n\n\nMaarten Van Strien\nSenior scientist"
  },
  {
    "objectID": "index.html#what-is-reproducible-research",
    "href": "index.html#what-is-reproducible-research",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "What is reproducible research?",
    "text": "What is reproducible research?\nReproducibility is a key aspect of reliable scientific research. It enables other researchers to reproduce the same results using the original data, code, and documentation [1]. Below are the core principles to ensure reproducibility in research:\n\n1. Essawy BT, Goodall JL, Voce D, et al (2020) A taxonomy for reproducible and replicable research in environmental modelling. Environmental Modelling & Software 134:104753. https://doi.org/10.1016/j.envsoft.2020.104753\nStarts with planning\nReproducibility begins during the planning stage. It is essential to organize data management and ensure clear protocols are in place even before starting the analysis. Consistent Data Storage Regular backups of data are crucial. Storing data in multiple locations ensures accessibility and minimizes the risk of data loss. [2]\nContains clear documentation\nThorough documentation is essential to guarantee that data and methods can be accurately interpreted and reproduced by others. This entails the use of well-organised files and the inclusion of metadata that describes the data, how it was obtained, and how it was processed. [[2]][3]\nUtilizes version control\nUsing version control systems helps track changes in the project over time. This approach preserves the history of the project and facilitates the reversion of files to a previous state in the event of an error. [2]\nIs accessible\nData should be stored in nonproprietary, portable formats to ensure broad accessibility and long-term usability. This practice ensures that researchers can access the data without relying on specific software tools. Making data and code publicly available in accessible repositories supports scientific transparency and allows broader use of research outputs. [[2]][3]\nBy following these steps, researchers contribute to the wider scientific community, ensuring that their work can be efficiently and accurately reproduced by others.\nIntroducing the FAIR Principles\nWhile the principles above lay the groundwork for reproducibility, the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) provide a more comprehensive framework for enhancing the value of research data in the digital age. These principles expand on reproducibility by emphasizing not only human access to research outputs but also machine actionability, ensuring that data can be effectively found, accessed, and reused by both people and computational tools​. [4]\nHow FAIR Principles Build on Reproducibility\nThe FAIR principles naturally complement and expand on the core aspects of reproducible research:\n\nFindability reinforces the importance of clear documentation. Assigning persistent identifiers and providing rich metadata makes it easier for researchers and search tools to locate and understand datasets, ensuring that your research remains accessible over time.\nAccessibility builds on the concept of using nonproprietary formats. FAIR emphasizes that data should be retrievable using open, standardized protocols, which ensures long-term access to both the data and its metadata, even if the data itself becomes unavailable.\nInteroperability relates to the consistent use of data standards and version control. By using standardized formats and vocabularies, research data can be more easily integrated with other datasets, supporting reuse and long-term relevance in broader research contexts.\nReusability directly aligns with the goals of reproducible research by ensuring that data is accompanied by clear licensing and provenance, allowing others to confidently reuse it. This principle reinforces the need for thorough documentation and transparent methods.\n\nBy incorporating the FAIR principles, researchers ensure that their data not only meets the standards of reproducibility but is also optimized for long-term use and discovery. This fosters a research environment where data is more transparent, accessible, and impactful over time​. [4]"
  },
  {
    "objectID": "index.html#why-strive-for-reproducible-research",
    "href": "index.html#why-strive-for-reproducible-research",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Why strive for reproducible research?",
    "text": "Why strive for reproducible research?\nIn recent years, various scientific disciplines have experienced what is known as a “replication crisis”. This crisis arises when researchers are unable to reproduce the headline results of key studies using the reported data and methods [[5]][6] [7]. This lack of reproducibility undermines public trust in science, as it raises doubts about the validity of research findings.\n\n5. Moonesinghe R, Khoury MJ, Janssens ACJW (2007) Most Published Research Findings Are False—But a Little Replication Goes a Long Way. PLOS Medicine 4(2):1–4. https://doi.org/10.1371/journal.pmed.0040028\n\n6. Collaboration OS (2015) Estimating the reproducibility of psychological science. Science 349(6251):aac4716. https://doi.org/10.1126/science.aac4716\n\n7. Bohannon J (2015) Many psychology papers fail replication test. Science 349(6251):910–911. https://doi.org/10.1126/science.349.6251.910\n\nAdvantages of Reproducibility for Your Research\nPersonal Reference\nConducting reproducible research simplifies the process of remembering how and why specific analyses were performed. This makes it easier to explain your work to collaborators, supervisors, and reviewers, enhancing communication throughout your project. [2]\nEfficient\nModifications Reproducible research enables you to quickly adjust analyses and figures when requested by supervisors, collaborators, or reviewers. This streamlined process can save substantial time during revisions. [2]\nStreamlined Future Projects\nBy maintaining well-organized and reproducible systems, you can reuse code and organizational structures for future projects. This reduces the time and effort required for similar tasks in subsequent research. [2]\nDemonstrates Rigor and Transparency\nReproducibility demonstrates scientific rigor and transparency. It allows others to verify your methods and results, improving the peer review process and reducing the risk of errors or accusations of misconduct. [2]\nIncreases Impact and Citations\nMaking your research reproducible can lead to higher citation rates [8] [9]. By sharing your code and data, you enable others to reuse your work, broadening its impact and increasing its relevance in the scientific community. [10] [11].\n\n8. Piwowar HA, Day RS, Fridsma DB (2007) Sharing Detailed Research Data Is Associated with Increased Citation Rate. PLOS ONE 2(3):1–5. https://doi.org/10.1371/journal.pone.0000308\n\n9. McKiernan EC, Bourne PE, Brown CT, et al (2016) How open science helps researchers succeed. eLife 5:e16800. https://doi.org/10.7554/eLife.16800\n\n10. Whitlock MC (2011) Data archiving in ecology and evolution: Best practices. Trends in Ecology & Evolution 26(2):61–65. https://doi.org/https://doi.org/10.1016/j.tree.2010.11.006\n\n11. Culina A, Crowther TW, Ramakers JJC, Gienapp P, Visser ME (2018) How to do meta-analysis of open datasets. Nature Ecology & Evolution 2(7):1053–1056. https://doi.org/10.1038/s41559-018-0579-2\n\n\nAdvantages of Reproducibility for Other Researchers\nFacilitates Learning\nSharing data and code helps others learn from your work more easily. New researchers can use your data and code as a reference, speeding up their learning curve and improving the quality of their analyses. [2]\nEnables Reproducibility\nReproducible research makes it simpler for others to reproduce and build upon your work, fostering more compatible and robust research across studies. [2]\nError Detection\nBy allowing others to access and review your data and code, reproducibility helps detect and correct errors, ensuring that mistakes are caught early and reducing the chance of their propagation in future research. [2]\n\n2. Alston JM, Rick JA (2021) A Beginner’s Guide to Conducting Reproducible Research. The Bulletin of the Ecological Society of America 102(2):e01801. https://doi.org/10.1002/bes2.1801"
  },
  {
    "objectID": "index.html#why-for-reproducible-research",
    "href": "index.html#why-for-reproducible-research",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Why  for reproducible research?",
    "text": "Why  for reproducible research?\nR is increasingly recognized as a powerful tool for ensuring reproducibility in scientific research. Here are some key advantages of using R for reproducible research:\nOpen Source\nAccessibility R is freely available to everyone, eliminating cost barriers and promoting inclusive access to research tools. This open-source model ensures that researchers around the world can use and contribute to its development, fostering a collaborative research environment. [3]\nComprehensive Documentation\nR encourages thorough documentation of the entire research process. This ensures that analyses are well-tracked and can be easily replicated across different projects, enhancing the overall transparency and reliability of the research.\nIntegrated Version Control\nR seamlessly integrates with version control systems like Git, allowing researchers to track changes to code, data, and documents. This helps maintain a detailed record of a project’s evolution and ensures that all steps are easily reproducible. [3]\n\n3. Siraji MA, Rahman M (2023) Primer on Reproducible Research in R: Enhancing Transparency and Scientific Rigor. Clocks & sleep 6(1):1–10. https://doi.org/10.3390/clockssleep6010001\nConsistency Across Platforms\nR provides a stable environment that works consistently across different operating systems, whether you are using Windows, Mac, or Linux. This cross-platform consistency greatly enhances the reproducibility of research across diverse systems.\nBroad Community Support\nThe R community is large and active, continuously contributing to the improvement of the software. This broad support makes R a reliable choice for long-term research projects, ensuring that new tools and methods are constantly being developed and shared.\nFlexibility and Adaptability\nR offers a wide range of tools and functions that can be adapted to various research needs. This flexibility allows researchers to handle diverse tasks within a reproducible framework, making it a versatile tool for projects of all kinds."
  },
  {
    "objectID": "index.html#sec-projects",
    "href": "index.html#sec-projects",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": " projects",
    "text": "projects\nHow many times have you opened an R script and been greeted by this line:\n\nsetwd(\"C:/Users/ben/path/that/only/I/have\")\n\nWhile it is well-intentioned (i.e. avoiding the need to have full paths for all objects that will subsequently be loaded or daved ) the problem with it is obvious: This specific path is only relevant for the author and not other potential users and even for the author it will be invalid if they happen to change computers. The good news is there is a very simple way to avoid having to use setwd() at all by using Rstudio Projects.\nRstudio projects designate new or existing folders as a defined working directory by creating an .RProj file within them. This means that when you open a project the working directory of the Rstudio session will automatically be set to the directory that the .RProj file is located in and the paths of all files in this folder will be relative to this.\nThe .Rproj file can be shared along with the rest of the research project files meaning that others users can easily open the Project to have the same working directory removing the need for those troublesome setwd() lines.\n\nCreating and opening projects\nCreating an Rstudio project is as simple as using File &gt; New Project in the top left and then choosing between creating the Project in a new or existing directory.\nThere are several ways to open a Project:\n\nUsing File &gt; Open Project in the top left of Rstudio.\n\n\n\n\n\n\n\nUsing the drop down menu in the top-right of the Rstudio session.\n\n\n\n\n\n\n\nOutside of R by double clicking on the .Rproj file in the folder.\n\n\n\n\n\n\n\n\nUtilising project specific .Rprofile’s\nAnother useful feature of Rstudio projects is the ability to store project-specific settings using the .Rprofile file which controls the initialisation behaviour of the R session when the project is opened. A useful application of this for reproducible research projects is automatically open a particular script, for example a master script that runs all the code in the project (which is a concept that will discussed under workflow decomposition).\nTo do this the contents of your .Rprofile file would like this:\n\nsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession)\n    # Open the script specificed by the path\n    rstudioapi::navigateToFile('scripts/script_to_open.R', line = -1L, column = -1L)\n}, action = \"append\")\n\nThe easiest way to create and edit .Rprofile files is to use the functions from the package usethis:\n\n# Note the use of scope = \"project\" to create a project specific .Rprofile\nusethis::edit_r_profile(scope = \"project\")"
  },
  {
    "objectID": "index.html#sec-environment-management",
    "href": "index.html#sec-environment-management",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Environment management",
    "text": "Environment management\nThese lines of code are also probably familiar from the beginning of many an R script:\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nBut what is wrong with these lines?\nWell firstly, there is no indication of what version of the package is to be installed and hence if the code installing this package is old it may not work with the most recent version of the package (This is less of a problem for well established packages like the Tidyverse but for less common packages, that may see large changes between versions, it could be substantial).\nSecondly, having the user install an unspecified version of a package could also cause dependency conflicts with other packages required by the code. This is because almost all packages have some form of dependency (i.e. they use the functionality of) on other packages. This is shown aptly by the image below which, while out-dated now, showed that in 2014 to install the 7 most popular R packages at the time would actually install 63 packages in total when considering their dependencies.\n\n\n\n\n\n\n\nPackage dependencies of popular R package [13]\n\n13. Vries A de (2014) Revisiting package dependencies\n\n\nHowever the problem is bigger than just packages because when your code runs it is also utilising:\n\nA specific version of R\nA specific operating system\nSpecific versions of system dependencies, i.e. other software in other languages that R packages themselves utilise e.g GDAL for spatial analysis packages like terra.\n\nAll of these things together make up what is known as the ‘environment’ of your code. Hence the process of documenting and managing this environment to is ensure that your code is reproducible (i.e. it not only runs but also consistently produces the same results).\nThere are different approaches to environment management that differ in their complexity and hence maybe suited to some projects and not others. For the purpose of this workshop we will focus on what we have found is one of the most user-friendly ways to manage your package environment (caveat that will be discussed) in R which is the package renv. Below we will introduce this package in more detail as it will form a central part of the three workflows for reproducibility that we present.\n\nCreating reproducible environments with renv\nAs mentioned above renv is an R package that helps you create reproducible environments for your R projects by not only documenting your package environment but also providing functionality to re-create it.\nIt does this by creating project specific libraries (i.e. directories: renv/library) which contain all the packages used by your project. This is different from the default approach to package usage and installation whereby all packages are stored in a single library on your machine (system library). Having separate project libraries means “that different projects can use different versions of packages and installing, updating, or removing packages in one project doesn’t affect any other project.” [14]. In order to make sure that your project uses the project library everytime it is opened renv utilises the functionality of .Rprofile's to set the project library as the default library.\n\n14. Ushey K, Wickham H (2024) Renv: Project environments\nAnother key process of renv is to create project specific lockfiles (renv.lock) which contain sufficient metadata about each package in the project library so that it can be re-installed on a new machine.\nAs alluded to, renv does a great job of managing your packages but is not intended to manage other aspects of your environment such as: tracking your version of R or your operating system. This is why if you want ‘bullet-proof’ reproducibility renv needs to be used alongside other approaches such as containerization which is the 3rd and most complex workflow we will discuss."
  },
  {
    "objectID": "index.html#sec-writing-clean-code",
    "href": "index.html#sec-writing-clean-code",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Writing clean code",
    "text": "Writing clean code\nThe notion of writing ‘clean’ code can be daunting, especially for those new to programming. However, the most important thing to bear in mind is that there is no objective measure that makes code ‘clean’ vs. ‘un-clean’, rather we should of think ‘clean’ coding as the pursuit of making your code easier to read, understand and maintain. Also while we should aspire to writing clean code, it is arguably more important that it functions correctly and efficiently.\nThe central concept of clean coding is that, like normal writing, we should follow a set of rules and conventions. For example, in English a sentence should start with a capital letter and end with a full stop. Unfortunately, in terms of writing R code there is not a single set of conventions that everyone proscribes to, instead there are numerous styles that have been outlined and the important thing is to choose a style and apply it consistently in your coding.\nPerhaps the two most common styles are the Tidyverse style and the Google R style (Which is actually a derivative of the former). Neither style can be said to be the more correct, rather they express opinionated preferences on a series of common topics such as: Object naming, use of assignment operators, spacing, indentation, line length, parentheses placement, etc.\nRather than detail all of these topics here we will focus on just on some related tips that we think are most relevant for scientific research coding, including how to automate the formatting of your code to a particular style. However, we encourage you to go through the different style guides when you have the time.\n\nScript headers\nStarting your scripts with a consistent header containing information about it’s purpose, author/s, creation and modification dates is a great step making your workflow more understandable and hopefully reproducible. There are no rules as to what such a header should look like but this is the style I like to use:\n\n#############################################################################\n## Script_title: Brief description of script purpose\n##\n## Notes: More detailed notes about the script and it's purpose\n##\n## Date created: \n## Author(s):\n#############################################################################\n\nTo save time inserting this header into new scripts you use Rstudio’s Code snippets feature. Code snippets are simply text macros that quickly insert a section of code using a short keyword.\nTo create your own Code snippet go to Tools &gt; Global Options &gt; Code &gt; Edit Snippets and then add a new snippet with your code below it:\n\n\n\n\n\nTo use a code snippet simply start typing the keyword in the script and the auto-completion list will appear then press Tab and the code section will be inserted:\n\n\n\n\n\n\n\nCode sections\nAs you may already know braced ({}) sections of code (i.e. function definitions, conditional blocks, etc.) can be folded to hide their contents in RStudio by clicking on the small triangle in the left margin.\n\n\n\n\n\nHowever, an often overlooked feature is the ability to create named code sections that can be also folded, as well as easily navigated between. These can be used to break longer scripts into a set of discrete regions according to specific parts of the analysis (discussed in more detail later). In this regard, another good tip is to give the resulting sections sequential alphabetical or numerical Pre-fixes. Code sections are created by inserting a comment line that contains at least four trailing dashes (-), equal signs (=), or pound signs (#):\n\n# Section One ---------------------------------\n \n# Section Two =================================\n \n# Section Three #############################\n\nAlternatively you can use the Code &gt; Insert Section command.\nTo navigate between code sections:\n\nUse the Jump To menu available at the bottom of the editor[15]\n\n\n15. Posit Support (2024) Code folding and sections in the RStudio IDE\n\n\n\n\n\n\nUse the document outline pane in the top right corner of the source pane\n\n\n\n\n\n\n\n\nAutomating the styling of your code\nThere are two R packages that are very helpful in terms of ensuring your code confirms to a consistent style: lintr and styler.\n\nlintr checks your code for common style issues and potential programming errors then presents them to you to correct, think of it like doing a ‘spellcheck’ on a written document.\nstyler is more active in the sense that it automatically format’s your code to a particular style, the default of which is the tidyverse style.\n\nTo use lintr and styler you call their functions like any package but styler can also be used through the Rstudio Addins menu below the Navigation bar as shown in this gif:\n\n\n\n\n\nAnother very useful feature of both packages is that they can be used as part of a continuous integration (CI) workflow using a version control application like Git. This is a topic that we will cover as part of our Version control with Git workflow but what it means is that the styler and lintr functions are run automatically when you push your code to a remote repository."
  },
  {
    "objectID": "index.html#sec-workflow-decomposition",
    "href": "index.html#sec-workflow-decomposition",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Workflow decomposition",
    "text": "Workflow decomposition\nIn computer sciences workflow decomposition refers to the structuring or compartmentalising of your code into seperate logical parts that makes it easier to maintain [16].\n\n16. (2024) Decomposition (computer science)\nIn terms of coding scientific research projects many of us probably already instinctively do decomposition to some degree by splitting typical processes such as data preparation, statistical modelling, analysis of results and producing final visualizations.\nHowever this is not always realized in the most understandable way, for example we may have seperate scripts with logical sounding names like: Data_prep.R and Data_analysis.R but can others really be expected to know exactly which order these must be run in, or indeed whether they even need to be run sequentially at all?\nA good 1st step to remedying this is to give your scripts sequential numeric tags in their names, e.g. 01_Data_prep.R, 02_Data_analysis.R. This will also ensure that they are presented in numerical order when placed in a designated directory Structuring your project directory and can be explicitly described in your project documentation.\nBut you can take this to the next level by creating a Master script that sources your other scripts in sequence (think of them as sub-scripts) so that users of your code need only run one script. To do this is as simple as creating the master script as you would any normal R script (File &gt; New File &gt; R script) and then using the base::source() function to run the sub-scripts:\n\n#############################################################################\n## Master_script: Run steps of research project in order\n##\n## Date created: 30/7/2024\n## Author(s): Jane Doe\n#############################################################################\n\n### =========================================================================\n### A- Prepare dependent variable data\n### =========================================================================\n\n#Prepare LULC data\nsource(\"Scripts/Preparation/Dep_var_dat_prep.R\", local = scripting_env)\n\n### =========================================================================\n### B- Prepare independent variable data\n### =========================================================================\n\n#Prepare predictor data\nsource(\"Scripts/Preparation/Ind_var_data_prep.R\", local = scripting_env)\n\n### =========================================================================\n### C- Perform statisical modelling\n### =========================================================================\n\nsource(\"Scripts/Modelling/Fit_stat_models.R\", local = scripting_env)\n\nAs you can see in this example code I have also made use of a script header and code sections, that were previously discussed, to make the division of sub-processes even clearer. Another advantage of this approach is that all sub-scripts can utilise the same environment (defined by the source(local= ) argument) which means that each individual script does not need to load packages or paths as objects.\nFinally, within your sub-scripts processes should also be seperated into code sections and ideally any repetitive tasks should be performed with custom functions which again are contained within their own files.\nFollowing this approach you end up with a workflow that will look something like this:\n\nThe benefit of this hierarchical approach to structuring is that it is not only easier to debug and maintain individual processes but it is also more amenable to adding new processes."
  },
  {
    "objectID": "index.html#sec-structuring",
    "href": "index.html#sec-structuring",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Structuring your project directory",
    "text": "Structuring your project directory\nSimilar to having clean code, having a clean project directory that has well-organised sub-directories goes a long way towards making your projects code easier to understand for others. For software development there are numerous sets of conventions for directories structures although these are not always so applicable for scientific research projects. However we can borrow some basic principles, try to use: - Use logical naming - Stick to a consistent style, i.e. use of captialisation and seperators - Make use of nested sub-directories e.g data/raw/climatic/precipitation/2020/precip_2020.rds vs. data/precip_2020_raw.rds. This is very helpful when it comes to programatically constructing file paths especially in projects with a lot of data.\nAs an example my go-to project directory structure looks like this:\n\n└── my_project\n    ├── data # The research data\n    │   ├── raw\n    │   └── processed\n    ├── output # Storing results\n    ├── publication # Containing the academic manuscript of the project\n    ├── src # For all files that perform operations in the project\n    │   ├── scripts\n    │   └── functions\n    └── tools # Auxilliary files and settings\n\nRather than manually create this directory structure everytime you start a new project, save yourself some time and automate it by using Rstudio’s Project Templates functionality. This allows you to select a custom template as an option when creating a new Rstudio project through the New project wizard (File &gt; New Project &gt; New Directory &gt; New Project Template).\nTo implement this even as an intermediate R user is fairly labor intensive as your custom project directory template needs to be contained within an R-package, in order to be available in the wizard. However, quite a few templates with directory structures appropriate for scientific research projects have been created by others:\n\nrrtools\nProjectTemplate\ntemplate\naddinit (Not a template but an interactive shiny add-in for project creation)"
  },
  {
    "objectID": "index.html#sec-documentation",
    "href": "index.html#sec-documentation",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Project documentation",
    "text": "Project documentation\nAs an example of why documentation is important think about if you bought a new table from Ikea only to excitedly rip open the box and find that there are no instructions for how to assemble it. Sure, you know what a table is supposedly to look like and given enough time you will end up with something that will probably be mostly right but maybe it’s missing small details. Also it will probably take you just as long to take it apart in 5 years time. Well, working with undocumented code for research projects is similar except a lot more complicated!\nWriting comprehensive documentation that covers all aspects of our projects is time-consuming which is why it is often neglected. For example, there are a lot of different metadata conventions that exist that you could apply. However, learning and adhering strictly to these can be overwhelming and possibly lead to the opposite effect i.e. they are not simple for others to understand either.\nIn response to this there has been a movement in the R research community to adopt the research as package approach, which, as the name suggests, involves creating your project as an R-package which has a strict set of conventions for documentation [17]. This is a viable approach for those who are familiar with R-packages but is arguably not the best for all projects and users.\n\n17. Marwick B, Boettiger C, Mullen L (2018) Packaging data analytical work reproducibly using r (and friends). The American Statistician 72(1):80–88. https://doi.org/10.1080/00031305.2017.1375986\nInstead, we would suggest to follow the maxim of not letting the perfect be the enemy of the good and to focus on these key areas:\n\nProvide adequate in-script commentary: This is perhaps contentious for those from a software development community, but given the choice I would rather have to read through a script with too many comments than one with too few. However remember that comments should be used to explain the purpose of the code, not what the code is doing. In line with this use script headers.\nDocument your functions with roxygen skeletons:\nInclude a README file: README files are where you should document your project at the macro-level i.e. what it is about and how it is supposed to work.\n\nThe latter of these two are more detailed so we have provided further information and tips in sections below.\n\nFunction documentation with roxygen2\nBase R provides a standard way of documenting a package where each function is documented in an .Rd file (R documentation). This documentation uses a custom syntax to detail key aspects of the functions such as their input parameters, outputs and any package dependencies [18].\n\n18. Wickham H, Bryan J (2024) R packages (2e), 2nd Edition\nIn the case of many research projects you will not be creating a package however it is still useful to apply this documentation style to your functions as it is a good way to make them understandable and easier to modify by others. For example, having clear information about the object (e.g. a vector or data.frame) that a function accepts, saves others time in guessing what the function is expecting if they are trying to use new data.\nHowever, rather than manually writing .Rd files, we can use the roxygen2 package to automatically generate these files from a block of comments that are added to the top of the function scripts. To add this comment block, place your cursor inside a function you want to document and press Ctrl + Shift + R (or Cmd + Shift + R on Mac) or you can go to code tools &gt; insert roxygen skeleton (code tools is represented by the wand icon in the top row of the source pane). As you can see in this gif below, when you insert the roxygen block it will already contain the names of the function, its arguments and any returns. You can then fill in the rest of the information, such as the description and dependencies etc. for a guide to these other fields see the roxygen2 documentation.\n\n\n\nInserting roxygen block\n\n\n\n\nTips for README writing\nIf you look at the source code of R packages or projects that use R in Github repositories you will see that they all contain README.md files. .md is Markdown format which is the most common format for README files in R projects because it can be read by many programs and rendered in a variety of formats. These files are often accompanied by the corresponding file README.Rmd which generates the README.md file. In this sense writing the README for your project in markdown makes sense and there tools available to help you do this such as the usethis package which has a function use_readme_rmd() that will create a README.Rmd file for you. However, depending on who you anticipate using your project you may also want to create your README as a raw text file (.txt) which may be a more familiar format for some users and again can be opened by many different programs.\nAgain there is not a single standardised format for what should be included in your README file but here is an example of a README file that was written for one of the authors code/data upload alongside a publication: README.txt\nYou will see that one of the things this README includes is a tree diagram which shows the directory structure of the project right down to the file level. This is a useful way to give an overview of what users should find included in the project and then explanatory notes can be added to explain the purpose of each file or directory. Such a diagram can be easily generated using the fs package:\n\ninstall.packages(\"fs\")\nlibrary(fs)\n\n#vector path of the target directory to make a file tree from\nTarget_dir &lt;- \"YOUR DIR\"\n\n#produce tree diagram of directory sub-dirs and files and save output using capture.ouput from base R utils.\ncapture.output(dir_tree(Target_dir), file= 'Dir_tree_output.txt')"
  },
  {
    "objectID": "index.html#sec-zenodo_workflow",
    "href": "index.html#sec-zenodo_workflow",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": " project to  pipeline",
    "text": "project to  pipeline\n\n\n\n\n\n\nThe graphic above shows the main steps of this workflow. It starts by developing your research project as an Rstudio project following the good practice project guidelines we have discussed. Then, it uses the renv package to manage the project environment so that others can re-create it. Finally, the code, data and environment are uploaded to the open-access repository Zenodo, which provides a DOI for your work, ensuring long-term accessibility and reproducibility.\nThe renv package helps maintain the R environment, allowing others to recreate the environment in which your analysis was conducted. By combining renv with Zenodo, you create a comprehensive solution for reproducible research. renv ensures that the computational environment is captured, while Zenodo makes your research outputs accessible and citable, supporting the FAIR principles of findability, accessibility, interoperability, and reusability [4].\n\n4. Wilkinson MD, Dumontier M, Aalbersberg IjJ, et al (2016) The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3(1):160018. https://doi.org/10.1038/sdata.2016.18\nThe next sections provide some additional detail on environment management with renv and some background and practical tips on Zenodo. You will have the chance to apply this workflow to an example project in the accompanying exercise.\n\nEnvironment Management with renv\nrenv is a powerful R package designed to help manage project environments by creating project-specific libraries and lockfiles. As mentioned earlier, renv captures the exact versions of R packages used in a project, storing this information in a renv.lock file. This allows users to recreate the exact package environment when revisiting a project or transferring it to a different machine, ensuring reproducibility.\nThe renv workflow is straightforward:\n\nInitialize renv in a project: renv creates a separate library in the project folder, isolating the packages from the system-wide library.\nSnapshot dependencies: renv scans the project, identifying which packages are being used and recording their versions in the lockfile.\nRestore environments: Anyone cloning or receiving the project can run renv::restore() to install the exact versions of the packages listed in the lockfile from the project library, reproducing the original project package environment.\n\nOne of the core strengths of renv is its flexibility. It integrates seamlessly with tools like RStudio, allowing easy management of dependencies without disrupting existing workflows. This makes it particularly well-suited for ensuring that research projects are reproducible across different systems and platforms.\nHowever, renv does not manage the entire system environment (such as the version of R itself or external dependencies like system libraries). For complete reproducibility, combining renv with\ncontainerization tools (like Docker) or publishing outputs (such as code or data) via repositories like Zenodo is recommended.\n\n\n as a research repository\nZenodo is a platform created under the European Commission‘s OpenAIRE project in partnership with CERN to publish, archive, and share scientific research outputs, including datasets, code, and publications.\nOf course there are many other similar research repositories, such as Dryad, Figshare, Mendeley Data and OSF, but we recommend Zenodo for several reasons:\n\nGenerous upload size of 50GB (100 files) per record\nAligns with FAIR and Open Science principles: The practical features of Zenodo that ensure this are described in it‘s principles\nAbility to create communities: Zenodo Communities are used to group similar records together. This is useful for creating a collection of related research outputs, either for a research group or a large-scale funded project.\nLong term preservation with assignment of DOIs: Each item published on Zenodo is assigned a permanent Digital Object Identifier (DOI), which is a better way than a URL to cite the record in academic writing.\nOpen source: This means that Zenodo is not just free to use but you can even see the code it is built on and contribute to it.\nVersioning functionality: Every record starts with a 1st version and new versions can be added as research is updated, while earlier versions remain accessible. This is crucial in scientific research, where updated analyses and data corrections are often necessary, but also transparency around earlier versions of the work should be maintained.\nIntegration with GitHub: When a research project (e.g., code) is hosted on GitHub, Zenodo can be used to archive the repository upon each new release, creating a snapshot with a DOI. This means that a version of the code can be more easily cited in scientific publications.\nApplication programming Interface (API) to access records programmatically: This a useful feature as it allows for interfacing with Zenodo records without using the website and is the backbone of the zen4R package that allows for publishing records directly from R which we discuss in more detail below.\n\n\n\nPublishing to Zenodo with zen4R\nThe zen4R package [19] provides functions to interact with Zenodo‘s API directly from R. The package allows you to:\n\nRetrieve the metadata of, and download, existing Zenodo records.\nCreate new records and versions of records, write their metadata and upload files to Zenodo.\n\nWe will use zen4R to publish the code, data, and environment of our example project to Zenodo in the accompanying exercise."
  },
  {
    "objectID": "index.html#sec-docker_workflow",
    "href": "index.html#sec-docker_workflow",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Containerization with ",
    "text": "Containerization with \n\n\n\n\n\n\nThe title of this workflow raises two questions, the first being: what is containerization? and the second: what is Docker? ### Containerisation\nSimply put containerization is the process of bundling code along with all of it’s dependencies, i.e. all the components we discussed as making up the environment, including the operating system, software libraries (packages), and other system software. The fact everything needed to run the code is included means that the code is portable and can be run on any platform or cloud service. This also makes containerization something of a gold standard for reproducibility as the entire environment is explicitly re-produced.\n\nDocker\nDocker is an open-source, and the most popular, platform for containerization. Before we dive into a practical example using Docker for research projects with R it is important to introduce some three key terms that we will come across:\n\nDockerfile: The first step in the containerization process, they are a straightforward text file containing a collection of commands or procedures to create a new Docker Image. In this sense we can consider a Dockerfile are the source code of Docker Image. Importantly, Dockerfiles typically start from a base image, which is a existing Docker Image that your image is extending.\nDocker Image: A read-only file that contains the instructions for creating a Docker Container. Think of an image as the blueprint of what will be in a container when it is running. Docker Images can be shared via Dockerhub, so that they can be used by others.\nDocker Container: Is an actual running instance of a Docker image. It runs completely isolated from the host environment by default and only accesses host files (i.e. data) if it has been configured to do so. It is possible to create multiple containers simultaneously from the same Docker Image, and each container can be started, stopped, moved, and deleted independently of the others.\n\nThe graphic above shows the relationships between these components including the central commands of Docker that connect them build and run.\n\n\nUsing Docker with R\nSo to create a Docker Image to containerize our R research projects we need to start by creating a Dockerfile and, as mentioned above, this should start with a base image. In our case this base image must logically include R and RStudio (if we want to utilise the RStudio Projects features).\nFortunately there is a project that specifically catalogs and manages Docker Images for R projects: Rocker. The images available through the Rocker project not only include different versions of R and RStudio but also images containing collections of R packages for specific purposes (e.g. tidyverse for data wrangling and visualisation, geospatial packages etc.).\nIn terms of actually creating the Dockerfile for our R project, this can be done manually (See a good R-focused tutorialhere), however there are also R packages that can help with this process such as dockerfiler and the [rrtools](https://github.com/benmarwick/rrtools) package.\nFor our exercise of this workflow we will use the dockerfiler package, which creates a custom class object that represents the Dockerfile and has slots corresponding to common elements of Docker images. This allows us to add elements to the dockerfile in a more R-like way. The following code snippet demonstrates adding Maintainer details to a Dockerfile object, before saving it:\n\nlibrary(dockerfiler)\n# Create a dockerfile template\nmy_dock &lt;- Dockerfile$new()\n\n# Add maintainer\nmy_dock$MAINTAINER(\"Jane Doe\", \"jane_doe@gmail.com\")\n\n# Save\nmy_dock$write()\n\n\n\nDocker with renv\nDocker can be used with the renv package to manage the package environment of your project.\nThere are two methods of implementing this which come with their own considerations:\n\nUse renv to install packages when the Docker image is built: This approach is useful if yo plan to have have multiple projects with identical package requirements. This because by creating an image containing this package library you can simply re-use the image as a base for new images for different projects [20].Warning: Restoring the package library (renv::restore()) when building the image will be slow if there are large numbers of packages so try to avoid the need to re-build the base image many times.\nUse renv to install/restore packages only when Docker containers are run: This approach is better when you plan to have multiple projects that are built from the same base image but require different package requirements. Hence it is preferable to not included the package library in the image but instead to mount different project specific libraries to the container when it is run [20]. If project libraries are dynamically provisioned in this way and renv::restore() is run with caching this means that the packages are not re-installed everytime the container is run.\n\n\n\n20. Ushey K, Wickham H (2024) Using renv with docker"
  },
  {
    "objectID": "index.html#sec-git_workflow",
    "href": "index.html#sec-git_workflow",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Version control with ",
    "text": "Version control with \n\n\n\n\n\nVersion control software output can have multiple uses including creating a systematic procedure for collaboration, working as a team along with increasing the ease of reproduction of the work by other users or by the original researcher as well. We all know the difficulty of tracing back the workflow of work projects a few months down the line after having shelved it, and it is here that Git and Github can be highly useful ([21]).\n\nAbout Git\nGit allows us to make snapshot or record of the changes undertaken in a script, and store it as with a message that defines the change. In this way even after multiple updates, the history is preserved allowing us to revert, compare and systematically trace back the workflow development.\nGit is useful also for data scientists and researchers that work individually yet want to create systematically reproducible workflows with version control ([21]).\n\n21. Alexander R (2023) Telling stories with data, 1st Edition\n\n\nGitHub repositories and functionalities\nThe version controlled code and all other auxiliary files related to the project are stored in a Github repository which was created by the user in their account on Github. A repository can either be set as public or private as per the users need for visibility of their work.\nTo help with version control Github repositories provide multiple functionalities like creating ‘branches’, ‘clones’, ‘merging’ multiple branches, setting up a ‘pull request’ before merge etc. As we get into more complicated workflows handled by multiple developers, Github allows many more functionalities as checks and balances to code development. However, we will limit our understanding to what is needed to create work with a version control history allowing for small scale collaboration, but with the main goal of creating reproducible research (see more details).\n\n\nBasic functionalities\n\nCreating a repository and setting up user authorisations: A project repository must first be set up on GitHub as either a private or a public repository. If it is not an individual project, collaborators can be added with appropriate (read or write) permission levels (see more details). It is good to elaborate the ‘Readme’ file so as to help viewers get an idea of the repository (see more details).\nPush and Pull: The data and code related to a project must be cloned from the remote version to a local version before changes are made. Make sure to pull from updated (i.e. merged branches; see below) branches before making changes. Once changes are made the user must ‘commit’ all the correct changes. Once this is done the changed code can be pushed back to the branch.\nBranching and merging : Git allows users to create branches that feed into the ‘main’ branch of a project repository on GitHub. Each branch can be created either for different tasks or for different users as per the requirement. To merge branches into ‘main’ users have to set up a pull-request which needs to be approved by an authorised user.\nGit provides additionally many more functionalities for identifying differences between changed files or between branches, to make temporary commits and to revert back to a certain commit in history. However this is beyond the scope of this workshop.\n\n\n\nGit in R-Studio\nWhen a Github repository is connected to an R project, R-studio adds a ‘Git’ tab (see image below) with ‘push’, ‘pull’, ‘commit’ and ‘diff’ functionalities. We can switch branches to pull from or push to and additionally trace the history of changes in the Github respository by all users.\n\n\n\n\n\n\n\nGit intergration into R-Studio\n\n\nAlternatively you can also use Github desktop to perform the same functionalities.\n\n\nAdditional functionalities\n\nPublic Github repositories can now be archived on Zenodo as a permanent record of the work with a Digital Object Identifier (DOI) that can be cited in academic work (Click to see more details).\nGitHub further provides advanced functionalities like GitHub Actions that allows the user to automate certain processes for application development or project management.\nGitHub releases can be automatically published on Docker Hub or GitHub packages as part of a Continuous Integration(CI) workflow (see more details)."
  },
  {
    "objectID": "index.html#sec-Rproj_zenodo_exercise",
    "href": "index.html#sec-Rproj_zenodo_exercise",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "RStudio Project to Zenodo exercise",
    "text": "RStudio Project to Zenodo exercise\nIn this exercise you will setup of a basic RStudio project following good practices for organizing data, writing clean code and decomposing your workflow introduced in Rproject section. We‘ll also initialize renv to manage package dependencies, ensuring reproducibility. Once the project is set up, we’ll go through the process of how to publish the project to Zenodo as described in the Zenodo workflow section.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\nStep 1: Download the exercise resources\n\nClick here to download the resources for the exercise:  Download resources for exercise\nUnzip the downloaded file and move the folder to a location on your computer where you can easily find it.\n\n\n\nStep 2: Create a new RStudio Project\nStart by creating a new RStudio project in the root of the exercise_1_data directory. you have just downloaded. You can name the project what you like but in this example, we have named it Rice_farm_analysis.proj.\n\nOpen RStudio.\nCreate the project using: File &gt; New Project &gt; Existing Directory.\nSelect the exercise_1_data folder as the location and give the project a name, for example, Rice_farm_analysis.proj.\n\nThis creates a .Rproj file in the root of your project to help manage the workspace and project-specific settings.\n\n\nStep 3: Organize Your Data\nIt’s good practice to organize raw and processed data in separate folders. Let’s start by organizing the data:\n\nCreate a directory Data/Raw inside your project folder.\nMove the provided CSV file into this Data/Raw directory.\n\n\n\nStep 4: Organize and Split Your Scripts\nWe’ll now organize the project’s scripts by splitting the original script into separate analysis and visualization scripts.\n\nCreate a Scripts folder inside your project directory.\nMove the original RiceFarm_project.R script into the Scripts folder.\n\nWe’ll now organize the project’s scripts by splitting the original script into separate analysis and visualization scripts.\n\nCreate a scripts folder inside your project directory.\nMove the original RiceFarm_project.R script into the scripts folder.\nCreate two new scripts named 01_data_analysis.R and 02_data_visualisation.R.\n\n\nFor 01_data_analysis.R\n\nCopy the following code from RiceFarm_project.R:\n\nThe call to the relevant library library(stringr)\nEverything before the call to the ggplot() function\n\nIn addition to this, replace the setwd() function with this code to set up relative paths, create a directory to save the processed data in and save rice_data_summary to disk after processing:\n\n\n# vector and create processed data save dir\nsave_dir &lt;- \"Data/Processed\"\ndir.create(save_dir,\n           showWarnings = FALSE,\n           recursive = TRUE)\n\n# Load csv file of data\nrice_data &lt;- read.csv(file.path(raw_dir, \"RiceFarms.csv\"))\n  \n# Save the summarized data\nwrite.csv(rice_data_summary,\n          file.path(save_dir, \"RiceFarms_summary.csv\"),\n          row.names = FALSE)\n\n\n\nFor 02_data_visualisation.R\n\nCopy the following code from RiceFarm_project.R:\n\nThe call to the relevant library library(ggplot2)\nThe call to the functions ggplot() and ggsave()\n\nAdd the following code after the library() call to create an output directory for the plots and load the summarized data from the processed data folder:\n\n\n# Directory for saving plots\nplot_dir &lt;- \"Output/Visualisations\"\ndir.create(plot_dir, showWarnings = FALSE, recursive = TRUE)\n\n# load the summarized data\nrice_data_summary &lt;- read.csv(\"Data/Processed/RiceFarms_summary.csv\")\n\n\n\n\nStep 5: Add Script Headers\n\nAdd headers to both new scripts. You can use this template:\n\n\n# -----------------------------------------------\n# Script Name: [01_data_analysis.R / 02_data_visualisation.R]\n# Project: Rice Farm Analysis\n# Purpose: [Data analysis / Data visualization]\n# Author: [Your Name]\n# Date: [YYYY-MM-DD]\n# -----------------------------------------------\n\n\n\nStep 6: Create a master script\nAs a next step you will create a master script that runs both the data analysis and visualization scripts.\n\nIn the root of your project, create a new file named RiceFarm_master.R\nAdd a header as in Step 5.\nAdd the following code snippet to the script to source 01_data_analysis.R and 02_data_visualisation.R:\n\n\n###=========================================================================\n### 01- Data analysis\n### =========================================================================\n\n# Source the data analysis script\nsource(\"Scripts/01_data_analysis.R\")\n\n### =========================================================================\n### 02- Data visualization\n### =========================================================================\n\n# Source the data visualization script\nsource(\"Scripts/02_data_visualisation.R\")\n\nRunning this master script will execute both analysis and visualization steps.\n\n\nStep 7: Initialize renv to manage package dependencies\nWe will use renv to make your projects package environment reproducible.\n\ninstall renv package\n\n\ninstall.packages(\"renv\")\n\n\nRun the following command in your master script to set up the project-specific environment\n\n\nrenv::init()\n\nThis creates a local library for your project and captures the required packages.\n\nOnce the initialization is complete, run:\n\n\n\nStep 8: Automate Opening the Master Script\nFor convenience, we can configure RStudio to automatically open the master script when the project is loaded.\n\ninstall the rstudioapi package:\n\n\ninstall.packages(\"rstudioapi\")\n\n\nOpen the .Rprofile file in the root of your project directory. The file might be hidden. On Windows click “View” &gt; “Show” &gt; “Hidden items” in the explorer and on MacOS click Press Command+Shift+Dot within the root directory to see the file.\n\n\nrenv::snapshot()\n\nThis records the project’s environment in a renv.lock file, which is essential for reproducibility.\n\n\nStep 8: Automate opening of the master script\nFor convenience, we will configure RStudio to automatically open the master script when the project is loaded.\n\nOpen the .Rprofile file in the root of your project directory. The file might be hidden. On Windows click View &gt; Show &gt; Hidden items in the explorer and on MacOS click Press Command+Shift+Dot within the root directory to see the file.\nAdd the following R code to the .Rprofile file:\n\n\nrsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession)\n    rstudioapi::navigateToFile('RiceFarm_master.R', line = -1L, column = -1L)\n}, action = \"append\")\n\n\n\nStep 9: Re-snapshot the Project\nAfter modifying the .Rprofile file, it’s important to capture these changes in the renv.lock file.\n\nRun the following command in your master script to ensure that the rstudioapi package (which enables automatic script opening) is included in the snapshot:\n\n\nrenv::snapshot()\n\nNow we a have a nicely organised project structure with the workflow decomposed into seperate scripts and a master script that to run the whole project.\n\n\nStep 10: Install zen4R to access Zenodo through R\nThe following steps are heavily based on [19]. We have extracted the most relevant parts to explain the workflow. If you are interested in more details, check out their user manual at: https://cran.r-project.org/web/packages/zen4R/vignettes/zen4R.html.\n\n19. Blondel E, Barde J (2024) zen4R: R Interface to Zenodo REST API\nFor this exercise we will not be using Zenodo directly but Zenodo Sandbox. The Zenodo Sandbox is a separate, secure testing environment where users can explore Zenodo‘s features without impacting the main platform‘s publicly accessible data. It allows you to test file uploads, generate test DOIs, and experiment with API integrations. DOIs created in the sandbox are only for testing and use a different prefix. You will need a separate account and access token for the sandbox, distinct from those used on Zenodo‘s main site.\n\nCreate an account on https://sandbox.zenodo.org.\n\nZenodo can be accessed with the R package zen4R to upload, edit, publish and download data.\n\nCreate a new R script outside of the project directory.\nInstall zen4R library with the following code:\n\n\n#install dependency \"remotes\"\ninstall.packages(\"remotes\")\n        \n#install zen4R\nrequire(\"remotes\")\ninstall_github(\"eblondel/zen4R\")\n\n\n\nStep 11: Create a new Zenodo record\nA Zenodo record includes metadata, data and a Digital Object Identifier (DOI) which is automatically generated by Zenodo for all uploads. But before you can add records to Zenodo, you need to get access to your account through R.\n\nGo to https://sandbox.zenodo.org/account/settings/applications/.\nLog into your account and then create a new “Personal access token” in the “Applications” section of your account.\nThen run the following code in your script to establish the access and create a new record.\n\n\nlibrary(zen4R)\n        \n#Create manager to access your Zenodo repository\nzenodo &lt;- ZenodoManager$new(\n    token = \"your_token\",\n    sandbox = TRUE,\n    logger = \"INFO\" \n    )\n        \n##Prepare a new record to be filled with metadata and uploaded to Zenodo\nmyrec &lt;- ZenodoRecord$new()\n\nIf you want to connect to Zenodo and not Zenodo Sandbox, create the token in your Zenodo account and remove the line sandbox = True in the code above.\nThe types of metadata that can be included in a Zenodo record are vast. A full list can be found in the documentation.\n\nCopy and run the example below to add metadata to your record.\n\n\nmyrec$setTitle(\"RiceFarm\") #title of the record\nmyrec$addAdditionalTitle(\"This is an alternative title\", type = \"alternative-title\")\nmyrec$setDescription(\"Calculating statistics of RiceFarm dataset\") #description\nmyrec$addAdditionalDescription(\"This is an abstract\", type = \"abstract\")\nmyrec$setPublicationDate(\"2024-09-16\") #Format YYYY-MM-DD\nmyrec$setResourceType(\"dataset\")\nmyrec$addCreator(firstname = \"Yourfirstname\", lastname = \"Yourlastname\", role = \"datamanager\", orcid = \"0000-0001-0002-0003\")\nmyrec$setKeywords(c(\"R\",\"dataset\")) #For filtering\nmyrec$addReference(\"Blondel E. et al., 2024 zen4R: R Interface to Zenodo REST API\")\n\nA record can be deposited on Zenodo before it is published. This will add the record to your account without making it public yet. A deposited record can still be edited or deleted. You can also upload data to a deposited record. If you prefer a graphical interface, you can also edit the record on the Zenodo website.\n\nDeposit the record on Zenodo:\n\n\n#deposit record\nmyrec &lt;- zenodo$depositRecord(myrec, publish = TRUE)\n\n\nView the deposited record at https://sandbox.zenodo.org/me/uploads?q=&l=list&p=1&s=10&sort=newest\nCompress your project directory to a .zip file.\nUpload the .zip file to your deposited record:\n\n\n#add data to the record, adjust the path below\nzenodo$uploadFile(\"path/to/your/file\", record = myrec)\n\n\nPublish the record:\n\n\n#make the record publicly available on Zenodo (Sandbox).\nmyrec &lt;- zenodo$publishRecord(myrec$id)\n\n\n\nStep 12: Edit a published Zenodo record\n\nIt is also possible to edit or update the metadata of published records:\n\n\n#get your record by metadata query, e.g. by title\nmyrec &lt;- zenodo$getDepositions(q='title:zen4R')\n\n#get depositions creates a list, access first element\nmyrec &lt;- myrec[[1]]\n\n#edit metadata\nmyrec &lt;- zenodo$editRecord(myrec$id)\nmyrec$setTitle(\"zen4R 2.0\")\n\n#redeposit and publish the edited record\nmyrec &lt;- zenodo$depositRecord(myrec, publish = TRUE)\n\n\nOnce a record has been published, it is not possible to edit the data that has been attached to it. However, it is possible to upload an updated version of the data. The previous version of the data will remain accessible via Zenodo. The record will have one overall DOI, while each version will have its own DOI.\nReconnect to your account:\n\n\nzenodo &lt;- ZenodoManager$new(\n      token = \"your_token\",\n      sandbox = TRUE,\n      logger = \"INFO\")\n\n\nAccess your record:\n\n\n#get your record by querying the metadata, e.g. by title, this will give you a list of all records with that title.\nmyrec &lt;- zenodo$getDepositions(q='title:RiceFarm Statistics')\n    \n#access the first item in the list, as there should only be one record with that particular title\nmyrec &lt;- myrec[[1]]\n\n\nRename your .zip file on your computer\nUpload the renamed .zip file:\n\n\n#edit data, delete_latest_files = TRUE to not include data of previous version in newer version\nmyrec &lt;- zenodo$depositRecordVersion(myrec, delete_latest_files = TRUE, files = \"path/to/your/new/file\", publish = TRUE)\n\n\nAgain, go to https://sandbox.zenodo.org/me/uploads?q=&l=list&p=1&s=10&sort=newest\nActivate “View all versions” on the left hand side.\nCheck if both versions show up"
  },
  {
    "objectID": "index.html#sec-docker_exercise",
    "href": "index.html#sec-docker_exercise",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Containerization with Docker exercise",
    "text": "Containerization with Docker exercise\nIn this exercise we will create and run a Docker container for an example R project. The project is the same that is created in the first workflow exercise, however to save time or in case you haven’t completed this exercise we will start with the finished output from it.\nWarning: Docker is a complex software and getting Docker Desktop running on different machines is not always smooth. For example, I had no problem getting it running on my desktop computer but my work laptop did not have the capabilities. If you do run into issues there is good support available online but also asking for help from your IT department may be a good idea.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\nStep 1: Download the resources\n\nClick here to download the resources for the exercise:  Download resources for exercise\nUnzip the downloaded file and move the folder to a location on your computer where you can easily find it.\n\n\n\nStep 2: Download Docker\n\nDownload Docker Desktop for your operating system from the Docker website.\nOnce downloaded run the installer like you would for other software. If your computer is managed by your institution or your employer you will likely need an admin account to run the installer and you may need to restart your computer after installation.\nWhile you are running the installer it is useful to make a Docker account. This is not necessary but can be useful for managing your containers. You can also sign in with your GitHub account.\n\n\n\nStep 3: Open Docker desktop\n\nOpen the Docker desktop app. If the app does not open you may need to yourself to the program user-group on your computer. This is a common issue on Windows machines because only the admin account is added to the user-group by default. To add yourself to the user-group search computer management in the start menu and right-click and select to run it with admin privileges. Then navigate to Local Users and Groups -&gt; Groups -&gt; Docker Users. Right click on Docker Users and select Add to Group. Then add your user account to the group.\nOnce the Docker desktop app is open it should automatically start the docker engine which is the software that runs the containers. In the bottom left of app window you will see the status of the engine.\n\n\n\n\n\n\n\n\nDocker Engine status in app\n\n\nAlternatively if you look in the system tray on Windows or the top menu bar on Mac. You will see an icon of the Docker whale logo and if you click on this you can see the status of the engine.\n\n\n\n\n\n\n\nDocker Engine status in system try\n\n\n\n\nStep 4: Creating the Dockerfile\n\nOpen Rstudio and navigate to the folder you downloaded in step 1.\nCreate a new R script and name it Create_Dockerfile.R.\nInstall the Dockerfiler package: install.packages(\"dockerfiler\").\nAdd the following code to the script and replace the entries with your details:\n\n\n# Load dockerfiler package\nlibrary(dockerfiler)\n\n# Get your R version to select a base image to use for your image/container\nR.Version()$version.string \n\n# Create a dockerfile template object using the Dockerfile class from the\n# dockerfiler package and specify your version of R in the base image name\n# my version is 4.3.1 hence the base image is rocker/r-ver:4.3.1\n# but you should replace the end of this string with your version number from above\nRiceFarm_dock &lt;- Dockerfile$new(FROM = \"rocker/r-ver:4.3.1\")\n\n# Add maintainer information (replace with your details)\nRiceFarm_dock$MAINTAINER(\"Your_name\", \"Your_email\")\n\n# By default docker images contain a home directory and because our project\n# is simple we will move the files we need there\n\n# Copy the data directory \n# (1st argument is the source, 2nd is the destination in the container)\nRiceFarm_dock$COPY(\"/Data\", \"/home/Data\")\n\n# Copy the scripts directory\nRiceFarm_dock$COPY(\"/Scripts\", \"/home/Scripts\")\n\n# Copy the master script\nRiceFarm_dock$COPY(\"/RiceFarm_master.R\", \"/home\")\n\n# For our project we need \"ggplot2\" and \"stringr\" packages\n# We could try to find a base image on Rocker that has these installed\n# But because we are not using lots of packages lets just install them in the container\n# Note that the R commands are wrapped in `r()` which is a helper function from dockerfiler\n# that then wraps the command in the correct syntax for the Dockerfile\nRiceFarm_dock$RUN(r(install.packages(\"ggplot2\")))\nRiceFarm_dock$RUN(r(install.packages(\"stringr\")))\n\n# Add the command to run the master script\n# Note the use of `Rscript` which is the command line tool included with R to run scripts\nRiceFarm_dock$CMD(\"Rscript /home/RiceFarm_master.R \")\n\n# Save the Dockerfile\nRiceFarm_dock$write()\n\n# Create dir in the host directory to receive the results from container\ndir.create(\"/output\")\n\n\nAfter running this code you will see that a Dockerfile has been created in the directory where you downloaded the resources.\n\n\n\nStep 5: Creating the Docker image\n\nThe Docker command build is used to create a Docker image from the instructions contained in your Dockerfile.\nThe build command should be called through a Command Line Interface (CLI) such as the terminal in Rstudio or the CLI of your operating system (e.g Command Prompt for Windows).\nIn Rstudio switch to the terminal tab next to the console pane:\n\n\n\n\n\n\n\n\nRstudio terminal\n\n\n\nRun the following command: docker build  -t ricerarm_01 . Note: The -t flag is used to tag the image with a name (in this case we are using ricerarm_01). The . at the end of the command is used to specify the current directory as the location of Dockerfile that is to be used.\nAfter running the command you will see the Docker engine pulling the base image from the Docker Hub and then building the image. This process can take a few minutes depending on the size of the base image and the number of packages you are installing. The output in the terminal will look something like this:\n\n\n\n\n\n\n\n\nDocker build output in terminal\n\n\n\nOnce the image has been built you can check that it is there by running the command docker images in the terminal. This will show you a list of all the images on your computer. You should see the image you just created in the list.\nAlternatively you can check the image in the Docker desktop app. You will see the image in the list of images on the left of the app window. You can inspect the image by clicking on it and see the details of the image:\n\n\n\n\n\n\n\n\nDocker build output in app\n\n\n\n\nStep 6: Running the Docker container\n\nThe Docker command run is used to run a container from an image.\nThis can be done through the CLI: In the terminal tab in Rstudio run the following command: docker run ricerarm_01.\nOr in the Docker desktop app: Click on the image you want to run and then click the run button in the top right. This will open a window where you can specify the settings for the container but for now you should just run the container with the default settings.\n\n\n\n\n\n\n\n\nDocker images in app\n\n\n\nAfter running the container you can also check the status of the container in the Docker desktop app. You will see the container in the list of containers on the left of the app window. You can inspect the container by clicking on it and see the details of the container.\n\n\n\n\n\n\n\n\nDocker containers in app\n\n\n\n\nStep 8 Copying files from the container to the host\n\nOne way to access the files created inside your container is to mount a directory from your host machine to a directory in the the container. This is done using the -v flag in the docker run command. However, this is not so effective in the example container we are using because the code is completed in a matter of seconds and after that the container is exited.\nInstead we will copy the output files from the container using the CLI. To do this you need to know the container ID of the container you want to copy files from. You can get the container ID by running the command: docker ps -a in the terminal, this will show you a list of all the containers on your computer and in the terminal output you can copy the ID:\n\n\n\n\n\n\n\n\nDocker container ID in terminal\n\n\n\nNow you have the ID in the terminal run the command: docker cp &lt;Container ID&gt;:/home/Output/Visualisations/Regional_size_summary_bar.png ./Output/ and replace the &lt;Container ID&gt; with the ID you copied. The first argument /home/Output/Visualisations/Regional_size_summary_bar.png is the path to the file you want to copy in the container. The second argument ./Output/ is the path to the directory to copy the file to on your host machine, again this is a relative path and the . specifies the current directory. After running the command you should a message printed in the terminal and the file should be copied to the directory you specified:\n\n\n\n\n\n\n\n\nDocker copy output in terminal\n\n\nThat’s it, you have successfully created a Dockerfile, Docker image and container, run your code inside the container and copied the output back to your host machine. If you were to share the Dockerfile with someone else they could build the image and run the container on their machine and get exactly the same results as you. Obviously this is a very simple example but the same principles apply to more complex projects where reproducibility becomes more challenging."
  },
  {
    "objectID": "index.html#version-control-with-git-exercise",
    "href": "index.html#version-control-with-git-exercise",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Version control with Git exercise",
    "text": "Version control with Git exercise\nIn this exercise we will show how version control with Git can be implemented for an example R project. The project is the same that is created in the first workflow exercise, however to save time or in case you haven’t completed this exercise we will start with the finished output from it.\nIf you would prefer to view the exercise script offline, here is a PDF version:  Download exercise instructions\n\nStep 1: Download and configure Git\n\nDownload Git from https://git-scm.com/downloads\nOnce downloaded open the Git terminal window and type in the following with your credentials\n\ngit config --global user.name \"NVHarisena1\" git config --global user.email \"NVHarisena1\\@ethz.ch\" git config --global --list\nThe third command should return your updated user-name and email id.\n\n\nStep 2: Create a repository on Github\nWe will make a quick repository on Github for an individual project, without changing much of the specific configurations since it will be beyond the scope of this workshop.\n\nLogin to your account at https://github.com/\nCreate a new repository by clicking the ‘+’ sign in the top right side of the website or in the ‘Start new repository’ section in the homepage\nProvide a clear name for the repository for e.g. “R_repro_nv” and a quick description like “Test for Reproducible research workshop”\nSet the visibility of the profile to “Public”\nInitialize this repository with: Add a README file.\nSelect a license for your repository in the “Choose your license” section. Check out this website to identify which license works for you. Even though it is optional to add license information to a repository, it is good practice to include this (See more details).\nClick the green ‘create repository’ button\n\n\n\n\n\n\n\n\nSetting up the repository\n\n\n\n\nStep 3: Link created repository to R-Studio\n\nOpen a new session in R studio and create a new project\nIn the ‘New Project Wizard’ navigate to ‘Version Control’&gt;‘Git’\nIn the “repository URL” paste the URL of your new GitHub repository. It will be something like this https://github.com/nvharisena1/R_repro_test.\nAdd folder location where you want the project to be saved locally in your computer in “Create project as subdirectory of” section\nClick ‘Create project’\n\n\n\n\n\n\n\n\nLinking repository to R-Studio\n\n\nYou will see R-studio has now been set up for your local clone of the project to communicate with your online repository. You can see a drop-down to the right of ‘New Branch’ button in the Git tab. This will show you all the branches available to pull or push data to. Your drop-down should show only a ‘main’ branch, since no new branches were created. As stated in the workflow introduction, creating branches is useful for projects with multiple collaborators or sub-themes. Pushing to different branches and then setting up a ‘pull-request’ to merge to the ‘main’ branch allows for systematic version control of the project.\n\n\n\n\n\n\n\nR-Studio new project session with git link\n\n\n\n\n\n\n\n\n\n\nStep 5: Edit the README.md file\n\nOpen the README.md from the file viewer pane\nAdd a description section for the project with a heading and a describing sentence, for e.g. “This project is is a test”.\nAdd license information for the project, for e.g. “This project is licensed under the terms of the MIT license.”\nSave the file.\n\n\n\n\n\n\n\nEdit README.md\n\n\n\n\n\nStep 6: Edit the .gitignore file\nThe git ignore functionality tells git which files to ignore while ‘pushing’ the local changes to the remote (online) repository see more details.In this example we will tell git to ignore all .html files. .html files are created when you preview a file, for example click preview on the edited README.md and a .html should be created.\n\nOpen the .gitignore file and add .html in a new line and save the file\n\n\n\n\n\n\n\n\nEditing the .gitignore file\n\n\n\n\nStep 7: Commit and push\n\nClick the Commit button in the Git tab\nCheck all the files listed in the top left section\nWrite a sentence describing the changes i.e. “Updated exercise, data, Readme and .gitignore”\nClick Commit and close the window\nclick Push in the Git tab, a window will pop up showing the interface with the remote system and details of the upload.\n\n\n\n\n\n\n\n\nCommit changes\n\n\nGreat! You have finished your first project update (local changes committed and pushed) via Git on Rstudio."
  },
  {
    "objectID": "index.html#write-a-paper-with-quarto",
    "href": "index.html#write-a-paper-with-quarto",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Write a paper with Quarto",
    "text": "Write a paper with Quarto\nBecause Quarto is bundled with Rstudio if you have a recent version of Rstudio you may already have Quarto installed. If not, you can install Quarto from CRAN with the following command:"
  },
  {
    "objectID": "index.html#sec-Quarto_exercise",
    "href": "index.html#sec-Quarto_exercise",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Write a paper with Quarto",
    "text": "Write a paper with Quarto\n** Exercise content comoing soon! **"
  },
  {
    "objectID": "presentation.html#quarto-1",
    "href": "presentation.html#quarto-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\n\n\n\nAn open source scientific and technical publishing system\nIntegrates code in multiple programming languages, written material, and interactive visual components\nProduces a range of document formats including HTML, PDF, and Word\nDeveloped by Posit the same company that created Rstudio."
  },
  {
    "objectID": "presentation.html#quarto-2",
    "href": "presentation.html#quarto-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\n\nQuarto website presents many examples of it’s applications\nWe will focus on some of it’s key uses and features that are relevant for academics and producing reproducible research.\n\nAcademic manuscripts\nPresentations\nWebsites\nInteractive dashboards\nData exploration and visualization"
  },
  {
    "objectID": "exercises/Quarto-manuscript_exercise.html#sec-Quarto_exercise",
    "href": "exercises/Quarto-manuscript_exercise.html#sec-Quarto_exercise",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "Exercise content coming soon!"
  },
  {
    "objectID": "presentation.html#quarto-3",
    "href": "presentation.html#quarto-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nWriting academic manuscripts\nHow many programs do you currently use when writing academic papers?\n\nA common workflow of academic papers [8]"
  },
  {
    "objectID": "presentation.html#quarto-4",
    "href": "presentation.html#quarto-4",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nWriting academic manuscripts\nQuarto solves this problem by allowing you to write full academic manuscripts from start to finish including text, code, and visualizations in a single document:"
  },
  {
    "objectID": "presentation.html#quarto-5",
    "href": "presentation.html#quarto-5",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nWriting academic manuscripts\nKey benefits:\n\nFigures and tables are dynamically updated as your code changes\nSupports code in R, Python and Julia as well as LaTeX and Markdown content\nEasy Cross-referencing capability for figures, tables, and sections\nDocuments can be rendered as Word, PDF, or HTML\nInclude Citations and bibliographies using Crossref, DataCite, PubMed and direct integration with Zotero\nQuarto’s .qmd files can be edited with various code/text editors (VS Code, RStudio etc.)\n\n\nMore reproducible as it allows others to use your underlying manuscript file in combination with your data to directly re-create your results."
  },
  {
    "objectID": "presentation.html#quarto-6",
    "href": "presentation.html#quarto-6",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nPresentations\nSeveral formats: RevealJS, Microsoft Powerpoint and Beamer using a common syntax.\nUseful features:\n\nModern themes with functionality to publish your own theme.\nInteractive content: Executable code blocks, graphs, maps\nDynamic resizing of content depending on screen size\nFunctionality for slide notes, automatic transitions, timers etc.\nEasy export to PDF or HTML\nSimilar to manuscripts code-based content is dynamically updated."
  },
  {
    "objectID": "presentation.html#quarto-7",
    "href": "presentation.html#quarto-7",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nWebsites\nWebsites to act as guides, tutorials or teaching materials:"
  },
  {
    "objectID": "presentation.html#quarto-8",
    "href": "presentation.html#quarto-8",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nWebsites\nPersonal websites to share publications and presentations:"
  },
  {
    "objectID": "presentation.html#quarto-9",
    "href": "presentation.html#quarto-9",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nWebsites\nWebsites for research projects to share progress and results:"
  },
  {
    "objectID": "presentation.html#quarto-10",
    "href": "presentation.html#quarto-10",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nDashboards\n\n\n\nArrange multiple interactive or static components in a single page with a highly customizable layout.\nComponents can include text summaries, tables, plots, maps and more.\nUses: Collect feedback on aspects of your research during development or present your results in a visually appealing way.\n\n\n\n\n\nGapminder dashboard from JJallaire"
  },
  {
    "objectID": "presentation.html#quarto-11",
    "href": "presentation.html#quarto-11",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\nData exploration and visualization\n\n\nMany options for interactive data visualisations, tables and diagrams using:\n\nPlotly\nLeaflet\nJupyter Widgets\nhtmlwidgets\nObservableJS\nShiny"
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\n\n\n\n\n\n\nThis visualization introduces the main steps of this workflow. It starts by developing your research project as an Rstudio project following the good practice guidelines we have just discussed. Then, it uses the renv package to manage the project environment so that others can re-create it. Finally, the code, data and environment are uploaded to the open-access repository Zenodo, which provides a DOI for your work, ensuring long-term accessibility and reproducibility."
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline-1",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\nManaging Project Environments with renv\n\nrenv creates project-specific libraries\nCaptures package versions in a renv.lockfile\nEnsures reproducibility of package environment\nCentralizes package environment management within each project\n\n\n\nrenv helps manage R package environments by creating isolated libraries specific to each project, ensuring that the project uses only the packages it needs.\nThe renv.lock file records exact versions of all installed packages, allowing consistent installation of these packages to improve reproducibility.\nThis feature is especially useful for transferring projects between machines, maintaining a controlled environment.\nIt centralizes the package environment management within the project folder, avoiding conflicts with global R libraries."
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline-2",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\nrenv Workflow\n\nInitialize renv inside the project directory to identify dependencies\nSnapshot dependencies to create a lockfile\nRestore environments using renv::restore()\nEasy integration with RStudio for workflow management\n\n\n\nInitialize renv in a project to isolate packages from the system-wide library.\nSnapshot the project’s package dependencies, generating a lockfile (renv.lock).\nRestore the environment by reinstalling packages using the lockfile.\nrenv integrates smoothly with RStudio, making it easy to use alongside other development tools."
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline-3",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\nLimitations of renv\n\nDoes not manage R versions or system-wide dependencies\nFocuses on managing package environments within R\nBest combined with containerization (e.g., Docker) for full reproducibility\nComplements external repositories (e.g., Zenodo) for sharing and preservation\n\n\n\nrenv does not manage system-level dependencies such as the R version or other software your project might rely on (e.g., geospatial libraries like GDAL\nrenv is designed to manage the R package environment, ensuring that the same R packages and versions are used across different machines.\nFor full reproducibility, renv should be combined with tools like Docker for system environment control.\nPublishing platforms like Zenodo can be used to store code and data for long-term preservation."
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline-4",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline-4",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\nPublishing and Archiving with Zenodo\n\nLong-term storage with generous 50GB upload limit per record\nPermanent DOIs for easy citation and versioning support for updates\nGitHub integration for seamless code archiving with DOI snapshots\nSupports FAIR principles: aligned with open access, transparency, and reusability\nCommunity creation for grouping related research outputs\nAPI and open-source: flexible for programmatic access and customization\n\n\n\nZenodo provides long-term storage for a variety of research outputs, including datasets, code, and publications, ensuring that these materials remain accessible over time.\nEvery record receives a permanent Digital Object Identifier (DOI), which allows for easy citation in research papers.\nIntegration with GitHub allows researchers to archive their code and generate DOI-linked snapshots with each release.\nZenodo aligns with the FAIR and Open Science principles, supporting open and reusable research outputs\nThe platform allows the creation of communities to group related records, making it useful for creating a collection of related research outputs, either for a research group or a large-scale funded project.\nZenodo’s API provides programmatic access for tasks like automating record creation, and its open-source nature allows for customization and contribution to the platform."
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline-5",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline-5",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\nStreamlining publishing to Zenodo with zen4R\n\nUpload datasets, code, and metadata from R to Zenodo\nAutomate publication and deposition management\nRetrieve and update Zenodo records directly in R\nFacilitates integration and reproducibility in R workflows\n\n\n\nzen4R allows R users to interact with Zenodo‘s API to upload data and code directly from R.\nIt supports automated publication, including metadata management and record updating.\nRetrieve and update Zenodo records programmatically within R.\nThis makes publishing more efficient, especially in workflows requiring frequent updates or version control."
  },
  {
    "objectID": "presentation.html#rstudio-project-to-zenodo-pipeline-6",
    "href": "presentation.html#rstudio-project-to-zenodo-pipeline-6",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Rstudio project to Zenodo pipeline",
    "text": "Rstudio project to Zenodo pipeline\nCombining renv and Zenodo\n\nrenv manages internal project environments\nZenodo ensures external reproducibility with archiving\nTogether, they provide a comprehensive solution\nAligns with open science and FAIR principles\n\n\n\nrenv manages internal environments by locking package versions and dependencies.\nZenodo provides external storage and ensures reproducibility by archiving and versioning research outputs.\nTogether, they create a comprehensive solution for reproducible, open science projects.\nThis combination aligns with FAIR principles, ensuring data is Findable, Accessible, Interoperable, and Reusable."
  },
  {
    "objectID": "presentation.html#containerization-with",
    "href": "presentation.html#containerization-with",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Containerization with ",
    "text": "Containerization with"
  },
  {
    "objectID": "presentation.html#containerization-with-1",
    "href": "presentation.html#containerization-with-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Containerization with ",
    "text": "Containerization with \nWhat is containerization?"
  },
  {
    "objectID": "presentation.html#containerization-with-2",
    "href": "presentation.html#containerization-with-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Containerization with \n\n",
    "text": "Containerization with \n\n\nWhat is Docker?"
  },
  {
    "objectID": "presentation.html#using-docker-with-r-1",
    "href": "presentation.html#using-docker-with-r-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Using Docker with R",
    "text": "Using Docker with R"
  },
  {
    "objectID": "presentation.html#docker-with-renv-1",
    "href": "presentation.html#docker-with-renv-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Docker with renv",
    "text": "Docker with renv\nWhy Git and GitHub?\n\nVersion control: A more systematic way to organise data beyond “dataprep_1”, “dataprep_final”, “dataprep_finalfinal” etc.\nSystematic documentation and storage of code changes allowing us to track changes and revert back to previous versions when needed.\n\nTerminologies\n\nPush and Pull?\n\n\nGit terminologies\nGit cheatsheet: https://education.github.com/git-cheat-sheet-education.pdf\n\nSteps for using Git and GitHub (To be done in the exercises - basic)\n\nCreate a GitHub repository in your account\nDownload and install Git\nAdd credentials for your account to Git\nLink RProject to Github repository\nOpen, checkout and navigate Git repository local version via Rstudio\nBasic functionalities of Git in Rstudio\n\nAdditional fucntionalities\n\nGitHub repositories can be archived on Zenodo and thus get a DOI.\nGitHub actions\nGitHub releases can be continuously integrated into DockerHub or GitHub Packages"
  },
  {
    "objectID": "presentation.html#about-us-1",
    "href": "presentation.html#about-us-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "About us",
    "text": "About us\nBen Black\n\n\n\n\n\n\n\n\n\n \n\n\n blenback.github.io  bblack@ethz.ch  Ben Black  @blenback  Black  @Blen_Back"
  },
  {
    "objectID": "presentation.html#about-us-2",
    "href": "presentation.html#about-us-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "About us",
    "text": "About us\nNivedita Harisena"
  },
  {
    "objectID": "presentation.html#about-us-3",
    "href": "presentation.html#about-us-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "About us",
    "text": "About us\nManuel Kurmann"
  },
  {
    "objectID": "presentation.html#what-is-reproducible-research-1",
    "href": "presentation.html#what-is-reproducible-research-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "What is reproducible research?",
    "text": "What is reproducible research?\nLet’s hear your thoughts: What does reproducible research mean to you?\n\n\n\n\n\n\nhttps://www.menti.com/alsw49tprwu7"
  },
  {
    "objectID": "presentation.html#the-fair-standard",
    "href": "presentation.html#the-fair-standard",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "The FAIR standard",
    "text": "The FAIR standard\n\nFindability, Accessibility, Interoperability, and Reusability (FAIR).\nDeveloped by diverse stakeholders (academia, industry, funders, publishers).\nAddressed the need for infrastructure supporting data reuse.\nEmphasis on both human and machine readability.\n\n[1]"
  },
  {
    "objectID": "presentation.html#why-strive-for-reproducible-research-1",
    "href": "presentation.html#why-strive-for-reproducible-research-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Why strive for reproducible research?",
    "text": "Why strive for reproducible research?"
  },
  {
    "objectID": "presentation.html#why-strive-for-reproducible-research-2",
    "href": "presentation.html#why-strive-for-reproducible-research-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Why strive for reproducible research?",
    "text": "Why strive for reproducible research?\n\nReplication crisis: Allows our work to be verified more thoroughly\nImproves science for all: Allows others to more easily build upon our work\n\nDon’t just take our word for it, research funders are increasingly focused on reproducible research too: EXAMPLE"
  },
  {
    "objectID": "presentation.html#why-for-reproducible-research-1",
    "href": "presentation.html#why-for-reproducible-research-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Why  for reproducible research?",
    "text": "Why  for reproducible research?\n\nOpen source\nLarge active user community for support\nPackages to suit just about every research need: statistics, modelling, spatial analysis, visualisation (Many packages developed by academics)\n\nBut just using  doesn’t necessarily make your research reproducible…"
  },
  {
    "objectID": "presentation.html#workshop-concept",
    "href": "presentation.html#workshop-concept",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Workshop concept",
    "text": "Workshop concept\n\nWe have put together this workshop to share some advice for best practice and tips that we have picked up along the way."
  },
  {
    "objectID": "presentation.html#research-projects-with-r-1",
    "href": "presentation.html#research-projects-with-r-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Research projects with R",
    "text": "Research projects with R\n\nJenny Bryan: A good R project… “creates everything it needs, in its own workspace or folder, and it touches nothing it did not create.” [2]\n\nProjects should be ‘self-contained’\nAdditional caveat: a good R project should explain itself."
  },
  {
    "objectID": "presentation.html#research-projects-with-r-2",
    "href": "presentation.html#research-projects-with-r-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Research projects with R",
    "text": "Research projects with R\n\nGraphical overview of components of a good research project in R\n\nWe will approach this topic by splitting it up into 6 topics which are highlighted in this graphic.\nAs we move through the 6 topics you will see that there are areas of overlap and complementarity between them.\nThese topics are also central to the choice of approaches in the three workflows for reproducibility that we will share."
  },
  {
    "objectID": "presentation.html#rstudio-projects",
    "href": "presentation.html#rstudio-projects",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\n\nRecognise this?\n\nsetwd(\"C:/Users/ben/path/that/only/I/have\")\n\nBut what’s the problem with it?\n\n\nThis path is only relevant for the author and not other users.\nEven for the author it will be invalid if they change computers."
  },
  {
    "objectID": "presentation.html#rstudio-projects-1",
    "href": "presentation.html#rstudio-projects-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nStay away from setwd()!\nUse Rstudio Projects:\n\nDesignates new or existing folders as working directory creating an .RProj file within them.\nWhen you open a project the working directory will automatically be set and all paths will be relative to this.\nThe .Rproj can be shared along with the rest of the research project, users can easily open the project to have the same working directory."
  },
  {
    "objectID": "presentation.html#rstudio-projects-2",
    "href": "presentation.html#rstudio-projects-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nCreating projects\nGo to File &gt; New Project, can be created in a new or existing directory"
  },
  {
    "objectID": "presentation.html#rstudio-projects-3",
    "href": "presentation.html#rstudio-projects-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nOpening projects\nUsing File &gt; Open Project in the top left of Rstudio."
  },
  {
    "objectID": "presentation.html#rstudio-projects-4",
    "href": "presentation.html#rstudio-projects-4",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nOpening projects\nUsing the drop down menu in the top-right of the Rstudio session."
  },
  {
    "objectID": "presentation.html#rstudio-projects-5",
    "href": "presentation.html#rstudio-projects-5",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nOpening projects\nOutside of R by double clicking on the .Rproj file in the folder."
  },
  {
    "objectID": "presentation.html#rstudio-projects-6",
    "href": "presentation.html#rstudio-projects-6",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nUtilising project specific .Rprofile’s\n\nRstudio projects can store project-specific settings using the .Rprofile file.\nFile is run every time the project is opened, can be used to perform actions such as opening a particular script:\n\n\nsetHook(\"rstudio.sessionInit\", function(newSession) {\n  if (newSession)\n    # Open the script specificed by the path\n    rstudioapi::navigateToFile('scripts/script_to_open.R', line = -1L, column = -1L)\n}, action = \"append\")"
  },
  {
    "objectID": "presentation.html#rstudio-projects-7",
    "href": "presentation.html#rstudio-projects-7",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "1. Rstudio projects",
    "text": "1. Rstudio projects\nUtilising project specific .Rprofile’s\nThe easiest way to create and edit .Rprofile files is to use the functions from the package usethis:\n\n# Note the use of scope = \"project\" to create a project specific .Rprofile\nusethis::edit_r_profile(scope = \"project\")"
  },
  {
    "objectID": "presentation.html#environment-management",
    "href": "presentation.html#environment-management",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\nFamiliar lines from the beginning of many an R script:\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nAgain, what is wrong?"
  },
  {
    "objectID": "presentation.html#environment-management-1",
    "href": "presentation.html#environment-management-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\n\nNo indication of version of package to be installed =\n\nPotential for to break code\nIntroduce dependency conflicts\n\n\n\n\n\nPackage dependencies of popular R package [3]\n\n\n\nNo indication of what version of package is to be installed and hence if the code installing this package is old it may not work with the most recent version of the package (This is less of a problem for well established packages like the Tidyverse but for less common packages, that may see large changes between versions, it could be substantial).\nSecondly, having the user install an unspecified version of a package could also cause dependency conflicts with other packages required by the code. This is because almost all packages have some form of dependency (i.e. they use the functionality of) on other packages.\nThis is shown aptly by the image below which, while out-dated now, showed that in 2014 to install the 7 most popular R packages at the time would actually install 63 packages in total when considering their dependencies."
  },
  {
    "objectID": "presentation.html#environment-management-2",
    "href": "presentation.html#environment-management-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\nBut the problem is bigger than just packages…\nWhen your code runs it is also utilizing:\n\nA specific version of R\nA specific operating system\nSpecific versions of system dependencies, i.e. other software that R packages utilise.\n\nCollectively, these are the Environment of your code, documenting and managing this is essential ensure reproducibilty"
  },
  {
    "objectID": "presentation.html#environment-management-3",
    "href": "presentation.html#environment-management-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\nBut how to manage your environment?\n\nDifferent approaches that range in complexity hence maybe suited to some projects and not others.\nMost user-friendly way to manage your package environment (caveat to be discussed) in R: renv package.\n\n\nrenv will form a central part of the three workflows for reproducibility that we will present later."
  },
  {
    "objectID": "presentation.html#environment-management-4",
    "href": "presentation.html#environment-management-4",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\nCreating reproducible environments with renv\nrenv helps you create reproducible environments for your R projects by:\n\nDocumenting your package environment\nProviding functionality to re-create it."
  },
  {
    "objectID": "presentation.html#environment-management-5",
    "href": "presentation.html#environment-management-5",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\nCreating reproducible environments with renv\n\nNormally all your R packages are stored in a single library on your machine (system library).\nrenv creates a project specific libraries of packages (renv/library) which contain all the packages used by your project.\nrenv also creates project specific lockfiles (renv.lock) which contain sufficient metadata so that the project library can be re-installed on a new machine.\n\nResult: Different projects can use different versions of packages and installing, updating, or removing packages in one project doesn’t affect any other project."
  },
  {
    "objectID": "presentation.html#environment-management-6",
    "href": "presentation.html#environment-management-6",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "2. Environment management",
    "text": "2. Environment management\nrenv limitation\nrenv is not intended to manage other aspects of your environment such as: tracking your version of R or your operating system.\nThis is why if you want ‘bullet-proof’ reproducibility renv needs to be used alongside other approaches such as containerization."
  },
  {
    "objectID": "presentation.html#writing-clean-code",
    "href": "presentation.html#writing-clean-code",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\n\nThere is no objective measure that makes code ‘clean’ vs. ‘un-clean’.\nThink of ‘clean coding’ as the pursuit of making your code easier to read, understand and maintain."
  },
  {
    "objectID": "presentation.html#writing-clean-code-1",
    "href": "presentation.html#writing-clean-code-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nCode styles\n\nLike writing, code should follow a set of rules and conventions. For example, in English, a sentence starts with a capital letter and ends with a full stop.\nFor R code there is not a single set of conventions instead there are numerous styles. Two most common are the Tidyverse style and the Google R style.\n\nMost important: Choose a style and apply it consistently in your coding."
  },
  {
    "objectID": "presentation.html#writing-clean-code-2",
    "href": "presentation.html#writing-clean-code-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nCode styles\nCode styles express opinionated preferences on a series of common topics:\n\nObject naming\nUse of assignment operators\nSpacing\nIndentation\nLine length\nParentheses placement\n\nWe won’t discuss in detail but you should read one of the style guides when you have the time.\n\nCode styles express opinionated preferences on a series of common topics such as: Object naming, use of assignment operators, spacing, indentation, line length, parentheses placement, etc.\nRather than detail all of these topics here we will focus on just on some related tips that we think are most relevant for scientific research coding, including how to automate the formatting of your code to a particular style. However, we encourage you to go through the different style guides when you have the time."
  },
  {
    "objectID": "presentation.html#writing-clean-code-3",
    "href": "presentation.html#writing-clean-code-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nAutomating the styling of your code\nTwo R packages for code styling, lintr and styler:\n\nlintr checks your code for style issues and potential programming errors then presents them to you to correct, like doing a ‘spellcheck’ on a written document.\nstyler automatically format’s your code to a particular style, the default of which is the tidyverse style."
  },
  {
    "objectID": "presentation.html#writing-clean-code-4",
    "href": "presentation.html#writing-clean-code-4",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nAutomating the styling of your code\n\nTo use lintr and styler call their functions like any package\nstyler can also be used through the Rstudio Addins menu below the Navigation bar: \nBoth packages can be used as part of a continuous integration (CI) workflow with Github, meaning that their functions can be run automatically when you update your code."
  },
  {
    "objectID": "presentation.html#writing-clean-code-5",
    "href": "presentation.html#writing-clean-code-5",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nScript headers\n\nStarting your scripts with a consistent header containing information about it’s purpose, author/s, creation and modification dates is very helpful!\nThere are no rules as to what this should look like but this is an example:"
  },
  {
    "objectID": "presentation.html#writing-clean-code-6",
    "href": "presentation.html#writing-clean-code-6",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nScript headers\n\n\nTo save time inserting your script header use Rstudio’s Code snippets feature.\nCode snippets are text macros that insert a section of code using a keyword.\nTo create your own Code snippet go to Tools &gt; Global Options &gt; Code &gt; Edit Snippets and then add a new snippet with your code below it"
  },
  {
    "objectID": "presentation.html#writing-clean-code-7",
    "href": "presentation.html#writing-clean-code-7",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nScript headers\nTo use a code snippet simply start typing the keyword in the script and the auto-completion list will appear then press Tab and the code section will be inserted:"
  },
  {
    "objectID": "presentation.html#writing-clean-code-8",
    "href": "presentation.html#writing-clean-code-8",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nCode sections\n\nBraced ({}) sections of code (i.e. function definitions, conditional blocks, etc.) can be folded to hide their contents by clicking on the small triangle in the left margin:\n\n\n\nBut you can also create custom named code sections to break longer scripts according to specific parts of the analysis."
  },
  {
    "objectID": "presentation.html#writing-clean-code-9",
    "href": "presentation.html#writing-clean-code-9",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nCode sections\n\nCode sections are created by inserting a comment line that contains at least four trailing dashes (-), equal signs (=), or pound signs (#):\n\n\nAlternatively you can use the Code &gt; Insert Section command."
  },
  {
    "objectID": "presentation.html#writing-clean-code-10",
    "href": "presentation.html#writing-clean-code-10",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nCode sections\nTo navigate between code sections:\n\nUse the Jump To menu available at the bottom of the editor[4]"
  },
  {
    "objectID": "presentation.html#writing-clean-code-11",
    "href": "presentation.html#writing-clean-code-11",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "3. Writing clean code",
    "text": "3. Writing clean code\nCode sections\nTo navigate between code sections:\n\nUse the document outline pane in the top right corner of the source pane"
  },
  {
    "objectID": "presentation.html#workflow-decomposition",
    "href": "presentation.html#workflow-decomposition",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "4. Workflow decomposition",
    "text": "4. Workflow decomposition\n\nWorkflow decomposition is the structuring or compartmentalising of code into seperate logical parts that makes it easier to maintain [5].\nYou probably already instinctively do decomposition by splitting typical processes such as:\n\nData preparation\nStatistical modelling\nAnalysis of results\nProducing final visualizations\n\nThis oftens leads to scripts with logical sounding names like: Data_prep.R and Data_analysis.R but can others be expected to know which order these must be run in?"
  },
  {
    "objectID": "presentation.html#workflow-decomposition-1",
    "href": "presentation.html#workflow-decomposition-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "4. Workflow decomposition",
    "text": "4. Workflow decomposition\nSolutions:\n\n1st step: Give your scripts sequential numeric tags in their names, e.g. 01_Data_prep.R, 02_Data_analysis.R ensuring that they are presented in numerical order in their designated directory.\nNext level: Create a Master script that sources your other scripts in sequence (think of them as sub-scripts) so that users need only run one script."
  },
  {
    "objectID": "presentation.html#workflow-decomposition-2",
    "href": "presentation.html#workflow-decomposition-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "4. Workflow decomposition",
    "text": "4. Workflow decomposition\n\nTo do this create the master script as you would any normal R script (File &gt; New File &gt; R script) and then use the base::source() function to run the sub-scripts:\n\n\n#############################################################################\n## Master_script: Run steps of research project in order\n#############################################################################\n\n#Prepare LULC data\nsource(\"Scripts/Preparation/Dep_var_dat_prep.R\", local = scripting_env)\n\n#Prepare predictor data\nsource(\"Scripts/Preparation/Ind_var_data_prep.R\", local = scripting_env)\n\n\nAnother advantage of this approach is that all sub-scripts can utilise the same environment (defined by the source(local= ) argument)."
  },
  {
    "objectID": "presentation.html#workflow-decomposition-3",
    "href": "presentation.html#workflow-decomposition-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "4. Workflow decomposition",
    "text": "4. Workflow decomposition\n\nWithin your sub-scripts processes should also be seperated into code sections and any repetitive tasks should be performed with custom functions.\nFollowing this approach you end up with a workflow that will look something like this:\n\n\n\nThe benefit of this hierarchical approach to structuring is that it is not only easier to debug and maintain individual processes but it is also more amenable to adding new processes."
  },
  {
    "objectID": "presentation.html#structuring-your-project-directory",
    "href": "presentation.html#structuring-your-project-directory",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "5. Structuring your project directory",
    "text": "5. Structuring your project directory\n\nA clean project directory that has well-organised sub-directories makes your projects code easier to understand for others.\nTry to use:\n\nLogical naming\nA consistent style (i.e. use of captialisation and seperators).\nNested sub-directories e.g data/raw/climatic/precipitation/2020/precip_2020.rds vs. data/precip_2020_raw.rds (helpful when it comes to programatically constructing file paths)"
  },
  {
    "objectID": "presentation.html#structuring-your-project-directory-1",
    "href": "presentation.html#structuring-your-project-directory-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "5. Structuring your project directory",
    "text": "5. Structuring your project directory\nAs an example my go-to project directory structure looks like this:\n\n└── my_project\n    ├── data # The research data\n    │   ├── raw\n    │   └── processed\n    ├── output # Storing results\n    ├── publication # Containing the academic manuscript of the project\n    ├── src # For all files that perform operations in the project\n    │   ├── scripts\n    │   └── functions\n    └── tools # Auxilliary files and settings"
  },
  {
    "objectID": "presentation.html#structuring-your-project-directory-2",
    "href": "presentation.html#structuring-your-project-directory-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "5. Structuring your project directory",
    "text": "5. Structuring your project directory\n\nCreation of project directory structure can be automated using using Rstudio’s Project Templates functionality.\nAllows selection of custom template when creating a new Rstudio project (File &gt; New Project &gt; New Directory &gt; New Project Template).\nWarning: Implementation of personal template is labor intensive as it needs to be contained within an R-package. But several template packages appropriate for scientific research projects are available:\n\nrrtools\nProjectTemplate\ntemplate\naddinit"
  },
  {
    "objectID": "presentation.html#project-documentation",
    "href": "presentation.html#project-documentation",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\n\nSinger 2024"
  },
  {
    "objectID": "presentation.html#project-documentation-1",
    "href": "presentation.html#project-documentation-1",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nBut writing comprehensive documentation that covers all aspects of projects is time-consuming…\nSuggested solution in the R research community: Research as package approach (i.e. creating your project as an R-package) [6].\nPro: R-packages have an existing strict set of conventions for documentation\nCons:\n\nLearning curve for those unfamiliar with R-packages\nMay not be appropriate for all project requirements."
  },
  {
    "objectID": "presentation.html#project-documentation-2",
    "href": "presentation.html#project-documentation-2",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nOur advice: don’t let the perfect be the enemy of the good and focus on these key areas:\n\nProvide adequate in-script commentary: Remember that comments should be used to explain the purpose of the code, not what the code is doing\nDocument your functions with roxygen skeletons\nInclude a README file: README files are where you should document your project at the macro-level i.e. what it is about and how it is supposed to work.\n\n\nThe latter of these two are more detailed so we have provided further information and tips in sections below."
  },
  {
    "objectID": "presentation.html#project-documentation-3",
    "href": "presentation.html#project-documentation-3",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nFunction documentation with roxygen2\n\nbase R provides a standard way of documenting functions in packages as seperate .Rd (R documentation) files.\n.Rd files use a custom syntax to detail key aspects of the functions such as input parameters, outputs, package dependencies [7].\nDocumenting functions in this way is a good practice for your project even if you are not creating a package.\n\n\nIn the case of many research projects you will not be creating a package however it is still useful to apply this documentation style to your functions as it is a good way to make them understandable and easier to modify by others. For example, having clear information about the object (e.g. a vector or data.frame) that a function accepts, saves others time in guessing what the function is expecting if they are trying to use new data."
  },
  {
    "objectID": "presentation.html#project-documentation-4",
    "href": "presentation.html#project-documentation-4",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nFunction documentation with roxygen2\n\nRather than manually create .Rd files, we can use the roxygen2 package.\nroxygen2 provides functionality to add blocks of comments (roxygen skeleton) to the top of the function scripts. These are then used to automatically generate .Rd files.\nTo add a roxygen skeleton, place your cursor inside a function you want to document and press Ctrl + Shift + R (or Cmd + Shift + R on Mac) or you can go to code tools &gt; insert roxygen skeleton (wand icon in the top row of the source pane).\n\n\nHowever, rather than manually writing .Rd files, we can use the roxygen2 package to automatically generate these files from a block of comments that are added to the top of the function scripts. To add this comment block, place your cursor inside a function you want to document and press Ctrl + Shift + R (or Cmd + Shift + R on Mac) or you can go to code tools &gt; insert roxygen skeleton (code tools is represented by the wand icon in the top row of the source pane)."
  },
  {
    "objectID": "presentation.html#project-documentation-5",
    "href": "presentation.html#project-documentation-5",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nFunction documentation with `roxygen2\n\nWhen you insert the roxygen block it will already contain the names of the function, its arguments and any returns. You can then fill in the rest of the information, such as the description and dependencies etc.\n\n\nInserting roxygen block"
  },
  {
    "objectID": "presentation.html#project-documentation-6",
    "href": "presentation.html#project-documentation-6",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nTips for README writing\n\nR packages or projects typical have README.md files.\n.md is the Markdown format which is the most common format for README files in R projects because it can be read by many programs and rendered in a variety of formats.\nREADME.md files are often accompanied by the corresponding file README.Rmd, an Rmarkdown file which generates them.\nREADME.Rmd files can be created using the usethis package (use_readme_rmd()).\nHowever, depending on anticipated project users creating the README as a raw text file (.txt) may be better.\n\n\nThese files are often accompanied by the corresponding file README.Rmd which generates the README.md file. Markdown format is used for README’s because it can be read by many programs and rendered in a variety of formats. In this sense writing the README for your project in markdown makes sense and there tools available to help you do this such as the usethis package which has a function use_readme_rmd() that will create a README.Rmd file for you. However, depending on who you anticipate using your project you may also want to create your README as a raw text file (.txt) which may be a more familiar format for some users and again can be opened by many different programs."
  },
  {
    "objectID": "presentation.html#project-documentation-7",
    "href": "presentation.html#project-documentation-7",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nTips for README writing\n\nNo single standardised format for what should be included but here is an example of a README.txt file from one of the authors publications.\nUseful to include a tree diagram of the project directory structure down to the file level:\n\n#| eval: false\n#| echo: true\n├── Data\n│   ├── Processed\n│   │   └── RiceFarms_summary.csv\n│   └── Raw\n│       └── RiceFarms.csv\n├── Output\n│   └── Regional_size_summary_bar.png\n└── Scripts\n    ├── 01_data_analysis.R\n    └── 02_data_visualisation.R\n\nYou will see that one of the things this README includes is a tree diagram which shows the directory structure of the project right down to the file level. This is a useful way to give an overview of what users should find included in the project and then explanatory notes can be added to explain the purpose of each file or directory. Such a diagram can be easily generated using the fs package:"
  },
  {
    "objectID": "presentation.html#project-documentation-8",
    "href": "presentation.html#project-documentation-8",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "6. Project documentation",
    "text": "6. Project documentation\nTips for README writing\n\nSuch a diagram can be easily generated using the fs package:\n\n\ninstall.packages(\"fs\")\nlibrary(fs)\n\n#vector path of the target directory to make a file tree from\nTarget_dir &lt;- \"Your_dir\"\n\n#produce tree diagram of directory sub-dirs and files and save output using capture.ouput from base R utils.\ncapture.output(dir_tree(Target_dir), file= 'Dir_tree_output.txt')"
  },
  {
    "objectID": "presentation.html#summary",
    "href": "presentation.html#summary",
    "title": "Reproducible Research with  and \n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n: Workflows for data, projects and publications",
    "section": "Summary",
    "text": "Summary\nNow this some of the details of the graphical overview probably make more sense to you:\n\nWe will implement some of these good practices in our 1st exercise."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "",
    "text": "Introduction (15 mins)\nResearch projects with R (30 mins)\nComfort break (10 mins)\n3 workflows for Reproducibility (20 mins)\nQuarto (20 mins)\nComfort break (10 mins)\nExercise time (45 mins)\nQ&A + feedback (30 mins)",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#about-us",
    "href": "presentation.html#about-us",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "About us",
    "text": "About us\nWe are four researchers from the research group Planning of Landscape and Urban Systems (PLUS) at ETH Zürich. Click on the social icons below our pictures to find out more about our individual research or get in touch with us.\n\n\n\n\nBen Black\nDoctoral Researcher\n    \n\n\n\n\n\nNivedita Harisena\nDoctoral Researcher\n   \n\n\n\n\n\nManuel Kurmann\nResearch Assistant\n   \n\n\n\n\n\nMaarten Van Strien\nSenior scientist",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#what-is-reproducible-research",
    "href": "presentation.html#what-is-reproducible-research",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "What is reproducible research?",
    "text": "What is reproducible research?\nReproducibility is a key aspect of reliable scientific research. It enables other researchers to reproduce the same results using the original data, code, and documentation [1]. Below are the core principles to ensure reproducibility in research:\nStarts with planning\nReproducibility begins during the planning stage. It is essential to organize data management and ensure clear protocols are in place even before starting the analysis. Consistent Data Storage Regular backups of data are crucial. Storing data in multiple locations ensures accessibility and minimizes the risk of data loss. [2]\nContains clear documentation\nThorough documentation is essential to guarantee that data and methods can be accurately interpreted and reproduced by others. This entails the use of well-organised files and the inclusion of metadata that describes the data, how it was obtained, and how it was processed. [[2]][3]\nUtilizes version control\nUsing version control systems helps track changes in the project over time. This approach preserves the history of the project and facilitates the reversion of files to a previous state in the event of an error. [2]\nIs accessible\nData should be stored in nonproprietary, portable formats to ensure broad accessibility and long-term usability. This practice ensures that researchers can access the data without relying on specific software tools. Making data and code publicly available in accessible repositories supports scientific transparency and allows broader use of research outputs. [[2]][3]\nBy following these steps, researchers contribute to the wider scientific community, ensuring that their work can be efficiently and accurately reproduced by others.\nIntroducing the FAIR Principles\nWhile the principles above lay the groundwork for reproducibility, the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) provide a more comprehensive framework for enhancing the value of research data in the digital age. These principles expand on reproducibility by emphasizing not only human access to research outputs but also machine actionability, ensuring that data can be effectively found, accessed, and reused by both people and computational tools​. [4]\nHow FAIR Principles Build on Reproducibility\nThe FAIR principles naturally complement and expand on the core aspects of reproducible research:\n\nFindability reinforces the importance of clear documentation. Assigning persistent identifiers and providing rich metadata makes it easier for researchers and search tools to locate and understand datasets, ensuring that your research remains accessible over time.\nAccessibility builds on the concept of using nonproprietary formats. FAIR emphasizes that data should be retrievable using open, standardized protocols, which ensures long-term access to both the data and its metadata, even if the data itself becomes unavailable.\nInteroperability relates to the consistent use of data standards and version control. By using standardized formats and vocabularies, research data can be more easily integrated with other datasets, supporting reuse and long-term relevance in broader research contexts.\nReusability directly aligns with the goals of reproducible research by ensuring that data is accompanied by clear licensing and provenance, allowing others to confidently reuse it. This principle reinforces the need for thorough documentation and transparent methods.\n\nBy incorporating the FAIR principles, researchers ensure that their data not only meets the standards of reproducibility but is also optimized for long-term use and discovery. This fosters a research environment where data is more transparent, accessible, and impactful over time​. [4]",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#why-strive-for-reproducible-research",
    "href": "presentation.html#why-strive-for-reproducible-research",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Why strive for reproducible research?",
    "text": "Why strive for reproducible research?\nIn recent years, various scientific disciplines have experienced what is known as a “replication crisis”. This crisis arises when researchers are unable to reproduce the headline results of key studies using the reported data and methods [[5]][6] [7]. This lack of reproducibility undermines public trust in science, as it raises doubts about the validity of research findings.\n\nAdvantages of Reproducibility for Your Research\nPersonal Reference\nConducting reproducible research simplifies the process of remembering how and why specific analyses were performed. This makes it easier to explain your work to collaborators, supervisors, and reviewers, enhancing communication throughout your project. [2]\nEfficient\nModifications Reproducible research enables you to quickly adjust analyses and figures when requested by supervisors, collaborators, or reviewers. This streamlined process can save substantial time during revisions. [2]\nStreamlined Future Projects\nBy maintaining well-organized and reproducible systems, you can reuse code and organizational structures for future projects. This reduces the time and effort required for similar tasks in subsequent research. [2]\nDemonstrates Rigor and Transparency\nReproducibility demonstrates scientific rigor and transparency. It allows others to verify your methods and results, improving the peer review process and reducing the risk of errors or accusations of misconduct. [2]\nIncreases Impact and Citations\nMaking your research reproducible can lead to higher citation rates [8] [9]. By sharing your code and data, you enable others to reuse your work, broadening its impact and increasing its relevance in the scientific community. [10] [11].\n\n\nAdvantages of Reproducibility for Other Researchers\nFacilitates Learning\nSharing data and code helps others learn from your work more easily. New researchers can use your data and code as a reference, speeding up their learning curve and improving the quality of their analyses. [2]\nEnables Reproducibility\nReproducible research makes it simpler for others to reproduce and build upon your work, fostering more compatible and robust research across studies. [2]\nError Detection\nBy allowing others to access and review your data and code, reproducibility helps detect and correct errors, ensuring that mistakes are caught early and reducing the chance of their propagation in future research. [2]",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#why-for-reproducible-research",
    "href": "presentation.html#why-for-reproducible-research",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Why  for reproducible research?",
    "text": "Why  for reproducible research?\nR is increasingly recognized as a powerful tool for ensuring reproducibility in scientific research. Here are some key advantages of using R for reproducible research:\nOpen Source\nAccessibility R is freely available to everyone, eliminating cost barriers and promoting inclusive access to research tools. This open-source model ensures that researchers around the world can use and contribute to its development, fostering a collaborative research environment. [3]\nComprehensive Documentation\nR encourages thorough documentation of the entire research process. This ensures that analyses are well-tracked and can be easily replicated across different projects, enhancing the overall transparency and reliability of the research.\nIntegrated Version Control\nR seamlessly integrates with version control systems like Git, allowing researchers to track changes to code, data, and documents. This helps maintain a detailed record of a project’s evolution and ensures that all steps are easily reproducible. [3]\nConsistency Across Platforms\nR provides a stable environment that works consistently across different operating systems, whether you are using Windows, Mac, or Linux. This cross-platform consistency greatly enhances the reproducibility of research across diverse systems.\nBroad Community Support\nThe R community is large and active, continuously contributing to the improvement of the software. This broad support makes R a reliable choice for long-term research projects, ensuring that new tools and methods are constantly being developed and shared.\nFlexibility and Adaptability\nR offers a wide range of tools and functions that can be adapted to various research needs. This flexibility allows researchers to handle diverse tasks within a reproducible framework, making it a versatile tool for projects of all kinds.",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#sec-projects",
    "href": "presentation.html#sec-projects",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": " projects",
    "text": "projects\nHow many times have you opened an R script and been greeted by this line:\nWhile it is well-intentioned (i.e. avoiding the need to have full paths for all objects that will subsequently be loaded or daved ) the problem with it is obvious: This specific path is only relevant for the author and not other potential users and even for the author it will be invalid if they happen to change computers. The good news is there is a very simple way to avoid having to use setwd() at all by using Rstudio Projects.\nRstudio projects designate new or existing folders as a defined working directory by creating an .RProj file within them. This means that when you open a project the working directory of the Rstudio session will automatically be set to the directory that the .RProj file is located in and the paths of all files in this folder will be relative to this.\nThe .Rproj file can be shared along with the rest of the research project files meaning that others users can easily open the Project to have the same working directory removing the need for those troublesome setwd() lines.\n\nCreating and opening projects\nCreating an Rstudio project is as simple as using File &gt; New Project in the top left and then choosing between creating the Project in a new or existing directory.\nThere are several ways to open a Project:\n\nUsing File &gt; Open Project in the top left of Rstudio.\n\n\n\n\n\n\n\nUsing the drop down menu in the top-right of the Rstudio session.\n\n\n\n\n\n\n\nOutside of R by double clicking on the .Rproj file in the folder.\n\n\n\n\n\n\n\n\nUtilising project specific .Rprofile’s\nAnother useful feature of Rstudio projects is the ability to store project-specific settings using the .Rprofile file which controls the initialisation behaviour of the R session when the project is opened. A useful application of this for reproducible research projects is automatically open a particular script, for example a master script that runs all the code in the project (which is a concept that will discussed under workflow decomposition).\nTo do this the contents of your .Rprofile file would like this:\nThe easiest way to create and edit .Rprofile files is to use the functions from the package usethis:",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#sec-environment-management",
    "href": "presentation.html#sec-environment-management",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Environment management",
    "text": "Environment management\nThese lines of code are also probably familiar from the beginning of many an R script:\nBut what is wrong with these lines?\nWell firstly, there is no indication of what version of the package is to be installed and hence if the code installing this package is old it may not work with the most recent version of the package (This is less of a problem for well established packages like the Tidyverse but for less common packages, that may see large changes between versions, it could be substantial).\nSecondly, having the user install an unspecified version of a package could also cause dependency conflicts with other packages required by the code. This is because almost all packages have some form of dependency (i.e. they use the functionality of) on other packages. This is shown aptly by the image below which, while out-dated now, showed that in 2014 to install the 7 most popular R packages at the time would actually install 63 packages in total when considering their dependencies.\n\n\n\nPackage dependencies of popular R package [13]\n\n\nHowever the problem is bigger than just packages because when your code runs it is also utilising:\n\nA specific version of R\nA specific operating system\nSpecific versions of system dependencies, i.e. other software in other languages that R packages themselves utilise e.g GDAL for spatial analysis packages like terra.\n\nAll of these things together make up what is known as the ‘environment’ of your code. Hence the process of documenting and managing this environment to is ensure that your code is reproducible (i.e. it not only runs but also consistently produces the same results).\nThere are different approaches to environment management that differ in their complexity and hence maybe suited to some projects and not others. For the purpose of this workshop we will focus on what we have found is one of the most user-friendly ways to manage your package environment (caveat that will be discussed) in R which is the package renv. Below we will introduce this package in more detail as it will form a central part of the three workflows for reproducibility that we present.\n\nCreating reproducible environments with renv\nAs mentioned above renv is an R package that helps you create reproducible environments for your R projects by not only documenting your package environment but also providing functionality to re-create it.\nIt does this by creating project specific libraries (i.e. directories: renv/library) which contain all the packages used by your project. This is different from the default approach to package usage and installation whereby all packages are stored in a single library on your machine (system library). Having separate project libraries means “that different projects can use different versions of packages and installing, updating, or removing packages in one project doesn’t affect any other project.” [14]. In order to make sure that your project uses the project library everytime it is opened renv utilises the functionality of .Rprofile's to set the project library as the default library.\nAnother key process of renv is to create project specific lockfiles (renv.lock) which contain sufficient metadata about each package in the project library so that it can be re-installed on a new machine.\nAs alluded to, renv does a great job of managing your packages but is not intended to manage other aspects of your environment such as: tracking your version of R or your operating system. This is why if you want ‘bullet-proof’ reproducibility renv needs to be used alongside other approaches such as containerization which is the 3rd and most complex workflow we will discuss.",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#sec-writing-clean-code",
    "href": "presentation.html#sec-writing-clean-code",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Writing clean code",
    "text": "Writing clean code\nThe notion of writing ‘clean’ code can be daunting, especially for those new to programming. However, the most important thing to bear in mind is that there is no objective measure that makes code ‘clean’ vs. ‘un-clean’, rather we should of think ‘clean’ coding as the pursuit of making your code easier to read, understand and maintain. Also while we should aspire to writing clean code, it is arguably more important that it functions correctly and efficiently.\nThe central concept of clean coding is that, like normal writing, we should follow a set of rules and conventions. For example, in English a sentence should start with a capital letter and end with a full stop. Unfortunately, in terms of writing R code there is not a single set of conventions that everyone proscribes to, instead there are numerous styles that have been outlined and the important thing is to choose a style and apply it consistently in your coding.\nPerhaps the two most common styles are the Tidyverse style and the Google R style (Which is actually a derivative of the former). Neither style can be said to be the more correct, rather they express opinionated preferences on a series of common topics such as: Object naming, use of assignment operators, spacing, indentation, line length, parentheses placement, etc.\nRather than detail all of these topics here we will focus on just on some related tips that we think are most relevant for scientific research coding, including how to automate the formatting of your code to a particular style. However, we encourage you to go through the different style guides when you have the time.\n\nScript headers\nStarting your scripts with a consistent header containing information about it’s purpose, author/s, creation and modification dates is a great step making your workflow more understandable and hopefully reproducible. There are no rules as to what such a header should look like but this is the style I like to use:\nTo save time inserting this header into new scripts you use Rstudio’s Code snippets feature. Code snippets are simply text macros that quickly insert a section of code using a short keyword.\nTo create your own Code snippet go to Tools &gt; Global Options &gt; Code &gt; Edit Snippets and then add a new snippet with your code below it:\n\n\n\n\n\nTo use a code snippet simply start typing the keyword in the script and the auto-completion list will appear then press Tab and the code section will be inserted:\n\n\n\n\n\n\n\nCode sections\nAs you may already know braced ({}) sections of code (i.e. function definitions, conditional blocks, etc.) can be folded to hide their contents in RStudio by clicking on the small triangle in the left margin.\n\n\n\n\n\nHowever, an often overlooked feature is the ability to create named code sections that can be also folded, as well as easily navigated between. These can be used to break longer scripts into a set of discrete regions according to specific parts of the analysis (discussed in more detail later). In this regard, another good tip is to give the resulting sections sequential alphabetical or numerical Pre-fixes. Code sections are created by inserting a comment line that contains at least four trailing dashes (-), equal signs (=), or pound signs (#):\nAlternatively you can use the Code &gt; Insert Section command.\nTo navigate between code sections:\n\nUse the Jump To menu available at the bottom of the editor[15]\n\n\n\n\n\n\n\nUse the document outline pane in the top right corner of the source pane\n\n\n\n\n\n\n\n\nAutomating the styling of your code\nThere are two R packages that are very helpful in terms of ensuring your code confirms to a consistent style: lintr and styler.\n\nlintr checks your code for common style issues and potential programming errors then presents them to you to correct, think of it like doing a ‘spellcheck’ on a written document.\nstyler is more active in the sense that it automatically format’s your code to a particular style, the default of which is the tidyverse style.\n\nTo use lintr and styler you call their functions like any package but styler can also be used through the Rstudio Addins menu below the Navigation bar as shown in this gif:\n\n\n\n\n\nAnother very useful feature of both packages is that they can be used as part of a continuous integration (CI) workflow using a version control application like Git. This is a topic that we will cover as part of our Version control with Git workflow but what it means is that the styler and lintr functions are run automatically when you push your code to a remote repository.",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#sec-workflow-decomposition",
    "href": "presentation.html#sec-workflow-decomposition",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Workflow decomposition",
    "text": "Workflow decomposition\nIn computer sciences workflow decomposition refers to the structuring or compartmentalising of your code into seperate logical parts that makes it easier to maintain [16].\nIn terms of coding scientific research projects many of us probably already instinctively do decomposition to some degree by splitting typical processes such as data preparation, statistical modelling, analysis of results and producing final visualizations.\nHowever this is not always realized in the most understandable way, for example we may have seperate scripts with logical sounding names like: Data_prep.R and Data_analysis.R but can others really be expected to know exactly which order these must be run in, or indeed whether they even need to be run sequentially at all?\nA good 1st step to remedying this is to give your scripts sequential numeric tags in their names, e.g. 01_Data_prep.R, 02_Data_analysis.R. This will also ensure that they are presented in numerical order when placed in a designated directory Structuring your project directory and can be explicitly described in your project documentation.\nBut you can take this to the next level by creating a Master script that sources your other scripts in sequence (think of them as sub-scripts) so that users of your code need only run one script. To do this is as simple as creating the master script as you would any normal R script (File &gt; New File &gt; R script) and then using the base::source() function to run the sub-scripts:\nAs you can see in this example code I have also made use of a script header and code sections, that were previously discussed, to make the division of sub-processes even clearer. Another advantage of this approach is that all sub-scripts can utilise the same environment (defined by the source(local= ) argument) which means that each individual script does not need to load packages or paths as objects.\nFinally, within your sub-scripts processes should also be seperated into code sections and ideally any repetitive tasks should be performed with custom functions which again are contained within their own files.\nFollowing this approach you end up with a workflow that will look something like this:\n\nThe benefit of this hierarchical approach to structuring is that it is not only easier to debug and maintain individual processes but it is also more amenable to adding new processes.",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#sec-structuring",
    "href": "presentation.html#sec-structuring",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Structuring your project directory",
    "text": "Structuring your project directory\nSimilar to having clean code, having a clean project directory that has well-organised sub-directories goes a long way towards making your projects code easier to understand for others. For software development there are numerous sets of conventions for directories structures although these are not always so applicable for scientific research projects. However we can borrow some basic principles, try to use: - Use logical naming - Stick to a consistent style, i.e. use of captialisation and seperators - Make use of nested sub-directories e.g data/raw/climatic/precipitation/2020/precip_2020.rds vs. data/precip_2020_raw.rds. This is very helpful when it comes to programatically constructing file paths especially in projects with a lot of data.\nAs an example my go-to project directory structure looks like this:\nRather than manually create this directory structure everytime you start a new project, save yourself some time and automate it by using Rstudio’s Project Templates functionality. This allows you to select a custom template as an option when creating a new Rstudio project through the New project wizard (File &gt; New Project &gt; New Directory &gt; New Project Template).\nTo implement this even as an intermediate R user is fairly labor intensive as your custom project directory template needs to be contained within an R-package, in order to be available in the wizard. However, quite a few templates with directory structures appropriate for scientific research projects have been created by others:\n\nrrtools\nProjectTemplate\ntemplate\naddinit (Not a template but an interactive shiny add-in for project creation)",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "presentation.html#sec-documentation",
    "href": "presentation.html#sec-documentation",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Project documentation",
    "text": "Project documentation\nAs an example of why documentation is important think about if you bought a new table from Ikea only to excitedly rip open the box and find that there are no instructions for how to assemble it. Sure, you know what a table is supposedly to look like and given enough time you will end up with something that will probably be mostly right but maybe it’s missing small details. Also it will probably take you just as long to take it apart in 5 years time. Well, working with undocumented code for research projects is similar except a lot more complicated!\nWriting comprehensive documentation that covers all aspects of our projects is time-consuming which is why it is often neglected. For example, there are a lot of different metadata conventions that exist that you could apply. However, learning and adhering strictly to these can be overwhelming and possibly lead to the opposite effect i.e. they are not simple for others to understand either.\nIn response to this there has been a movement in the R research community to adopt the research as package approach, which, as the name suggests, involves creating your project as an R-package which has a strict set of conventions for documentation [17]. This is a viable approach for those who are familiar with R-packages but is arguably not the best for all projects and users.\nInstead, we would suggest to follow the maxim of not letting the perfect be the enemy of the good and to focus on these key areas:\n\nProvide adequate in-script commentary: This is perhaps contentious for those from a software development community, but given the choice I would rather have to read through a script with too many comments than one with too few. However remember that comments should be used to explain the purpose of the code, not what the code is doing. In line with this use script headers.\nDocument your functions with roxygen skeletons:\nInclude a README file: README files are where you should document your project at the macro-level i.e. what it is about and how it is supposed to work.\n\nThe latter of these two are more detailed so we have provided further information and tips in sections below.\n\nFunction documentation with roxygen2\nBase R provides a standard way of documenting a package where each function is documented in an .Rd file (R documentation). This documentation uses a custom syntax to detail key aspects of the functions such as their input parameters, outputs and any package dependencies [18].\nIn the case of many research projects you will not be creating a package however it is still useful to apply this documentation style to your functions as it is a good way to make them understandable and easier to modify by others. For example, having clear information about the object (e.g. a vector or data.frame) that a function accepts, saves others time in guessing what the function is expecting if they are trying to use new data.\nHowever, rather than manually writing .Rd files, we can use the roxygen2 package to automatically generate these files from a block of comments that are added to the top of the function scripts. To add this comment block, place your cursor inside a function you want to document and press Ctrl + Shift + R (or Cmd + Shift + R on Mac) or you can go to code tools &gt; insert roxygen skeleton (code tools is represented by the wand icon in the top row of the source pane). As you can see in this gif below, when you insert the roxygen block it will already contain the names of the function, its arguments and any returns. You can then fill in the rest of the information, such as the description and dependencies etc. for a guide to these other fields see the roxygen2 documentation.\n\n\n\nInserting roxygen block\n\n\n\n\nTips for README writing\nIf you look at the source code of R packages or projects that use R in Github repositories you will see that they all contain README.md files. .md is Markdown format which is the most common format for README files in R projects because it can be read by many programs and rendered in a variety of formats. These files are often accompanied by the corresponding file README.Rmd which generates the README.md file. In this sense writing the README for your project in markdown makes sense and there tools available to help you do this such as the usethis package which has a function use_readme_rmd() that will create a README.Rmd file for you. However, depending on who you anticipate using your project you may also want to create your README as a raw text file (.txt) which may be a more familiar format for some users and again can be opened by many different programs.\nAgain there is not a single standardised format for what should be included in your README file but here is an example of a README file that was written for one of the authors code/data upload alongside a publication: README.txt\nYou will see that one of the things this README includes is a tree diagram which shows the directory structure of the project right down to the file level. This is a useful way to give an overview of what users should find included in the project and then explanatory notes can be added to explain the purpose of each file or directory. Such a diagram can be easily generated using the fs package:",
    "crumbs": [
      "View in presentation format"
    ]
  },
  {
    "objectID": "contents/Resources.html#r-and-git",
    "href": "contents/Resources.html#r-and-git",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "",
    "text": "[ to  pipeline] https://rstudio.github.io/renv/articles/renv.html\n[ to  pipeline] https://intro2r.com/"
  },
  {
    "objectID": "contents/Resources.html#quarto",
    "href": "contents/Resources.html#quarto",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Quarto",
    "text": "Quarto\n[ to  pipeline] https://youtu.be/w3X76e_tZr8?si=bE4OfrNm-wLaeBlk\nhttps://youtu.be/EbAAmrB0luA?si=yUXEup4qpRXCDh5k\n[ to  pipeline] https://www.marvinschmitt.com/blog/website-tutorial-quarto/\nhttps://marioangst.com/en/blog/posts/multi-language-quarto/"
  },
  {
    "objectID": "contents/Resources.html#zenodo",
    "href": "contents/Resources.html#zenodo",
    "title": "Reproducible Research with <img style=\"vertical-align:middle; height:1em; border: none; background: none;\" src=\"assets/images/Rlogo.png\"> and <img style=\"vertical-align:middle; height:1em;\" src=\"assets/images/quarto-logo-trademark.svg\">",
    "section": "Zenodo",
    "text": "Zenodo\n[ to  pipeline] https://cran.r-project.org/web/packages/zen4R/vignettes/zen4R.html"
  },
  {
    "objectID": "index.html#r-and-git",
    "href": "index.html#r-and-git",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "R and Git",
    "text": "R and Git\n[ to  pipeline] https://rstudio.github.io/renv/articles/renv.html\n[ to  pipeline] https://intro2r.com/"
  },
  {
    "objectID": "index.html#quarto-11",
    "href": "index.html#quarto-11",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Quarto",
    "text": "Quarto\n[ to  pipeline] https://youtu.be/w3X76e_tZr8?si=bE4OfrNm-wLaeBlk\nhttps://youtu.be/EbAAmrB0luA?si=yUXEup4qpRXCDh5k\n[ to  pipeline] https://www.marvinschmitt.com/blog/website-tutorial-quarto/\nhttps://marioangst.com/en/blog/posts/multi-language-quarto/"
  },
  {
    "objectID": "index.html#zenodo",
    "href": "index.html#zenodo",
    "title": "Reproducible Research with  and : Workflows for data, projects and publications",
    "section": "Zenodo",
    "text": "Zenodo\n[ to  pipeline] https://cran.r-project.org/web/packages/zen4R/vignettes/zen4R.html"
  }
]