
@misc{bryan2017,
	title = {Project-oriented workflow},
	author = {Bryan, Jenny},
	year = {2017},
	date = {2017},
	url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
	langid = {en-us}
}

@misc{positsupport2024,
	title = {Code Folding and Sections in the RStudio IDE},
	author = {Posit Support, },
	year = {2024},
	date = {2024},
	url = {https://support.posit.co/hc/en-us/articles/200484568-Code-Folding-and-Sections-in-the-RStudio-IDE}
}

@misc{devries2014,
	title = {Revisiting package dependencies},
	author = {de Vries, Andrie},
	year = {2014},
	date = {2014},
	url = {https://blog.revolutionanalytics.com/2014/07/revisiting-package-dependencies.html}
}

@misc{devries2014,
	title = {Revisiting package dependencies},
	author = {de Vries, Andrie},
	year = {2014},
	date = {2014},
	url = {https://blog.revolutionanalytics.com/2014/07/revisiting-package-dependencies.html}
}

@article{renv,
	title = {renv: Project Environments},
	author = {Ushey, Kevin and Wickham, Hadley},
	year = {2024},
	date = {2024},
	url = {https://CRAN.R-project.org/package=renv}
}

@article{renv,
	title = {renv: Project Environments},
	author = {Ushey, Kevin and Wickham, Hadley},
	year = {2024},
	date = {2024},
	url = {https://CRAN.R-project.org/package=renv}
}

@misc{decompos2024,
	title = {Decomposition (computer science)},
	year = {2024},
	month = {05},
	date = {2024-05-23},
	url = {https://en.wikipedia.org/w/index.php?title=Decomposition_(computer_science)&oldid=1225228761},
	note = {Page Version ID: 1225228761},
	langid = {en}
}

@article{marwick2018,
	title = {Packaging Data Analytical Work Reproducibly Using R (and Friends)},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	year = {2018},
	month = {01},
	date = {2018-01-02},
	journal = {The American Statistician},
	pages = {80--88},
	volume = {72},
	number = {1},
	doi = {10.1080/00031305.2017.1375986},
	url = {https://doi.org/10.1080/00031305.2017.1375986},
	note = {Publisher: ASA Website
{\_}eprint: https://doi.org/10.1080/00031305.2017.1375986}
}

@book{wickham2024,
	title = {R Packages (2e)},
	author = {Wickham, Hadley and Bryan, Jennifer},
	year = {2024},
	date = {2024},
	edition = {2nd Edition},
	url = {https://r-pkgs.org/}
}

@misc{hajnala2018,
	title = {RStudio:addins part 2 - roxygen documentation formatting made easy - Jozef's Rblog},
	author = {Hajnala, Jozef},
	year = {2018},
	date = {2018},
	url = {https://jozef.io/r102-addin-roxytags/},
	langid = {en-us}
}

@article{alston_beginners_2021,
	title = {A {Beginner}'s {Guide} to {Conducting} {Reproducible} {Research}},
	volume = {102},
	issn = {0012-9623},
	url = {https://doi.org/10.1002/bes2.1801},
	doi = {10.1002/bes2.1801},
	number = {2},
	urldate = {2024-09-04},
	journal = {The Bulletin of the Ecological Society of America},
	author = {Alston, Jesse M. and Rick, Jessica A.},
	month = apr,
	year = {2021},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {e01801},
}

@article{siraji_primer_2023,
	title = {Primer on {Reproducible} {Research} in {R}: {Enhancing} {Transparency} and {Scientific} {Rigor}.},
	volume = {6},
	issn = {2624-5175},
	doi = {10.3390/clockssleep6010001},
	abstract = {Achieving research reproducibility is a precarious aspect of scientific practice. However, many studies across disciplines fail to be fully reproduced due to  inadequate dissemination methods. Traditional publication practices often fail to  provide a comprehensive description of the research context and procedures,  hindering reproducibility. To address these challenges, this article presents a  tutorial on reproducible research using the R programming language. The tutorial  aims to equip researchers, including those with limited coding knowledge, with  the necessary skills to enhance reproducibility in their work. It covers three  essential components: version control using Git, dynamic document creation using  rmarkdown, and managing R package dependencies with renv. The tutorial also  provides insights into sharing reproducible research and offers specific  considerations for the field of sleep and chronobiology research. By following  the tutorial, researchers can adopt practices that enhance the transparency,  rigor, and replicability of their work, contributing to a culture of reproducible  research and advancing scientific knowledge.},
	language = {eng},
	number = {1},
	journal = {Clocks \& sleep},
	author = {Siraji, Mushfiqul Anwar and Rahman, Munia},
	month = dec,
	year = {2023},
	pmid = {38534796},
	pmcid = {PMC10969410},
	note = {Place: Switzerland},
	keywords = {Git, GitHub, R, renv, reproducible research, version control},
	pages = {1--10},
}

@article{essawy_taxonomy_2020,
	title = {A taxonomy for reproducible and replicable research in environmental modelling},
	volume = {134},
	issn = {1364-8152},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815219311612},
	doi = {10.1016/j.envsoft.2020.104753},
	abstract = {Despite the growing acknowledgment of reproducibility crisis in computational science, there is still a lack of clarity around what exactly constitutes a reproducible or replicable study in many computational fields, including environmental modelling. To this end, we put forth a taxonomy that defines an environmental modelling study as being either 1) repeatable, 2) runnable, 3) reproducible, or 4) replicable. We introduce these terms with illustrative examples from hydrology using a hydrologic modelling framework along with cyberinfrastructure aimed at fostering reproducibility. Using this taxonomy as a guide, we argue that containerization is an important but lacking component needed to achieve the goal of computational reproducibility in hydrology and environmental modelling. Examples from hydrology are provided to demonstrate how new tools, including a user-friendly tool for containerization of computational analyses called Sciunit, can lower the barrier to reproducibility and replicability in the environmental modelling community.},
	journal = {Environmental Modelling \& Software},
	author = {Essawy, Bakinam T. and Goodall, Jonathan L. and Voce, Daniel and Morsy, Mohamed M. and Sadler, Jeffrey M. and Choi, Young Don and Tarboton, David G. and Malik, Tanu},
	month = dec,
	year = {2020},
	keywords = {Containers, Docker, Replicability, Reproducibility, Singularity},
	pages = {104753},
}

@article{collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/abs/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Collaboration, Open Science},
	year = {2015},
	note = {\_eprint: https://www.science.org/doi/pdf/10.1126/science.aac4716},
	pages = {aac4716},
}

@article{bohannon_many_2015,
	title = {Many psychology papers fail replication test},
	volume = {349},
	url = {https://www.science.org/doi/abs/10.1126/science.349.6251.910},
	doi = {10.1126/science.349.6251.910},
	abstract = {An effort to repeat 100 studies yields sobering results, but many researchers are positive about the process. The largest effort yet to replicate psychology studies has yielded both good and bad news. On the down side, of the 100 prominent papers analyzed, only 39\% could be replicated unambiguously, as a group of 270 researchers describes on page 943. On the up side, despite the sobering results, the effort seems to have drawn little of the animosity that greeted a similar replication effort last year. This time around, even some of the original authors see the replications as a useful addition to their own research. "This is how science works," says Joshua Correll, a psychologist at the University of Colorado, Boulder, and one of the authors whose results could not be replicated. "How else will we converge on the truth? Really, the surprising thing is that this kind of systematic attempt at replication is not more common." That's encouraging news to Brian Nosek, a psychologist at the University of Virginia in Charlottesville who led the mass replication effort, which began in 2011 with the goal of putting psychological science on more rigorous experimental footing.},
	number = {6251},
	journal = {Science},
	author = {Bohannon, John},
	year = {2015},
	note = {\_eprint: https://www.science.org/doi/pdf/10.1126/science.349.6251.910},
	pages = {910--911},
}

@article{moonesinghe_most_2007,
	title = {Most {Published} {Research} {Findings} {Are} {False}—{But} a {Little} {Replication} {Goes} a {Long} {Way}},
	volume = {4},
	url = {https://doi.org/10.1371/journal.pmed.0040028},
	doi = {10.1371/journal.pmed.0040028},
	abstract = {While the authors agree with John Ioannidis that "most research findings are false," here they show that replication of research findings enhances the positive predictive value of research findings being true.},
	number = {2},
	journal = {PLOS Medicine},
	author = {Moonesinghe, Ramal and Khoury, Muin J and Janssens, A. Cecile J. W},
	month = feb,
	year = {2007},
	note = {Publisher: Public Library of Science},
	pages = {1--4},
}

@article{piwowar_sharing_2007,
	title = {Sharing {Detailed} {Research} {Data} {Is} {Associated} with {Increased} {Citation} {Rate}},
	volume = {2},
	url = {https://doi.org/10.1371/journal.pone.0000308},
	doi = {10.1371/journal.pone.0000308},
	abstract = {BackgroundSharing research data provides benefit to the general scientific community, but the benefit is less obvious for the investigator who makes his or her data available.Principal FindingsWe examined the citation history of 85 cancer microarray clinical trial publications with respect to the availability of their data. The 48\% of trials with publicly available microarray data received 85\% of the aggregate citations. Publicly available data was significantly (p = 0.006) associated with a 69\% increase in citations, independently of journal impact factor, date of publication, and author country of origin using linear regression.SignificanceThis correlation between publicly available data and increased literature impact may further motivate investigators to share their detailed research data.},
	number = {3},
	journal = {PLOS ONE},
	author = {Piwowar, Heather A. and Day, Roger S. and Fridsma, Douglas B.},
	month = mar,
	year = {2007},
	note = {Publisher: Public Library of Science},
	pages = {1--5},
}

@article{mckiernan_how_2016,
	title = {How open science helps researchers succeed.},
	volume = {5},
	issn = {2050-084X},
	doi = {10.7554/eLife.16800},
	abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these  practices has not yet been achieved. One reason is that researchers are uncertain  about how sharing their work will affect their careers. We review literature  demonstrating that open research is associated with increases in citations, media  attention, potential collaborators, job opportunities and funding opportunities.  These findings are evidence that open research practices bring significant  benefits to researchers relative to more traditional closed practices.},
	language = {eng},
	journal = {eLife},
	author = {McKiernan, Erin C. and Bourne, Philip E. and Brown, C. Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A. and Ram, Karthik and Soderberg, Courtney K. and Spies, Jeffrey R. and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H. and Yarkoni, Tal},
	month = jul,
	year = {2016},
	pmid = {27387362},
	pmcid = {PMC4973366},
	note = {Place: England},
	keywords = {*Access to Information, *Open Access Publishing, none, open access, open data, open science, open source, research, Research Personnel/*psychology, Research/*trends},
	pages = {e16800},
}

@article{whitlock_data_2011,
	title = {Data archiving in ecology and evolution: best practices},
	volume = {26},
	issn = {0169-5347},
	url = {https://www.sciencedirect.com/science/article/pii/S0169534710002697},
	doi = {https://doi.org/10.1016/j.tree.2010.11.006},
	abstract = {Many ecology and evolution journals have recently adopted policies requiring that data from their papers be publicly archived. I present suggestions on how data generators, data re-users, and journals can maximize the fairness and scientific value of data archiving. Data should be archived with enough clarity and supporting information that they can be accurately interpreted by others. Re-users should respect their intellectual debt to the originators of data through citation both of the paper and of the data package. In addition, journals should consider requiring that all data for published papers be archived, just as DNA sequences must be deposited in GenBank. Data are another valuable part of the legacy of a scientific career and archiving them can lead to new scientific insights. Archiving also increases opportunities for credit to be given to the scientists who originally collected the data.},
	number = {2},
	journal = {Trends in Ecology \& Evolution},
	author = {Whitlock, Michael C.},
	year = {2011},
	pages = {61--65},
}

@article{culina_how_2018,
	title = {How to do meta-analysis of open datasets},
	volume = {2},
	issn = {2397-334X},
	url = {https://doi.org/10.1038/s41559-018-0579-2},
	doi = {10.1038/s41559-018-0579-2},
	abstract = {The amount of open data in ecology and evolution is increasing rapidly, yet this resource remains underused. Here, we introduce a new framework and case study for conducting meta-analyses of open datasets, and discuss its benefits and current limitations.},
	number = {7},
	journal = {Nature Ecology \& Evolution},
	author = {Culina, Antica and Crowther, Thomas W. and Ramakers, Jip J. C. and Gienapp, Phillip and Visser, Marcel E.},
	month = jul,
	year = {2018},
	pages = {1053--1056},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	issn = {2052-4463},
	url = {https://doi.org/10.1038/sdata.2016.18},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	number = {1},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	pages = {160018},
}

@misc{blondel_zen4r_2024,
	title = {{zen4R}: {R} {Interface} to {Zenodo} {REST} {API}},
	url = {https://doi.org/10.5281/zenodo.11500665},
	publisher = {Zenodo},
	author = {Blondel, Emmanuel and Barde, Julien},
	month = jun,
	year = {2024},
	doi = {10.5281/zenodo.11500665},
}


